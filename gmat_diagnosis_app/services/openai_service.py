"""
OpenAIæœå‹™æ¨¡çµ„
æä¾›èˆ‡OpenAI APIé€šä¿¡çš„åŠŸèƒ½
"""

import logging
import streamlit as st
import openai

def summarize_report_with_openai(report_markdown, api_key):
    """Attempts to summarize/reformat a report using OpenAI API."""
    if not api_key:
        logging.warning("OpenAI API key is missing. Skipping summarization.")
        return report_markdown # Return original if no key

    try:
        # Ensure openai library is accessible
        client = openai.OpenAI(api_key=api_key) # Initialize client
        system_prompt = """You are an assistant that reformats GMAT diagnostic reports. Use uniform heading levels: Level-2 headings for all major sections (## Section Title). Level-3 headings for subsections (### Subsection Title). Reformat content into clear Markdown tables where appropriate. Fix minor grammatical errors or awkward phrasing for readability. IMPORTANT: Do NOT add or remove any substantive information or conclusions. Only reformat and polish the existing text. Output strictly in Markdown format, without code blocks.  """

        response = client.chat.completions.create(
            model="gpt-4.1-nano", # Use the specified model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": report_markdown}
            ],
            # temperature=0.2 # Removed: o4-mini only supports default (1)
        )
        summarized_report = response.choices[0].message.content
        # Basic check if response seems valid (not empty)
        if summarized_report and summarized_report.strip():
             logging.info(f"OpenAI summarization successful for report starting with: {report_markdown[:50]}...")
             return summarized_report.strip()
        else:
             logging.warning("OpenAI returned empty response. Using original report.")
             st.warning("AI æ•´ç†å ±å‘Šæ™‚è¿”å›äº†ç©ºå…§å®¹ï¼Œå°‡ä½¿ç”¨åŸå§‹å ±å‘Šã€‚", icon="âš ï¸")
             return report_markdown

    except openai.AuthenticationError:
        st.warning("OpenAI API Key ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•æ•´ç†å ±å‘Šæ–‡å­—ã€‚è«‹æª¢æŸ¥å´é‚Šæ¬„è¼¸å…¥æˆ–ç’°å¢ƒè®Šæ•¸ã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError.")
        return report_markdown
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â³")
        logging.error("OpenAI RateLimitError.")
        return report_markdown
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError: {e}")
        return report_markdown
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError.")
        return report_markdown
    except openai.BadRequestError as e:
        # Often happens with context length issues or invalid requests
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ã€‚å¯èƒ½æ˜¯å ±å‘Šéé•·æˆ–æ ¼å¼å•é¡Œã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â—") # Use valid emoji
        logging.error(f"OpenAI BadRequestError: {e}")
        return report_markdown
    except Exception as e:
        st.warning(f"èª¿ç”¨ OpenAI API æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="âš ï¸")
        logging.error(f"Unknown OpenAI API error: {e}", exc_info=True)
        return report_markdown

def generate_ai_consolidated_report(report_dict, api_key):
    """Generates a consolidated report of suggestions and next steps using OpenAI o4-mini.

    Args:
        report_dict (dict): Dictionary containing the diagnostic reports for each subject (Q, V, DI).
        api_key (str): OpenAI API key.

    Returns:
        str: The generated consolidated report in Markdown, or None if an error occurs.
    """
    if not api_key:
        logging.warning("OpenAI API key missing. Skipping consolidated report generation.")
        return None
    if not report_dict:
        logging.warning("Report dictionary is empty. Skipping consolidated report generation.")
        return None

    client = openai.OpenAI(api_key=api_key)

    # Construct the full input text from individual reports
    full_report_text = ""
    for subject, report in report_dict.items():
        if report:
            full_report_text += f"## {subject} Report:\n\n{report}\n\n---\n\n"

    if not full_report_text.strip():
        logging.warning("No valid reports found in the dictionary. Skipping consolidated report generation.")
        return None

    # Define the system prompt emphasizing strict extraction and formatting
    system_prompt = """You are an assistant that extracts specific sections from GMAT diagnostic reports and consolidates them into a single, structured document.
Your task is to identify and extract ONLY the sections related to 'ç·´ç¿’å»ºè­°' (Practice Suggestions) and 'å¾ŒçºŒè¡Œå‹•' (Next Steps, including reflection and secondary evidence gathering) for each subject (Q, V, DI) from the provided text.

**CRITICAL INSTRUCTIONS:**
1.  **Strict Extraction:** Only extract text explicitly under 'ç·´ç¿’å»ºè­°' or 'å¾ŒçºŒè¡Œå‹•' headings or clearly discussing these topics.
2.  **Subject Separation:** Keep the extracted information strictly separated under standardized headings for each subject: `## Q ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`, `## V ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`, `## DI ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`.
3.  **Complete Information:** Transfer ALL original text, data, details, parenthetical notes, specific question numbers, difficulty codes, time limits, percentages, scores, etc., accurately and completely. DO NOT SUMMARIZE OR OMIT ANY DETAILS.
4.  **Standardized Formatting:** Use Markdown format. Use Level-2 headings (##) for each subject as specified above. Use bullet points or numbered lists within each subject section as they appear in the original text, or to improve readability if appropriate, but maintain all original content.
5.  **No Additions:** Do not add any information, interpretation, or introductory/concluding text not present in the extracted sections.
6.  **Output Format:** Output only the consolidated Markdown report. Do not include any other text or commentary.
"""

    try:
        logging.info("Calling OpenAI responses.create with model o4-mini for consolidated report.")
        response = client.responses.create(
            model="o4-mini",
            input=f"""
System: {system_prompt}

User: å¾ä»¥ä¸‹ GMAT è¨ºæ–·å ±å‘Šä¸­æå–ç·´ç¿’å»ºè­°å’Œå¾ŒçºŒè¡Œå‹•éƒ¨åˆ†ï¼ŒæŒ‰ç§‘ç›®åˆ†é¡ä¸¦æ•´ç†æˆçµ±ä¸€æ ¼å¼ï¼š

{full_report_text}
""",
            # No prompt parameter - incorporate system instructions into input instead
            # No previous_response_id needed for this one-off task
            # max_output_tokens=1000 # Optional: Set a limit if needed
        )
        logging.info(f"OpenAI consolidated report response received. Status: {response.status}")

        if response.status == 'completed' and response.output:
            response_text = ""
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break

            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        response_text += content_block.text

            if not response_text:
                logging.error("OpenAI consolidated report response completed but no text found.")
                st.warning("AI ç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚", icon="âš ï¸")
                return None
            else:
                logging.info("Successfully generated consolidated report.")
                return response_text.strip()
        elif response.status == 'error':
            error_details = response.error if response.error else "Unknown error"
            logging.error(f"OpenAI API error (consolidated report): {error_details}")
            st.warning(f"AI ç”ŸæˆåŒ¯ç¸½å ±å‘Šæ™‚å‡ºéŒ¯ï¼š{error_details}", icon="â—")
            return None
        else:
            logging.error(f"OpenAI consolidated report status not completed or output empty. Status: {response.status}")
            st.warning(f"AI æœªèƒ½æˆåŠŸç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆç‹€æ…‹ï¼š{response.status}ï¼‰ã€‚", icon="âš ï¸")
            return None

    except openai.AuthenticationError:
        st.warning("OpenAI API Key ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError (consolidated report).")
        return None
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â³")
        logging.error("OpenAI RateLimitError (consolidated report).")
        return None
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError (consolidated report): {e}")
        return None
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError (consolidated report).")
        return None
    except openai.BadRequestError as e:
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â—")
        logging.error(f"OpenAI BadRequestError (consolidated report): {e}")
        return None
    except Exception as e:
        st.warning(f"ç”Ÿæˆ AI åŒ¯ç¸½å»ºè­°æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}", icon="âš ï¸")
        logging.error(f"Unknown error during consolidated report generation: {e}", exc_info=True)
        return None

def get_chat_context(session_state, max_rows=100):
    """Get combined report and dataframe context for chat."""
    context = {
        "report": _get_combined_report_context(session_state),
        "dataframe": _get_dataframe_context(session_state, max_rows)
    }
    return context

def _get_combined_report_context(session_state):
    """Combines markdown reports from all subjects."""
    full_report = ""
    
    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¦åˆ†æ•¸/ç™¾åˆ†ä½è³‡è¨Šä¸¦æ·»åŠ åˆ°å ±å‘Šé–‹é ­
    if hasattr(session_state, 'total_score') and session_state.total_score:
        full_report += "## ç¸½é«”åˆ†æ•¸èˆ‡ç™¾åˆ†ä½\n\n"
        
        # æ·»åŠ å„ç§‘ç›®åˆ†æ•¸
        scores_text = []
        if hasattr(session_state, 'q_score') and session_state.q_score:
            scores_text.append(f"Q (Quantitative): {session_state.q_score}")
        if hasattr(session_state, 'v_score') and session_state.v_score:
            scores_text.append(f"V (Verbal): {session_state.v_score}")
        if hasattr(session_state, 'di_score') and session_state.di_score:
            scores_text.append(f"DI (Data Insights): {session_state.di_score}")
        if hasattr(session_state, 'total_score') and session_state.total_score:
            scores_text.append(f"Total: {session_state.total_score}")
            
        if scores_text:
            full_report += "### å„ç§‘ç›®åˆ†æ•¸\n\n"
            for score in scores_text:
                full_report += f"- {score}\n"
            full_report += "\n"
    
    # æ·»åŠ AIç¸½çµå»ºè­°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(session_state, 'consolidated_report_text') and session_state.consolidated_report_text:
        full_report += "## AI ç¸½çµå»ºè­°\n\n"
        full_report += session_state.consolidated_report_text
        full_report += "\n\n---\n\n"
    
    # æ·»åŠ å„ç§‘ç›®è¨ºæ–·å ±å‘Š
    if session_state.report_dict:
        for subject in ['Q', 'V', 'DI']:  # æ˜ç¢ºå®šç¾©ç§‘ç›®é †åº
            report = session_state.report_dict.get(subject)
            if report:
                full_report += f"## {subject} ç§‘è¨ºæ–·å ±å‘Š\n\n{report}\n\n---\n\n"
                
    # å¦‚æœå·²ç¶“å­˜åœ¨æ–°è¨ºæ–·å ±å‘Šï¼ˆå¾ç·¨è¼¯æ¨™ç±¤ç”Ÿæˆï¼‰ï¼Œå‰‡æ·»åŠ åˆ°å ±å‘Šä¸­
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None:
        try:
            # ä½¿ç”¨å…§éƒ¨å‡½æ•¸ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šï¼Œé¿å…å¾ªç’°å°å…¥
            df = session_state.editable_diagnostic_df
            if 'generated_new_diagnostic_report' in session_state:
                # ä½¿ç”¨å·²ç”Ÿæˆçš„å ±å‘Š
                new_report = session_state.generated_new_diagnostic_report
                full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                full_report += new_report
                full_report += "\n\n"
            else:
                # å˜—è©¦ç”Ÿæˆæ–°å ±å‘Š
                logging.info("å˜—è©¦ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Š...")
                # æˆ‘å€‘å°‡ä½¿ç”¨å‹•æ…‹å°å…¥ä¾†é¿å…å¾ªç’°å°å…¥å•é¡Œ
                import importlib
                try:
                    results_display_module = importlib.import_module('gmat_diagnosis_app.ui.results_display')
                    generate_report_func = getattr(results_display_module, 'generate_new_diagnostic_report')
                    
                    new_report = generate_report_func(df)
                    if new_report:
                        # ä¿å­˜åˆ°session_stateä»¥ä¾¿é‡è¤‡ä½¿ç”¨
                        session_state.generated_new_diagnostic_report = new_report
                        full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                        full_report += new_report
                        full_report += "\n\n"
                except (ImportError, AttributeError) as ie:
                    logging.error(f"ç„¡æ³•å°å…¥æˆ–ä½¿ç”¨generate_new_diagnostic_reportå‡½æ•¸: {ie}")
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šæ™‚å‡ºéŒ¯: {e}")
    
    return full_report.strip()

def _get_dataframe_context(session_state, max_rows=100):
    """Converts the processed dataframe to a string format (markdown) for context."""
    # å„ªå…ˆä½¿ç”¨ä¿®å‰ªæ¨™ç±¤å¾Œçš„æ•¸æ“šè¡¨æ ¼ (å¦‚æœå­˜åœ¨)
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None and not session_state.editable_diagnostic_df.empty:
        df_context = session_state.editable_diagnostic_df.copy()
        logging.info(f"æº–å‚™è½‰æ›ä¿®å‰ªæ¨™ç±¤å¾Œçš„è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    elif session_state.processed_df is not None and not session_state.processed_df.empty:
        df_context = session_state.processed_df.copy()
        logging.info(f"æº–å‚™è½‰æ›åŸå§‹è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    else:
        logging.warning("è¨ºæ–·è©¦ç®—è¡¨ç‚ºç©ºæˆ–ä¸å­˜åœ¨")
        return "(ç„¡è©³ç´°æ•¸æ“šè¡¨æ ¼)"
        
    try:
        # é¸æ“‡é—œéµåˆ—ä»¥æé«˜å¯è®€æ€§ï¼Œæ’é™¤ä¸å¿…è¦çš„åˆ—
        important_cols = [
            'Subject', 'question_position', 'question_type', 'question_fundamental_skill',
            'content_domain', 'is_invalid', 'is_correct', 'question_time',
            'time_performance_category', 'diagnostic_params_list'
        ]
        
        # åªä¿ç•™å­˜åœ¨æ–¼ DataFrame ä¸­çš„åˆ—
        cols_to_use = [col for col in important_cols if col in df_context.columns]
        
        # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—ï¼ˆä½†å„ªå…ˆä½¿ç”¨ä¸Šé¢å®šç¾©çš„é—œéµåˆ—ï¼‰
        for col in df_context.columns:
            if col not in cols_to_use and col not in ['raw_content']:
                cols_to_use.append(col)
        
        logging.info(f"é¸æ“‡çš„åˆ—: {', '.join(cols_to_use)}")
        
        # å¦‚æœæœ‰åˆ—å¯ç”¨ï¼Œå‰‡ä½¿ç”¨é€™äº›åˆ—ï¼›å¦å‰‡ä½¿ç”¨æ‰€æœ‰åˆ—
        if cols_to_use:
            df_context = df_context[cols_to_use]
        
        # Convert boolean columns to Yes/No for better readability for the LLM
        bool_cols = df_context.select_dtypes(include=bool).columns
        for col in bool_cols:
            df_context[col] = df_context[col].map({True: 'Yes', False: 'No'})
            logging.info(f"å·²å°‡å¸ƒçˆ¾åˆ— '{col}' è½‰æ›ç‚º Yes/No æ ¼å¼")

        # Convert list column to string
        if 'diagnostic_params_list' in df_context.columns:
            df_context['diagnostic_params_list'] = df_context['diagnostic_params_list'].apply(
                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x)
            )
            logging.info("å·²å°‡ 'diagnostic_params_list' åˆ—è½‰æ›ç‚ºå­—ç¬¦ä¸²æ ¼å¼")

        # Limit rows to avoid excessive context length
        if len(df_context) > max_rows:
            logging.info(f"æ•¸æ“šè¡Œæ•¸è¶…éé™åˆ¶ ({len(df_context)} > {max_rows})ï¼Œåªå–å‰ {max_rows} è¡Œ")
            df_context_str = df_context.head(max_rows).to_markdown(index=False)
            df_context_str += f"\n... (åªé¡¯ç¤ºå‰ {max_rows} è¡Œï¼Œå…± {len(df_context)} è¡Œ)"
        else:
            logging.info(f"è½‰æ›å…¨éƒ¨ {len(df_context)} è¡Œæ•¸æ“šç‚º markdown æ ¼å¼")
            df_context_str = df_context.to_markdown(index=False)
        
        logging.info(f"æˆåŠŸè½‰æ›è¨ºæ–·è©¦ç®—è¡¨ï¼Œè¼¸å‡ºé•·åº¦ç´„ {len(df_context_str)} å­—ç¬¦")
        return df_context_str
    except Exception as e:
        error_msg = f"Error converting dataframe to markdown context: {e}"
        logging.error(error_msg, exc_info=True)
        return f"(ç„¡æ³•è½‰æ›è©³ç´°æ•¸æ“šè¡¨æ ¼: {str(e)})"

def get_openai_response(current_chat_history, report_context, dataframe_context, api_key):
    """Gets response from OpenAI using the responses API and handles conversation history.

    Args:
        current_chat_history (list): The current chat history list.
        report_context (str): The combined markdown report context.
        dataframe_context (str): The dataframe context string.
        api_key (str): OpenAI API key.

    Returns:
        tuple: (ai_response_text, response_id) or raises an exception.
    """
    if not api_key:
        raise ValueError("OpenAI API Key is missing.")

    client = openai.OpenAI(api_key=api_key)
    
    # æº–å‚™ç”¨æ–¼æ§‹å»ºå°è©±çš„è¼¸å…¥æ–‡æœ¬
    input_text = ""
    
    # æ‰¾å‡ºæœ€å¾Œä¸€æ¢ assistant æ¶ˆæ¯çš„ response_idï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    previous_response_id = None
    for message in reversed(current_chat_history):
        if message["role"] == "assistant" and "response_id" in message:
            previous_response_id = message["response_id"]
            break
    
    # é¡¯ç¤ºæœ€æ–°è¨Šæ¯è¨˜éŒ„
    latest_message = current_chat_history[-1]["content"] if current_chat_history else None
    logging.info(f"æº–å‚™ç™¼é€è‡³ OpenAI çš„æœ€æ–°è¨Šæ¯: {latest_message[:30]}...")

    # æª¢æŸ¥ä¸Šä¸‹æ–‡å…§å®¹çš„æœ‰æ•ˆæ€§
    report_context_valid = bool(report_context and report_context.strip())
    dataframe_context_valid = bool(dataframe_context and dataframe_context.strip())

    logging.info(f"å ±å‘Šä¸Šä¸‹æ–‡æœ‰æ•ˆ: {report_context_valid}, é•·åº¦: {len(report_context) if report_context_valid else 0}")
    logging.info(f"è¨ºæ–·è©¦ç®—è¡¨ä¸Šä¸‹æ–‡æœ‰æ•ˆ: {dataframe_context_valid}, é•·åº¦: {len(dataframe_context) if dataframe_context_valid else 0}")

    # å¦‚æœè¨ºæ–·è©¦ç®—è¡¨å…§å®¹ç‚ºç©ºï¼Œæ·»åŠ è­¦å‘Š
    if not dataframe_context_valid:
        logging.warning("è¨ºæ–·è©¦ç®—è¡¨å…§å®¹ç‚ºç©ºï¼ŒAIå›æ‡‰å¯èƒ½ä¸å®Œæ•´")
        dataframe_context = "(è¨ºæ–·è©¦ç®—è¡¨æ•¸æ“šä¸å¯ç”¨)"

    # æ§‹å»ºç³»çµ±æŒ‡ä»¤å’Œå°è©±æ­·å²
    chat_system_prompt = f"""You are a helpful GMAT diagnostic assistant.
Your primary goal is to answer questions based SOLELY on the provided GMAT diagnostic report and detailed data table.
Do NOT use any external knowledge or make assumptions beyond what is in the context.
If the answer cannot be found in the provided report or data, state that clearly.

**IMPORTANT**: You have access to both text reports and a diagnostic spreadsheet data table. Please analyze both to provide comprehensive answers.

**Provided Context:**

```markdown
### Report
{report_context}
```

```markdown
### Diagnostic Data Table (è¨ºæ–·è©¦ç®—è¡¨)
{dataframe_context}
```

Answer questions about the GMAT diagnostic results in a direct, helpful, and accurate manner. 
If answering in Chinese, use traditional Chinese characters (ç¹é«”ä¸­æ–‡).
"""

    # æ§‹å»ºç³»çµ±æŒ‡ä»¤å’Œå°è©±æ­·å²
    input_text = f"System: {chat_system_prompt}\n\n"
    
    # æ·»åŠ å°è©±æ­·å²ï¼ˆæœ€å¤šå–æœ€è¿‘10æ¢ï¼Œé¿å…éé•·ï¼‰
    history_start_idx = max(0, len(current_chat_history) - 11)  # ä¿ç•™æœ€å¤š10è¼ªå°è©±
    for i, msg in enumerate(current_chat_history[history_start_idx:], start=history_start_idx):
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            input_text += f"User: {content}\n\n"
        elif role == "assistant":
            input_text += f"Assistant: {content}\n\n"

    try:
        # ä¿®å¾©ï¼šæª¢æŸ¥previous_response_idæ˜¯å¦ç‚ºNone
        log_info = "èª¿ç”¨ OpenAI responses.create API"
        if previous_response_id:
            log_info += f"ï¼Œprevious_response_id: {previous_response_id[:10]}..."
        logging.info(log_info)
        
        # è¨­å®šAPIå‘¼å«åƒæ•¸
        api_params = {
            "model": "o4-mini",  # ä½¿ç”¨o4-miniæ¨¡å‹
            "input": input_text,
            "store": True,  # å­˜å„²å°è©±æ­·å²
            "max_output_tokens": 4000,  # å¢åŠ è¼¸å‡ºæ¨™è¨˜çš„æœ€å¤§æ•¸é‡
        }
        
        # å¦‚æœå­˜åœ¨å‰ä¸€å€‹å›æ‡‰IDï¼ŒåŠ å…¥previous_response_idåƒæ•¸
        if previous_response_id:
            api_params["previous_response_id"] = previous_response_id
            
        # è¨˜éŒ„å®Œæ•´çš„input_texté•·åº¦ï¼Œç”¨æ–¼èª¿è©¦
        logging.info(f"ç™¼é€è‡³OpenAIçš„input_textç¸½é•·åº¦: {len(input_text)} å­—ç¬¦")
        
        # è¨˜éŒ„æœ€å¾Œçš„ç”¨æˆ¶è¨Šæ¯èˆ‡è¨ºæ–·è©¦ç®—è¡¨é•·åº¦æ¯”è¼ƒ
        if latest_message:
            logging.info(f"ç”¨æˆ¶æœ€æ–°è¨Šæ¯é•·åº¦: {len(latest_message)} å­—ç¬¦ï¼Œè¨ºæ–·è©¦ç®—è¡¨é•·åº¦: {len(dataframe_context) if dataframe_context_valid else 0} å­—ç¬¦")
        
        # èª¿ç”¨responses API
        response = client.responses.create(**api_params)
        
        # è¨˜éŒ„å›æ‡‰ç‹€æ…‹
        logging.info(f"OpenAI API å›æ‡‰ç‹€æ…‹: {response.status}")
        
        if response.status == "completed" and response.output:
            # æå–å›æ‡‰æ–‡æœ¬
            response_text = ""
            for output_item in response.output:
                if hasattr(output_item, 'type') and output_item.type == 'message':
                    for content_item in output_item.content:
                        if hasattr(content_item, 'type') and content_item.type == 'output_text':
                            response_text += content_item.text

            if not response_text.strip():
                logging.error("OpenAIè¿”å›ç©ºå›æ‡‰")
                raise ValueError("AIæœªæä¾›æœ‰æ•ˆçš„å›æ‡‰")
                
            # ç²å–response_idä»¥ä¾¿ä¸‹æ¬¡å°è©±ä½¿ç”¨
            response_id = response.id
            # ä¿®å¾©ï¼šå®‰å…¨è¨˜éŒ„ID
            logging.info(f"æˆåŠŸç²å–OpenAIå›æ‡‰ï¼ŒID: {response_id[:10] if response_id else 'N/A'}...")
            
            return response_text.strip(), response_id
            
        elif response.status == "error":
            error_detail = response.error.get("message", "æœªçŸ¥éŒ¯èª¤") if response.error else "æœªçŸ¥éŒ¯èª¤"
            logging.error(f"OpenAI APIéŒ¯èª¤: {error_detail}")
            raise ValueError(f"AIå›æ‡‰éŒ¯èª¤: {error_detail}")
            
        else:
            logging.error(f"ç„¡æ³•è™•ç†çš„OpenAIå›æ‡‰ç‹€æ…‹: {response.status}")
            raise ValueError("ç²å–AIå›æ‡‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤")
            
    except openai.AuthenticationError:
        error_msg = "OpenAI APIå¯†é‘°ç„¡æ•ˆ"
        logging.error(error_msg)
        raise ValueError(error_msg)
        
    except openai.RateLimitError:
        error_msg = "OpenAI APIè«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦"
        logging.error(error_msg)
        raise ValueError(error_msg)
        
    except openai.APIConnectionError as e:
        error_msg = f"é€£æ¥OpenAI APIæ™‚å‡ºéŒ¯: {e}"
        logging.error(error_msg)
        raise ValueError(error_msg)
        
    except openai.APITimeoutError:
        error_msg = "OpenAI APIè«‹æ±‚è¶…æ™‚"
        logging.error(error_msg)
        raise ValueError(error_msg)
        
    except openai.BadRequestError as e:
        error_msg = f"OpenAI APIè«‹æ±‚ç„¡æ•ˆ: {e}"
        logging.error(error_msg)
        raise ValueError(error_msg)
        
    except Exception as e:
        error_msg = f"èˆ‡OpenAI APIé€šè¨Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
        logging.error(error_msg, exc_info=True)
        raise ValueError(error_msg)

def trim_diagnostic_tags_with_openai(original_tags_str: str, user_description: str, api_key: str) -> str:
    """
    Uses OpenAI to suggest a trimmed list of 1-2 most relevant diagnostic tags
    based on original tags and user's description of their difficulty.

    Args:
        original_tags_str: A string containing the original diagnostic tags (e.g., comma-separated or list-like).
        user_description: User's description of their difficulty with the question.
        api_key: OpenAI API key.

    Returns:
        A string containing the AI's suggested trimmed tags, or an error message.
    """
    if not api_key:
        logging.warning("OpenAI API key is missing for tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°æœªæä¾›ã€‚"
    if not original_tags_str.strip():
        return "éŒ¯èª¤ï¼šåŸå§‹è¨ºæ–·æ¨™ç±¤ä¸èƒ½ç‚ºç©ºã€‚"
    if not user_description.strip():
        return "éŒ¯èª¤ï¼šä½¿ç”¨è€…æè¿°ä¸èƒ½ç‚ºç©ºã€‚"

    client = openai.OpenAI(api_key=api_key)

    system_prompt = """
æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„ GMAT è¨ºæ–·æ¨™ç±¤ä¿®å‰ªåŠ©æ‰‹ã€‚
æ‚¨çš„ä»»å‹™æ˜¯åˆ†æä½¿ç”¨è€…æä¾›çš„ã€ŒåŸå§‹è¨ºæ–·æ¨™ç±¤ã€ï¼ˆä»¥é€—è™Ÿ,åˆ†éš”ï¼‰ä»¥åŠä»–å€‘ã€Œå°é¡Œç›®çš„æè¿°æˆ–é‡åˆ°çš„å›°é›£ã€ã€‚
æ ¹æ“šä½¿ç”¨è€…çš„æè¿°ï¼Œå¾åŸå§‹æ¨™ç±¤ä¸­ç¯©é¸å‡º 1 è‡³ 2 å€‹æœ€èƒ½ç›´æ¥å°æ‡‰ä¸¦è§£é‡‹ä½¿ç”¨è€…æ‰€è¿°å›°é›£çš„æ ¸å¿ƒè¨ºæ–·æ¨™ç±¤ã€‚

è¼¸å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¿”å›ä¿®å‰ªå¾Œå»ºè­°çš„ 1 æˆ– 2 å€‹æ¨™ç±¤å…¨åï¼Œä¸è¦çœç•¥æˆç°¡åã€‚
- å¦‚æœæœ‰å¤šå€‹å»ºè­°æ¨™ç±¤ï¼Œè«‹ç”¨é€—è™Ÿå’Œä¸€å€‹ç©ºæ ¼ï¼ˆä¾‹å¦‚ï¼šæ¨™ç±¤1, æ¨™ç±¤2ï¼‰åˆ†éš”ã€‚
- å¦‚æœåŸå§‹æ¨™ç±¤å·²ç¶“å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰1-2å€‹ï¼‰ä¸”èˆ‡ä½¿ç”¨è€…æè¿°ç›¸é—œï¼Œå¯ä»¥ç›´æ¥è¿”å›åŸå§‹æ¨™ç±¤ï¼ˆæˆ–ç›¸é—œçš„éƒ¨åˆ†ï¼‰ã€‚
- å¦‚æœæ ¹æ“šä½¿ç”¨è€…æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æ²’æœ‰æ˜é¡¯ç›´æ¥ç›¸é—œçš„æ¨™ç±¤ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€Œæ ¹æ“šæ‚¨çš„æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æœªæ‰¾åˆ°ç›´æ¥å°æ‡‰çš„é …ç›®ã€‚ã€
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€å¼•è¨€æˆ–é¡å¤–çš„å®¢å¥—è©±ï¼Œåªéœ€æä¾›å»ºè­°çš„æ¨™ç±¤æˆ–ä¸Šè¿°æ˜ç¢ºçš„æç¤ºã€‚
"""
    
    user_content = f"åŸå§‹è¨ºæ–·æ¨™ç±¤ï¼š\n{original_tags_str}\n\nä½¿ç”¨è€…æè¿°ï¼š\n{user_description}"

    try:
        logging.info("Calling OpenAI ChatCompletion for tag trimming with model gpt-3.5-turbo.")
        response = client.chat.completions.create(
            model="gpt-4.1-mini", # Using a cost-effective and capable model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.2, # Lower temperature for more deterministic output
            max_tokens=100 # Expecting a short list of tags
        )
        
        trimmed_suggestion = response.choices[0].message.content
        
        if trimmed_suggestion and trimmed_suggestion.strip():
            logging.info(f"OpenAI tag trimming successful. Suggestion: {trimmed_suggestion}")
            return trimmed_suggestion.strip()
        else:
            logging.warning("OpenAI returned empty response for tag trimming.")
            return "AI æœªèƒ½æä¾›ä¿®å‰ªå»ºè­°ï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚"

    except openai.AuthenticationError:
        logging.error("OpenAI AuthenticationError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ã€‚"
    except openai.RateLimitError:
        logging.error("OpenAI RateLimitError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    except openai.APIConnectionError as e:
        logging.error(f"OpenAI APIConnectionError during tag trimming: {e}")
        return f"éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ã€‚"
    except openai.APITimeoutError:
        logging.error("OpenAI APITimeoutError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚è¶…æ™‚ã€‚"
    except openai.BadRequestError as e:
        logging.error(f"OpenAI BadRequestError during tag trimming: {e}")
        return f"éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ã€‚å¯èƒ½æ˜¯è¼¸å…¥å…§å®¹å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unknown OpenAI API error during tag trimming: {e}", exc_info=True)
        return f"èª¿ç”¨ OpenAI API æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚" 