"""
OpenAIæœå‹™æ¨¡çµ„
æä¾›èˆ‡OpenAI APIé€šä¿¡çš„åŠŸèƒ½
"""

import logging
import streamlit as st
import openai
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential
import os
from ..utils.rate_limiter import get_client_ip, check_rate_limit # Import rate limiting functions
from ..i18n import translate as t # Import translation function

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client
client = None
api_key_env = os.environ.get("OPENAI_API_KEY")
master_key_env = os.environ.get("MASTER_KEY")

# Decorator for retrying API calls
@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def chat_completion_request_with_retry(**kwargs):
    """Makes a request to the OpenAI API with retries on failure."""
    if not client:
        st.error("OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œç„¡æ³•è™•ç†è«‹æ±‚ã€‚è«‹æª¢æŸ¥é©—è­‰ç‹€æ…‹ã€‚")
        raise Exception("OpenAI client not initialized.") # Raise exception to stop retry
    
    # Rate Limiting Check
    ip_address = get_client_ip()
    if not check_rate_limit(ip_address):
        st.error(f"æŠ±æ­‰ï¼Œæ‚¨ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸å·²é”ä¸Šé™ ({check_rate_limit.__globals__['DAILY_LIMIT']}æ¬¡)ã€‚è«‹æ˜å¤©å†è©¦ã€‚")
        raise Exception("Rate limit exceeded") # This will stop the retry mechanism too

    try:
        response = client.chat.completions.create(**kwargs)
        return response
    except Exception as e:
        logger.error(f"OpenAI API request failed: {repr(e)}")
        st.warning(f"èˆ‡ OpenAI é€£ç·šæ™‚ç™¼ç”Ÿæš«æ™‚æ€§éŒ¯èª¤ï¼Œæ­£åœ¨é‡è©¦... ({repr(e)})")
        raise e # Reraise exception to trigger tenacity retry

def validate_master_key(input_key):
    """é©—è­‰æä¾›çš„master keyæ˜¯å¦èˆ‡ç’°å¢ƒè®Šé‡ä¸­çš„ç›¸ç¬¦"""
    if not master_key_env:
        logger.error("MASTER_KEYç’°å¢ƒè®Šé‡æœªè¨­ç½®")
        return False
    
    # ç°¡å–®çš„å­—ç¬¦ä¸²æ¯”è¼ƒï¼Œæ­£å¼ç’°å¢ƒå¯èƒ½éœ€è¦æ›´è¤‡é›œçš„é©—è­‰æ–¹æ³•
    return input_key == master_key_env

def initialize_openai_client_with_master_key(master_key):
    """ä½¿ç”¨master keyé©—è­‰ä¸¦åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯"""
    global client
    
    if not validate_master_key(master_key):
        logger.error("Invalid master key provided.")
        st.error("æä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚")
        client = None
        return False
    
    # å¦‚æœmaster keyé©—è­‰æˆåŠŸï¼Œä½¿ç”¨ç’°å¢ƒè®Šé‡ä¸­çš„API key
    if api_key_env:
        client = OpenAI(api_key=api_key_env)
        logger.info("OpenAI client initialized successfully with environment API key.")
        return True
    else:
        logger.error("OPENAI_API_KEY environment variable not set.")
        st.error("ç³»çµ±éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°æœªåœ¨ç’°å¢ƒè®Šé‡ä¸­è¨­ç½®ã€‚è«‹è¯çµ¡ç³»çµ±ç®¡ç†å“¡ã€‚")
        client = None
        return False

# ä¿ç•™åŸå§‹æ–¹æ³•ä»¥ç¢ºä¿å‘å¾Œå…¼å®¹ï¼Œä½†å…§éƒ¨æ”¹ç‚ºä½¿ç”¨æ–°æ–¹æ³•
def initialize_openai_client(api_key):
    """
    å·²æ£„ç”¨ï¼šç›´æ¥ä½¿ç”¨API keyåˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯
    ç¾åœ¨è½‰ç‚ºæª¢æŸ¥æ­¤API keyæ˜¯å¦ç‚ºæœ‰æ•ˆçš„master key
    """
    global client
    logger.warning("ä½¿ç”¨èˆŠæ–¹æ³•initialize_openai_clienté€²è¡Œå®¢æˆ¶ç«¯åˆå§‹åŒ–")
    return initialize_openai_client_with_master_key(api_key)

def get_openai_client():
    """Returns the initialized OpenAI client."""
    return client

def generate_response(system_prompt, user_prompt, model="gpt-4o-mini", temperature=0.7, max_tokens=1500):
    """
    Generates a response from OpenAI using the specified prompts and parameters.

    Args:
        system_prompt (str): The system message to guide the assistant.
        user_prompt (str): The user's message.
        model (str): The model to use (default: "gpt-4o-mini").
        temperature (float): Controls randomness (default: 0.7).
        max_tokens (int): Maximum number of tokens to generate (default: 1500).

    Returns:
        str: The generated response content, or None if an error occurs or client is not initialized.
    """
    if not client:
        logger.error("OpenAI client not initialized. Cannot generate response.")
        st.error("OpenAI client æœªè¨­ç½®ï¼Œè«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥æœ‰æ•ˆçš„ç®¡ç†é‡‘é‘°ã€‚")
        return None

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    logger.info(f"Sending request to OpenAI model {model} with max_tokens={max_tokens}")
    
    try:
        # Use the retry wrapper function for the actual API call
        response = chat_completion_request_with_retry(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        response_content = response.choices[0].message.content
        logger.info("Received response from OpenAI.")
        return response_content

    except Exception as e:
        # Handle rate limit exception if Option 2 was chosen in the wrapper
        if "Rate limit exceeded" in str(e): # str(e) here is for a specific string check, less likely to cause encoding error itself on common errors.
             logger.warning(f"Rate limit exceeded for IP (retrieved internally). User message already shown.")
             # Error message is already displayed by the wrapper via st.error
        else:
            logger.error(f"An unexpected error occurred during OpenAI API call: {repr(e)}")
            st.error(f"å‘¼å« OpenAI API æ™‚ç™¼ç”ŸéŒ¯èª¤: {repr(e)}")
        return None

def summarize_report_with_openai(report_markdown, api_key):
    """Attempts to summarize/reformat a report using OpenAI API."""
    # æª¢æŸ¥api_keyæ˜¯å¦æ˜¯æœ‰æ•ˆçš„master key
    global client
    original_client = client
    
    # å¦‚æœå·²é€šéé©—è­‰ä¸¦è¨­ç½®äº†å®¢æˆ¶ç«¯ï¼Œå‰‡ç›´æ¥ä½¿ç”¨
    if not client and api_key:
        if not validate_master_key(api_key):
            logging.warning("æä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚è·³éæ‘˜è¦è™•ç†ã€‚")
            return report_markdown
        elif not api_key_env:
            logging.warning("ç’°å¢ƒè®Šé‡ä¸­æœªè¨­ç½®OPENAI_API_KEYã€‚è·³éæ‘˜è¦è™•ç†ã€‚")
            return report_markdown
        else:
            # è‡¨æ™‚åˆå§‹åŒ–å®¢æˆ¶ç«¯ç”¨æ–¼æ­¤æ¬¡è«‹æ±‚
            temp_client = OpenAI(api_key=api_key_env)
    else:
        # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å®¢æˆ¶ç«¯
        temp_client = client

    if temp_client is None:
        logging.warning("ç„¡æ³•åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯ã€‚è·³éæ‘˜è¦è™•ç†ã€‚")
        return report_markdown

    ip_address = get_client_ip()
    if ip_address is None:
        st.error("IP ä½å€ç„¡æ³•ç¢ºå®šã€‚ç‚ºç¢ºä¿æœå‹™å®‰å…¨ï¼ŒAPI è«‹æ±‚ç„¡æ³•è™•ç†ã€‚è‹¥åœ¨æœ¬æ©ŸåŸ·è¡Œæ­¤ç‚ºæ­£å¸¸ç¾è±¡ã€‚")
        return report_markdown # Return original

    # Proceed with rate limiting only if IP is known
    if not check_rate_limit(ip_address):
        daily_limit = check_rate_limit.__globals__.get('DAILY_LIMIT', 'æ¯æ—¥')
        st.error(f"IP: {ip_address} - æŠ±æ­‰ï¼Œæ‚¨ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸å·²é”æ•´ç†å ±å‘Šæ–‡å­—çš„ä¸Šé™ ({daily_limit}æ¬¡)ã€‚è«‹æ˜å¤©å†è©¦ã€‚")
        return report_markdown

    try:
        system_prompt = """You are an assistant that reformats GMAT diagnostic reports. Use uniform heading 
levels: Level-2 headings for all major sections (## Section Title). Level-3 headings for subsections (### Su
bsection Title). Reformat content into clear Markdown tables where appropriate. Fix minor grammatical errors
 or awkward phrasing for readability. IMPORTANT: Do NOT add or remove any substantive information or conclus
ions. Only reformat and polish the existing text. Output strictly in Markdown format, without code blocks.
"""

        response = temp_client.chat.completions.create(
            model="gpt-4.1-nano", 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": report_markdown}
            ],
        )
        summarized_report = response.choices[0].message.content
        if summarized_report and summarized_report.strip():
             logging.info(f"OpenAI summarization successful for report starting with: {report_markdown[:50]}\n...")
             return summarized_report.strip()
        else:
             logging.warning("OpenAI returned empty response. Using original report.")
             st.warning("AI æ•´ç†å ±å‘Šæ™‚è¿”å›äº†ç©ºå…§å®¹ï¼Œå°‡ä½¿ç”¨åŸå§‹å ±å‘Šã€‚", icon="âš ï¸")
             return report_markdown

    except openai.AuthenticationError:
        st.warning("OpenAI API é©—è­‰å¤±æ•—ï¼Œç„¡æ³•æ•´ç†å ±å‘Šæ–‡å­—ã€‚è«‹æª¢æŸ¥ç³»çµ±ç®¡ç†å“¡è¨­å®šçš„APIé‡‘é‘°ã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError.")
        return report_markdown
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ (æœå‹™ç«¯é™åˆ¶)ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â³")
        logging.error("OpenAI RateLimitError.")
        return report_markdown
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({repr(e)})ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError: {repr(e)}")
        return report_markdown
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError.")
        return report_markdown
    except openai.BadRequestError as e:
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({repr(e)})ã€‚å¯èƒ½æ˜¯å ±å‘Šéé•·æˆ–æ ¼å¼å•é¡Œã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â—") 
        logging.error(f"OpenAI BadRequestError: {repr(e)}")
        return report_markdown
    except Exception as e:
        st.warning(f"èª¿ç”¨ OpenAI API æ•´ç†å ±å‘Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{repr(e)}ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="âš ï¸")
        logging.error(f"Unknown OpenAI API error during summarization: {repr(e)}", exc_info=True)
        return report_markdown
    finally:
        # ç¢ºä¿å…¨å±€å®¢æˆ¶ç«¯ç‹€æ…‹ä¸å—å½±éŸ¿
        client = original_client

def generate_ai_consolidated_report(report_dict, api_key):
    """Generates a consolidated report of suggestions and next steps using OpenAI o4-mini."""
    # æª¢æŸ¥api_keyæ˜¯å¦æ˜¯æœ‰æ•ˆçš„master key
    global client
    original_client = client
    
    # å¦‚æœæ²’æœ‰å ±å‘Šè³‡æ–™ï¼Œç›´æ¥è¿”å›
    if not report_dict:
        logging.warning("Report dictionary is empty. Skipping consolidated report generation.")
        return None
    
    # é©—è­‰master keyä¸¦ç²å–æœ‰æ•ˆçš„å®¢æˆ¶ç«¯
    if not client and api_key:
        if not validate_master_key(api_key):
            logging.warning("æä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚è·³éåŒ¯ç¸½å ±å‘Šç”Ÿæˆã€‚")
            return None
        elif not api_key_env:
            logging.warning("ç’°å¢ƒè®Šé‡ä¸­æœªè¨­ç½®OPENAI_API_KEYã€‚è·³éåŒ¯ç¸½å ±å‘Šç”Ÿæˆã€‚")
            return None
        else:
            # è‡¨æ™‚åˆå§‹åŒ–å®¢æˆ¶ç«¯ç”¨æ–¼æ­¤æ¬¡è«‹æ±‚
            temp_client = openai.OpenAI(api_key=api_key_env)
    else:
        # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å®¢æˆ¶ç«¯
        temp_client = client

    if temp_client is None:
        logging.warning("ç„¡æ³•åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯ã€‚è·³éåŒ¯ç¸½å ±å‘Šç”Ÿæˆã€‚")
        return None

    ip_address = get_client_ip()
    if ip_address is None:
        st.error("IP ä½å€ç„¡æ³•ç¢ºå®šã€‚ç‚ºç¢ºä¿æœå‹™å®‰å…¨ï¼ŒAI åŒ¯ç¸½å ±å‘Šç”Ÿæˆè«‹æ±‚ç„¡æ³•è™•ç†ã€‚è‹¥åœ¨æœ¬æ©ŸåŸ·è¡Œæ­¤ç‚ºæ­£å¸¸ç¾è±¡ã€‚")
        return None

    if not check_rate_limit(ip_address):
        daily_limit = check_rate_limit.__globals__.get('DAILY_LIMIT', 'æ¯æ—¥')
        st.error(f"IP: {ip_address} - æŠ±æ­‰ï¼Œæ‚¨ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸å·²é”ç”ŸæˆåŒ¯ç¸½å ±å‘Šçš„ä¸Šé™ ({daily_limit}æ¬¡)ã€‚è«‹æ˜å¤©å†è©¦ã€‚")
        return None

    # Construct the full input text from individual reports
    full_report_text = ""
    for subject, report in report_dict.items():
        if report:
            full_report_text += f"## {subject} Report:\n\n{report}\n\n---\n\n"

    if not full_report_text.strip():
        logging.warning("No valid reports found in the dictionary. Skipping consolidated report generation.")
        return None

    # Define the system prompt emphasizing strict extraction and formatting
    system_prompt = """You are an assistant that extracts specific sections from GMAT diagnostic reports and consolidates them into a single, structured document.
Your task is to identify and extract ONLY the sections related to '{t('openai_practice_suggestion')}' (Practice Suggestions) and '{t('openai_subsequent_action')}' (Next Steps, including reflection and secondary evidence gathering) for each subject (Q, V, DI) from the provided text.

**CRITICAL INSTRUCTIONS:**
1.  **Strict Extraction:** Only extract text explicitly under '{t('openai_practice_suggestion')}' or '{t('openai_subsequent_action')}' headings or clearly discussing these topics.
2.  **Subject Separation:** Keep the extracted information strictly separated under standardized headings for each subject: "## Q ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•", "## V ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•", "## DI ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•".
3.  **Complete Information:** Transfer ALL original text, data, details, parenthetical notes, specific question numbers, difficulty codes, time limits, percentages, scores, etc., accurately and completely. DO NOT SUMMARIZE OR OMIT ANY DETAILS.
4.  **Standardized Formatting:** Use Markdown format. Use Level-2 headings (##) for each subject as specified above. Use bullet points or numbered lists within each subject section as they appear in the original text, or to improve readability if appropriate, but maintain all original content.
5.  **No Additions:** Do not add any information, interpretation, or introductory/concluding text not present in the extracted sections.
6.  **Output Format:** Output only the consolidated Markdown report. Do not include any other text or commentary.
"""

    try:
        logging.info("Calling OpenAI responses.create with model o4-mini for consolidated report.")
        # WARNING: client.responses.create might be outdated. Consider migrating to chat.completions.create.
        response = temp_client.responses.create(
            model="o4-mini",
            input=f"""
System: {system_prompt}

User: å¾ä»¥ä¸‹ GMAT è¨ºæ–·å ±å‘Šä¸­æå–ç·´ç¿’å»ºè­°å’Œå¾ŒçºŒè¡Œå‹•éƒ¨åˆ†ï¼ŒæŒ‰ç§‘ç›®åˆ†é¡ä¸¦æ•´ç†æˆçµ±ä¸€æ ¼å¼ï¼š

{full_report_text}
""",
        )
        logging.info(f"OpenAI consolidated report response received. Status: {response.status}")

        if response.status == 'completed' and response.output:
            response_text = ""
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break

            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        response_text += content_block.text

            if not response_text:
                logging.error("OpenAI consolidated report response completed but no text found.")
                st.warning("AI ç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚", icon="âš ï¸")
                return None
            else:
                logging.info("Successfully generated consolidated report.")
                return response_text.strip()
        elif response.status == 'error':
            error_details = response.error if response.error else "Unknown error"
            logging.error(f"OpenAI API error (consolidated report): {repr(error_details)}")
            st.warning(f"AI ç”ŸæˆåŒ¯ç¸½å ±å‘Šæ™‚å‡ºéŒ¯ï¼š{repr(error_details)}", icon="â—")
            return None
        else:
            logging.error(f"OpenAI consolidated report status not completed or output empty. Status: {response.status}")
            st.warning(f"AI æœªèƒ½æˆåŠŸç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆç‹€æ…‹ï¼š{response.status}ï¼‰ã€‚", icon="âš ï¸")
            return None

    except openai.AuthenticationError:
        st.warning("OpenAI API é©—è­‰å¤±æ•—ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚è«‹æª¢æŸ¥ç³»çµ±ç®¡ç†å“¡è¨­å®šçš„APIé‡‘é‘°ã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError (consolidated report).")
        return None
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â³")
        logging.error("OpenAI RateLimitError (consolidated report).")
        return None
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({repr(e)})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError (consolidated report): {repr(e)}")
        return None
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError (consolidated report).")
        return None
    except openai.BadRequestError as e:
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({repr(e)})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â—")
        logging.error(f"OpenAI BadRequestError (consolidated report): {repr(e)}")
        return None
    except Exception as e:
        st.warning(f"ç”Ÿæˆ AI åŒ¯ç¸½å»ºè­°æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{repr(e)}", icon="âš ï¸")
        logging.error(f"Unknown error during consolidated report generation: {repr(e)}", exc_info=True)
        return None
    finally:
        # ç¢ºä¿å…¨å±€å®¢æˆ¶ç«¯ç‹€æ…‹ä¸å—å½±éŸ¿
        client = original_client

def get_chat_context(session_state, max_rows=100):
    """Get combined report and dataframe context for chat."""
    context = {
        "report": _get_combined_report_context(session_state),
        "dataframe": _get_dataframe_context(session_state, max_rows)
    }
    return context

def _get_combined_report_context(session_state):
    """Combines markdown reports from all subjects."""
    full_report = ""
    
    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¦åˆ†æ•¸/ç™¾åˆ†ä½è³‡è¨Šä¸¦æ·»åŠ åˆ°å ±å‘Šé–‹é ­
    if hasattr(session_state, 'total_score') and session_state.total_score:
        full_report += "## ç¸½é«”åˆ†æ•¸èˆ‡ç™¾åˆ†ä½\n\n"
        
        # æ·»åŠ å„ç§‘ç›®åˆ†æ•¸
        scores_text = []
        if hasattr(session_state, 'q_score') and session_state.q_score:
            scores_text.append(f"Q (Quantitative): {session_state.q_score}")
        if hasattr(session_state, 'v_score') and session_state.v_score:
            scores_text.append(f"V (Verbal): {session_state.v_score}")
        if hasattr(session_state, 'di_score') and session_state.di_score:
            scores_text.append(f"DI (Data Insights): {session_state.di_score}")
        if hasattr(session_state, 'total_score') and session_state.total_score:
            scores_text.append(f"Total: {session_state.total_score}")
            
        if scores_text:
            full_report += "### å„ç§‘ç›®åˆ†æ•¸\n\n"
            for score in scores_text:
                full_report += f"- {score}\n"
            full_report += "\n"
    
    # æ·»åŠ AIç¸½çµå»ºè­°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(session_state, 'consolidated_report_text') and session_state.consolidated_report_text:
        full_report += "## AI ç¸½çµå»ºè­°\n\n"
        full_report += session_state.consolidated_report_text
        full_report += "\n\n---\n\n"
    
    # æ·»åŠ å„ç§‘ç›®è¨ºæ–·å ±å‘Š
    if session_state.report_dict:
        for subject in ['Q', 'V', 'DI']:  # æ˜ç¢ºå®šç¾©ç§‘ç›®é †åº
            report = session_state.report_dict.get(subject)
            if report:
                full_report += f"## {subject} ç§‘è¨ºæ–·å ±å‘Š\n\n{report}\n\n---\n\n"
                
    # å¦‚æœå·²ç¶“å­˜åœ¨æ–°è¨ºæ–·å ±å‘Šï¼ˆå¾ç·¨è¼¯æ¨™ç±¤ç”Ÿæˆï¼‰ï¼Œå‰‡æ·»åŠ åˆ°å ±å‘Šä¸­
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None:
        try:
            # ä½¿ç”¨å…§éƒ¨å‡½æ•¸ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šï¼Œé¿å…å¾ªç’°å°å…¥
            df = session_state.editable_diagnostic_df
            if 'generated_new_diagnostic_report' in session_state:
                # ä½¿ç”¨å·²ç”Ÿæˆçš„å ±å‘Š
                new_report = session_state.generated_new_diagnostic_report
                full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                full_report += new_report
                full_report += "\n\n"
            else:
                # å˜—è©¦ç”Ÿæˆæ–°å ±å‘Š
                logging.info("å˜—è©¦ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Š...")
                # æˆ‘å€‘å°‡ä½¿ç”¨å‹•æ…‹å°å…¥ä¾†é¿å…å¾ªç’°å°å…¥å•é¡Œ
                import importlib
                try:
                    results_display_module = importlib.import_module('gmat_diagnosis_app.ui.results_display')
                    generate_report_func = getattr(results_display_module, 'generate_new_diagnostic_report')
                    
                    new_report = generate_report_func(df)
                    if new_report:
                        # ä¿å­˜åˆ°session_stateä»¥ä¾¿é‡è¤‡ä½¿ç”¨
                        session_state.generated_new_diagnostic_report = new_report
                        full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                        full_report += new_report
                        full_report += "\n\n"
                except (ImportError, AttributeError) as ie:
                    logging.error(f"ç„¡æ³•å°å…¥æˆ–ä½¿ç”¨generate_new_diagnostic_reportå‡½æ•¸: {repr(ie)}")
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šæ™‚å‡ºéŒ¯: {repr(e)}")
    
    return full_report.strip()

def _get_dataframe_context(session_state, max_rows=100):
    """Converts the processed dataframe to a string format (markdown) for context."""
    # å„ªå…ˆä½¿ç”¨ä¿®å‰ªæ¨™ç±¤å¾Œçš„æ•¸æ“šè¡¨æ ¼ (å¦‚æœå­˜åœ¨)
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None and not session_state.editable_diagnostic_df.empty:
        df_context = session_state.editable_diagnostic_df.copy()
        logging.info(f"æº–å‚™è½‰æ›ä¿®å‰ªæ¨™ç±¤å¾Œçš„è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    elif session_state.processed_df is not None and not session_state.processed_df.empty:
        df_context = session_state.processed_df.copy()
        logging.info(f"æº–å‚™è½‰æ›åŸå§‹è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    else:
        logging.warning("è¨ºæ–·è©¦ç®—è¡¨ç‚ºç©ºæˆ–ä¸å­˜åœ¨")
        return "(ç„¡è©³ç´°æ•¸æ“šè¡¨æ ¼)"
        
    try:
        # é¸æ“‡é—œéµåˆ—ä»¥æé«˜å¯è®€æ€§ï¼Œæ’é™¤ä¸å¿…è¦çš„åˆ—
        important_cols = [
            'Subject', 'question_position', 'question_type', 'question_fundamental_skill',
            'content_domain', 'is_invalid', 'is_correct', 'question_time',
            'time_performance_category', 'diagnostic_params_list'
        ]
        
        # åªä¿ç•™å­˜åœ¨æ–¼ DataFrame ä¸­çš„åˆ—
        cols_to_use = [col for col in important_cols if col in df_context.columns]
        
        # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—ï¼ˆä½†å„ªå…ˆä½¿ç”¨ä¸Šé¢å®šç¾©çš„é—œéµåˆ—ï¼‰
        for col in df_context.columns:
            if col not in cols_to_use and col not in ['raw_content']:
                cols_to_use.append(col)
        
        logging.info(f"é¸æ“‡çš„åˆ—: {', '.join(cols_to_use)}")
        
        # å¦‚æœæœ‰åˆ—å¯ç”¨ï¼Œå‰‡ä½¿ç”¨é€™äº›åˆ—ï¼›å¦å‰‡ä½¿ç”¨æ‰€æœ‰åˆ—
        if cols_to_use:
            df_context = df_context[cols_to_use]
        
        # Convert boolean columns to Yes/No for better readability for the LLM
        bool_cols = df_context.select_dtypes(include=bool).columns
        for col in bool_cols:
            df_context[col] = df_context[col].map({True: 'Yes', False: 'No'})
            logging.info(f"å·²å°‡å¸ƒçˆ¾åˆ— '{col}' è½‰æ›ç‚º Yes/No æ ¼å¼")

        # Convert list column to string
        if 'diagnostic_params_list' in df_context.columns:
            df_context['diagnostic_params_list'] = df_context['diagnostic_params_list'].apply(
                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x)
            )
            logging.info("å·²å°‡ 'diagnostic_params_list' åˆ—è½‰æ›ç‚ºå­—ç¬¦ä¸²æ ¼å¼")

        # Limit rows to avoid excessive context length
        if len(df_context) > max_rows:
            logging.info(f"æ•¸æ“šè¡Œæ•¸è¶…éé™åˆ¶ ({len(df_context)} > {max_rows})ï¼Œåªå–å‰ {max_rows} è¡Œ")
            df_context_str = df_context.head(max_rows).to_markdown(index=False)
            df_context_str += f"\n... ({t('openai_showing_first')} {max_rows} {t('openai_rows_total')} {len(df_context)} {t('openai_rows')})"
        else:
            logging.info(f"è½‰æ›å…¨éƒ¨ {len(df_context)} è¡Œæ•¸æ“šç‚º markdown æ ¼å¼")
            df_context_str = df_context.to_markdown(index=False)
        
        logging.info(f"æˆåŠŸè½‰æ›è¨ºæ–·è©¦ç®—è¡¨ï¼Œè¼¸å‡ºé•·åº¦ç´„ {len(df_context_str)} å­—ç¬¦")
        return df_context_str
    except Exception as e:
        error_msg = f"Error converting dataframe to markdown context: {repr(e)}"
        logging.error(error_msg, exc_info=True)
        return f"(ç„¡æ³•è½‰æ›è©³ç´°æ•¸æ“šè¡¨æ ¼: {repr(e)})"

def get_openai_response(current_chat_history, report_context, dataframe_context, api_key):
    """Get response from OpenAI based on chat history and context, using o4-mini."""
    # æª¢æŸ¥api_keyæ˜¯å¦æ˜¯æœ‰æ•ˆçš„master key
    global client
    original_client = client
    
    # é©—è­‰master keyä¸¦ç²å–æœ‰æ•ˆçš„å®¢æˆ¶ç«¯
    if not client and api_key:
        if not validate_master_key(api_key):
            logging.error("æä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚ç„¡æ³•ä½¿ç”¨èŠå¤©åŠŸèƒ½ã€‚")
            return "éŒ¯èª¤ï¼šæä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚", None
        elif not api_key_env:
            logging.error("ç’°å¢ƒè®Šé‡ä¸­æœªè¨­ç½®OPENAI_API_KEYã€‚ç„¡æ³•ä½¿ç”¨èŠå¤©åŠŸèƒ½ã€‚")
            return "éŒ¯èª¤ï¼šç³»çµ±æœªé…ç½®OpenAI APIé‡‘é‘°ã€‚è«‹è¯çµ¡ç³»çµ±ç®¡ç†å“¡ã€‚", None
        else:
            # è‡¨æ™‚åˆå§‹åŒ–å®¢æˆ¶ç«¯ç”¨æ–¼æ­¤æ¬¡è«‹æ±‚
            temp_client = openai.OpenAI(api_key=api_key_env)
    else:
        # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å®¢æˆ¶ç«¯
        temp_client = client

    if temp_client is None:
        logging.error("ç„¡æ³•åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯ã€‚ç„¡æ³•ä½¿ç”¨èŠå¤©åŠŸèƒ½ã€‚")
        return "éŒ¯èª¤ï¼šOpenAIå®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—ã€‚è«‹ç¢ºèªç®¡ç†é‡‘é‘°å’Œç³»çµ±è¨­ç½®ã€‚", None

    ip_address = get_client_ip()
    if ip_address is None:
        st.error("IP ä½å€ç„¡æ³•ç¢ºå®šã€‚ç‚ºç¢ºä¿æœå‹™å®‰å…¨ï¼ŒèŠå¤©è«‹æ±‚ç„¡æ³•è™•ç†ã€‚è‹¥åœ¨æœ¬æ©ŸåŸ·è¡Œæ­¤ç‚ºæ­£å¸¸ç¾è±¡ã€‚")
        return "éŒ¯èª¤ï¼šIP ä½å€ç„¡æ³•ç¢ºå®šï¼Œç„¡æ³•è™•ç†è«‹æ±‚ã€‚", None

    if not check_rate_limit(ip_address):
        daily_limit = check_rate_limit.__globals__.get('DAILY_LIMIT', 'æ¯æ—¥')
        error_message = f"IP: {ip_address} - æŠ±æ­‰ï¼Œæ‚¨ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸å·²é”èŠå¤©åŠŸèƒ½çš„ä¸Šé™ ({daily_limit}æ¬¡)ã€‚è«‹æ˜å¤©å†è©¦ã€‚"
        st.error(error_message)
        return error_message, None

    # Determine if this is the first user message in the session for this specific chat.
    # A simple heuristic: if chat history has only one user message (the current one being processed)
    # or two messages (system (if we decide to add one initially), user), it's likely the first real interaction.
    # For robustness, let's count user messages.
    user_message_count = sum(1 for msg in current_chat_history if msg['role'] == 'user')

    is_first_exchange = user_message_count <= 1

    if is_first_exchange:
        # First exchange: include the full report and dataframe context
        system_prompt_content = f"""You are an expert GMAT diagnostic assistant. 
Your goal is to help the user understand their GMAT performance based on the provided diagnostic report and data.

Here is the user's GMAT diagnostic report:
--- BEGIN REPORT ---
{report_context}
--- END REPORT ---

Here is the user's detailed diagnostic data (questions, answers, time spent, etc.):
--- BEGIN DATA ---
{dataframe_context}
--- END DATA ---

Answer the user's questions based on this report and data. Be concise, helpful, and refer to specific details from the report or data when possible.
If the user asks for something not in the report or data, state that the information is unavailable.
Maintain a conversational and supportive tone.
"""
    else:
        # Subsequent exchanges: use a simpler system prompt, assuming context is maintained by conversation history.
        system_prompt_content = """You are an expert GMAT diagnostic assistant. 
Continue the conversation based on the provided history. Be concise, helpful, and refer to specific details from the report or data if they were mentioned earlier or are relevant.
If the user asks for something not in the report or data that wasn't previously established, state that the information is unavailable.
Maintain a conversational and supportive tone.
"""

    messages_for_api = [
        {"role": "system", "content": system_prompt_content}
    ]
    # Append the actual chat history (user and assistant messages)
    # We need to ensure the history format is correct for the API
    for message in current_chat_history:
        role = message.get("role")
        content = message.get("content", "") # Default to empty string if content is missing
        if not isinstance(content, str):
            content = str(content) # Ensure content is a string

        if role:
            # Only include 'role' and 'content' keys in the message sent to the API
            messages_for_api.append({"role": role, "content": content})
        # No longer need the separate case for `elif message.get("role")` with empty content,
        # as `message.get("content", "")` handles it.

    # Get previous response_id if available from the last assistant message
    previous_response_id = None
    if len(current_chat_history) > 1:
        # Iterate backwards to find the last assistant message with a response_id
        for i in range(len(current_chat_history) - 1, -1, -1):
            msg = current_chat_history[i]
            if msg.get("role") == "assistant" and msg.get("response_id"):
                previous_response_id = msg.get("response_id")
                logging.info(f"Found previous_response_id: {previous_response_id[:10]}...")
                break 

    try:
        logging.info(f"Calling OpenAI o4-mini. is_first_exchange: {is_first_exchange}. History length: {len(messages_for_api)}. Previous_id: {previous_response_id[:10] if previous_response_id else 'None'}")
        
        # Debug: Log the system prompt being used
        if is_first_exchange:
            logging.info("Sending request with full context in system prompt.")
        else:
            logging.info("Sending request with simplified system prompt.")

        # WARNING: client.responses.create might be outdated. Consider migrating to chat.completions.create.
        response = temp_client.responses.create(
            model="o4-mini", 
            input=messages_for_api, 
            previous_response_id=previous_response_id,
        )
        logging.info(f"OpenAI response status: {response.status}")

        if response.status == 'completed' and response.output:
            ai_response_text = ""
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break
            
            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        ai_response_text += content_block.text

            if not ai_response_text:
                logging.error("OpenAI response completed but no text found.")
                return "AI å›æ‡‰ç‚ºç©ºï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", None
            
            new_response_id = response.id
            logging.info(f"OpenAI response successful. New response_id: {new_response_id[:10] if new_response_id else 'N/A'}...")
            return ai_response_text.strip(), new_response_id
        
        elif response.status == 'error':
            error_details = response.error if response.error else "Unknown error"
            logging.error(f"OpenAI API error: {repr(error_details)}")
            return f"AI æœå‹™å‡ºéŒ¯: {repr(error_details)}", None
        else:
            logging.error(f"OpenAI response status not completed or output empty. Status: {response.status}")
            return f"AI æœªèƒ½æˆåŠŸå›æ‡‰ (ç‹€æ…‹: {response.status})ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", None

    except openai.AuthenticationError:
        error_msg = "OpenAI APIé©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç³»çµ±ç®¡ç†å“¡è¨­å®šçš„APIé‡‘é‘°ã€‚"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.RateLimitError:
        error_msg = "OpenAI APIè«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.APIConnectionError as e:
        error_msg = f"é€£æ¥OpenAI APIæ™‚å‡ºéŒ¯: {repr(e)}"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.APITimeoutError:
        error_msg = "OpenAI APIè«‹æ±‚è¶…æ™‚"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.BadRequestError as e:
        error_msg = f"OpenAI APIè«‹æ±‚ç„¡æ•ˆ: {repr(e)}"
        logging.error(error_msg)
        return error_msg, None
        
    except Exception as e:
        error_msg = f"èˆ‡OpenAI APIé€šè¨Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {repr(e)}"
        logging.error(error_msg, exc_info=True)
        return error_msg, None
    finally:
        # ç¢ºä¿å…¨å±€å®¢æˆ¶ç«¯ç‹€æ…‹ä¸å—å½±éŸ¿
        client = original_client

def trim_diagnostic_tags_with_openai(original_tags_str: str, user_description: str, api_key: str) -> str:
    """
    Uses OpenAI to suggest a trimmed list of 1-2 most relevant diagnostic tags
    based on original tags and user's description of their difficulty.

    Args:
        original_tags_str: A string containing the original diagnostic tags (e.g., comma-separated or list-like).
        user_description: User's description of their difficulty with the question.
        api_key: OpenAI API key or master key for authentication.

    Returns:
        A string containing the AI's suggested trimmed tags, or an error message.
    """
    # æª¢æŸ¥api_keyæ˜¯å¦æ˜¯æœ‰æ•ˆçš„master key
    global client
    original_client = client
    
    # åŸºæœ¬æª¢æŸ¥
    if not original_tags_str.strip():
        return "éŒ¯èª¤ï¼šåŸå§‹è¨ºæ–·æ¨™ç±¤ä¸èƒ½ç‚ºç©ºã€‚"
    if not user_description.strip():
        return "éŒ¯èª¤ï¼šä½¿ç”¨è€…æè¿°ä¸èƒ½ç‚ºç©ºã€‚"
    
    # ä¿®æ”¹é©—è­‰é‚è¼¯ï¼šå¦‚æœapi_keyç‚ºç©ºï¼Œç›´æ¥ä½¿ç”¨ç’°å¢ƒè®Šé‡çš„API key
    if not api_key:
        # ç›´æ¥ä½¿ç”¨ç’°å¢ƒè®Šé‡ä¸­çš„API keyï¼Œç„¡éœ€master_keyé©—è­‰
        if not api_key_env:
            logging.warning("ç’°å¢ƒè®Šé‡ä¸­æœªè¨­ç½®OPENAI_API_KEYã€‚ç„¡æ³•åŸ·è¡Œæ¨™ç±¤ä¿®å‰ªã€‚")
            return "éŒ¯èª¤ï¼šç³»çµ±æœªé…ç½®OpenAI APIé‡‘é‘°ã€‚è«‹è¯çµ¡ç³»çµ±ç®¡ç†å“¡ã€‚"
        # ç›´æ¥åˆå§‹åŒ–å®¢æˆ¶ç«¯ç”¨æ–¼æ­¤æ¬¡è«‹æ±‚
        temp_client = openai.OpenAI(api_key=api_key_env)
    else:
        # å¦‚æœæä¾›äº†api_keyï¼Œé€²è¡ŒåŸæœ‰çš„master_keyé©—è­‰é‚è¼¯
        if not client and api_key:
            if not validate_master_key(api_key):
                logging.warning("æä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚ç„¡æ³•åŸ·è¡Œæ¨™ç±¤ä¿®å‰ªã€‚")
                return "éŒ¯èª¤ï¼šæä¾›çš„ç®¡ç†é‡‘é‘°ç„¡æ•ˆã€‚"
            elif not api_key_env:
                logging.warning("ç’°å¢ƒè®Šé‡ä¸­æœªè¨­ç½®OPENAI_API_KEYã€‚ç„¡æ³•åŸ·è¡Œæ¨™ç±¤ä¿®å‰ªã€‚")
                return "éŒ¯èª¤ï¼šç³»çµ±æœªé…ç½®OpenAI APIé‡‘é‘°ã€‚è«‹è¯çµ¡ç³»çµ±ç®¡ç†å“¡ã€‚"
            else:
                # è‡¨æ™‚åˆå§‹åŒ–å®¢æˆ¶ç«¯ç”¨æ–¼æ­¤æ¬¡è«‹æ±‚
                temp_client = openai.OpenAI(api_key=api_key_env)
        else:
            # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å®¢æˆ¶ç«¯
            temp_client = client

    if temp_client is None:
        logging.warning("ç„¡æ³•åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯ã€‚ç„¡æ³•åŸ·è¡Œæ¨™ç±¤ä¿®å‰ªã€‚")
        return "éŒ¯èª¤ï¼šOpenAIå®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—ã€‚è«‹ç¢ºèªç®¡ç†é‡‘é‘°å’Œç³»çµ±è¨­ç½®ã€‚"
        
    ip_address = get_client_ip()
    if ip_address is None:
        st.error("IP ä½å€ç„¡æ³•ç¢ºå®šã€‚ç‚ºç¢ºä¿æœå‹™å®‰å…¨ï¼Œä¿®å‰ªæ¨™ç±¤è«‹æ±‚ç„¡æ³•è™•ç†ã€‚è‹¥åœ¨æœ¬æ©ŸåŸ·è¡Œæ­¤ç‚ºæ­£å¸¸ç¾è±¡ã€‚")
        return "éŒ¯èª¤ï¼šIP ä½å€ç„¡æ³•ç¢ºå®šï¼Œç„¡æ³•è™•ç†è«‹æ±‚ã€‚"

    if not check_rate_limit(ip_address):
        daily_limit = check_rate_limit.__globals__.get('DAILY_LIMIT', 'æ¯æ—¥')
        error_message = f"IP: {ip_address} - æŠ±æ­‰ï¼Œæ‚¨ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸å·²é”ä¿®å‰ªæ¨™ç±¤åŠŸèƒ½çš„ä¸Šé™ ({daily_limit}æ¬¡)ã€‚è«‹æ˜å¤©å†è©¦ã€‚"
        st.error(error_message)
        return error_message

    system_prompt = """
æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„ GMAT è¨ºæ–·æ¨™ç±¤ä¿®å‰ªåŠ©æ‰‹ã€‚
æ‚¨çš„ä»»å‹™æ˜¯åˆ†æä½¿ç”¨è€…æä¾›çš„ã€ŒåŸå§‹è¨ºæ–·æ¨™ç±¤ã€ï¼ˆä»¥é€—è™Ÿ,åˆ†éš”ï¼‰ä»¥åŠä»–å€‘ã€Œå°é¡Œç›®çš„æè¿°æˆ–é‡åˆ°çš„å›°é›£ã€ã€‚
æ ¹æ“šä½¿ç”¨è€…çš„æè¿°ï¼Œå¾åŸå§‹æ¨™ç±¤ä¸­ç¯©é¸å‡º 1 è‡³ 2 å€‹æœ€èƒ½ç›´æ¥å°æ‡‰ä¸¦è§£é‡‹ä½¿ç”¨è€…æ‰€è¿°å›°é›£çš„æ ¸å¿ƒè¨ºæ–·æ¨™ç±¤ã€‚

è¼¸å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¿”å›ä¿®å‰ªå¾Œå»ºè­°çš„ 1 æˆ– 2 å€‹æ¨™ç±¤å…¨åï¼Œä¸è¦çœç•¥æˆç°¡åã€‚
- å¦‚æœæœ‰å¤šå€‹å»ºè­°æ¨™ç±¤ï¼Œè«‹ç”¨é€—è™Ÿå’Œä¸€å€‹ç©ºæ ¼ï¼ˆä¾‹å¦‚ï¼šæ¨™ç±¤1, æ¨™ç±¤2ï¼‰åˆ†éš”ã€‚
- å¦‚æœåŸå§‹æ¨™ç±¤å·²ç¶“å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰1-2å€‹ï¼‰ä¸”èˆ‡ä½¿ç”¨è€…æè¿°ç›¸é—œï¼Œå¯ä»¥ç›´æ¥è¿”å›åŸå§‹æ¨™ç±¤ï¼ˆæˆ–ç›¸é—œçš„éƒ¨åˆ†ï¼‰ã€‚
- å¦‚æœæ ¹æ“šä½¿ç”¨è€…æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æ²’æœ‰æ˜é¡¯ç›´æ¥ç›¸é—œçš„æ¨™ç±¤ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€Œæ ¹æ“šæ‚¨çš„æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æœªæ‰¾åˆ°ç›´æ¥å°æ‡‰çš„é …ç›®ã€‚ã€
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€å¼•è¨€æˆ–é¡å¤–çš„å®¢å¥—è©±ï¼Œåªéœ€æä¾›å»ºè­°çš„æ¨™ç±¤æˆ–ä¸Šè¿°æ˜ç¢ºçš„æç¤ºã€‚
"""
    
    user_content = f"{t('openai_original_diagnostic_tags')}ï¼š\n{original_tags_str}\n\n{t('openai_user_description')}ï¼š\n{user_description}"

    try:
        logging.info("Calling OpenAI ChatCompletion for tag trimming with model gpt-4.1-mini.")
        response = temp_client.chat.completions.create(
            model="gpt-4.1-mini", 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.2, 
            max_tokens=100 
        )
        
        trimmed_suggestion = response.choices[0].message.content
        
        if trimmed_suggestion and trimmed_suggestion.strip():
            logging.info(f"OpenAI tag trimming successful. Suggestion: {trimmed_suggestion}")
            return trimmed_suggestion.strip()
        else:
            logging.warning("OpenAI returned empty response for tag trimming.")
            return "AI æœªèƒ½æä¾›ä¿®å‰ªå»ºè­°ï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚"

    except openai.AuthenticationError:
        logging.error("OpenAI AuthenticationError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API é©—è­‰å¤±æ•—ã€‚è«‹æª¢æŸ¥ç³»çµ±ç®¡ç†å“¡è¨­å®šçš„APIé‡‘é‘°ã€‚"
    except openai.RateLimitError:
        logging.error("OpenAI RateLimitError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    except openai.APIConnectionError as e:
        logging.error(f"OpenAI APIConnectionError during tag trimming: {repr(e)}")
        return f"éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è‡³ OpenAI API ({repr(e)})ã€‚"
    except openai.APITimeoutError:
        logging.error("OpenAI APITimeoutError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚è¶…æ™‚ã€‚"
    except openai.BadRequestError as e:
        logging.error(f"OpenAI BadRequestError during tag trimming: {repr(e)}")
        return f"éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚ç„¡æ•ˆ ({repr(e)})ã€‚å¯èƒ½æ˜¯è¼¸å…¥å…§å®¹å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unknown OpenAI API error during tag trimming: {repr(e)}", exc_info=True)
        return f"èª¿ç”¨ OpenAI API ä¿®å‰ªæ¨™ç±¤æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{repr(e)}ã€‚"
    finally:
        # ç¢ºä¿å…¨å±€å®¢æˆ¶ç«¯ç‹€æ…‹ä¸å—å½±éŸ¿
        client = original_client 