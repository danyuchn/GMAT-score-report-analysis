"""
OpenAIæœå‹™æ¨¡çµ„
æä¾›èˆ‡OpenAI APIé€šä¿¡çš„åŠŸèƒ½
"""

import logging
import streamlit as st
import openai

def summarize_report_with_openai(report_markdown, api_key):
    """Attempts to summarize/reformat a report using OpenAI API."""
    if not api_key:
        logging.warning("OpenAI API key is missing. Skipping summarization.")
        return report_markdown # Return original if no key

    try:
        # Ensure openai library is accessible
        client = openai.OpenAI(api_key=api_key) # Initialize client
        system_prompt = """You are an assistant that reformats GMAT diagnostic reports. Use uniform heading levels: Level-2 headings for all major sections (## Section Title). Level-3 headings for subsections (### Subsection Title). Reformat content into clear Markdown tables where appropriate. Fix minor grammatical errors or awkward phrasing for readability. IMPORTANT: Do NOT add or remove any substantive information or conclusions. Only reformat and polish the existing text. Output strictly in Markdown format, without code blocks.  """

        response = client.chat.completions.create(
            model="gpt-4.1-nano", # Use the specified model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": report_markdown}
            ],
            # temperature=0.2 # Removed: o4-mini only supports default (1)
        )
        summarized_report = response.choices[0].message.content
        # Basic check if response seems valid (not empty)
        if summarized_report and summarized_report.strip():
             logging.info(f"OpenAI summarization successful for report starting with: {report_markdown[:50]}...")
             return summarized_report.strip()
        else:
             logging.warning("OpenAI returned empty response. Using original report.")
             st.warning("AI æ•´ç†å ±å‘Šæ™‚è¿”å›äº†ç©ºå…§å®¹ï¼Œå°‡ä½¿ç”¨åŸå§‹å ±å‘Šã€‚", icon="âš ï¸")
             return report_markdown

    except openai.AuthenticationError:
        st.warning("OpenAI API Key ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•æ•´ç†å ±å‘Šæ–‡å­—ã€‚è«‹æª¢æŸ¥å´é‚Šæ¬„è¼¸å…¥æˆ–ç’°å¢ƒè®Šæ•¸ã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError.")
        return report_markdown
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â³")
        logging.error("OpenAI RateLimitError.")
        return report_markdown
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError: {e}")
        return report_markdown
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError.")
        return report_markdown
    except openai.BadRequestError as e:
        # Often happens with context length issues or invalid requests
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ã€‚å¯èƒ½æ˜¯å ±å‘Šéé•·æˆ–æ ¼å¼å•é¡Œã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â—") # Use valid emoji
        logging.error(f"OpenAI BadRequestError: {e}")
        return report_markdown
    except Exception as e:
        st.warning(f"èª¿ç”¨ OpenAI API æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="âš ï¸")
        logging.error(f"Unknown OpenAI API error: {e}", exc_info=True)
        return report_markdown

def generate_ai_consolidated_report(report_dict, api_key):
    """Generates a consolidated report of suggestions and next steps using OpenAI o4-mini.

    Args:
        report_dict (dict): Dictionary containing the diagnostic reports for each subject (Q, V, DI).
        api_key (str): OpenAI API key.

    Returns:
        str: The generated consolidated report in Markdown, or None if an error occurs.
    """
    if not api_key:
        logging.warning("OpenAI API key missing. Skipping consolidated report generation.")
        return None
    if not report_dict:
        logging.warning("Report dictionary is empty. Skipping consolidated report generation.")
        return None

    client = openai.OpenAI(api_key=api_key)

    # Construct the full input text from individual reports
    full_report_text = ""
    for subject, report in report_dict.items():
        if report:
            full_report_text += f"## {subject} Report:\n\n{report}\n\n---\n\n"

    if not full_report_text.strip():
        logging.warning("No valid reports found in the dictionary. Skipping consolidated report generation.")
        return None

    # Define the system prompt emphasizing strict extraction and formatting
    system_prompt = """You are an assistant that extracts specific sections from GMAT diagnostic reports and consolidates them into a single, structured document.
Your task is to identify and extract ONLY the sections related to 'ç·´ç¿’å»ºè­°' (Practice Suggestions) and 'å¾ŒçºŒè¡Œå‹•' (Next Steps, including reflection and secondary evidence gathering) for each subject (Q, V, DI) from the provided text.

**CRITICAL INSTRUCTIONS:**
1.  **Strict Extraction:** Only extract text explicitly under 'ç·´ç¿’å»ºè­°' or 'å¾ŒçºŒè¡Œå‹•' headings or clearly discussing these topics.
2.  **Subject Separation:** Keep the extracted information strictly separated under standardized headings for each subject: `## Q ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`, `## V ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`, `## DI ç§‘ç›®å»ºè­°èˆ‡è¡Œå‹•`.
3.  **Complete Information:** Transfer ALL original text, data, details, parenthetical notes, specific question numbers, difficulty codes, time limits, percentages, scores, etc., accurately and completely. DO NOT SUMMARIZE OR OMIT ANY DETAILS.
4.  **Standardized Formatting:** Use Markdown format. Use Level-2 headings (##) for each subject as specified above. Use bullet points or numbered lists within each subject section as they appear in the original text, or to improve readability if appropriate, but maintain all original content.
5.  **No Additions:** Do not add any information, interpretation, or introductory/concluding text not present in the extracted sections.
6.  **Output Format:** Output only the consolidated Markdown report. Do not include any other text or commentary.
"""

    try:
        logging.info("Calling OpenAI responses.create with model o4-mini for consolidated report.")
        response = client.responses.create(
            model="o4-mini",
            input=f"""
System: {system_prompt}

User: å¾ä»¥ä¸‹ GMAT è¨ºæ–·å ±å‘Šä¸­æå–ç·´ç¿’å»ºè­°å’Œå¾ŒçºŒè¡Œå‹•éƒ¨åˆ†ï¼ŒæŒ‰ç§‘ç›®åˆ†é¡ä¸¦æ•´ç†æˆçµ±ä¸€æ ¼å¼ï¼š

{full_report_text}
""",
            # No prompt parameter - incorporate system instructions into input instead
            # No previous_response_id needed for this one-off task
            # max_output_tokens=1000 # Optional: Set a limit if needed
        )
        logging.info(f"OpenAI consolidated report response received. Status: {response.status}")

        if response.status == 'completed' and response.output:
            response_text = ""
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break

            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        response_text += content_block.text

            if not response_text:
                logging.error("OpenAI consolidated report response completed but no text found.")
                st.warning("AI ç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚", icon="âš ï¸")
                return None
            else:
                logging.info("Successfully generated consolidated report.")
                return response_text.strip()
        elif response.status == 'error':
            error_details = response.error if response.error else "Unknown error"
            logging.error(f"OpenAI API error (consolidated report): {error_details}")
            st.warning(f"AI ç”ŸæˆåŒ¯ç¸½å ±å‘Šæ™‚å‡ºéŒ¯ï¼š{error_details}", icon="â—")
            return None
        else:
            logging.error(f"OpenAI consolidated report status not completed or output empty. Status: {response.status}")
            st.warning(f"AI æœªèƒ½æˆåŠŸç”ŸæˆåŒ¯ç¸½å ±å‘Šï¼ˆç‹€æ…‹ï¼š{response.status}ï¼‰ã€‚", icon="âš ï¸")
            return None

    except openai.AuthenticationError:
        st.warning("OpenAI API Key ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError (consolidated report).")
        return None
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â³")
        logging.error("OpenAI RateLimitError (consolidated report).")
        return None
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError (consolidated report): {e}")
        return None
    except openai.APITimeoutError:
        st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â±ï¸")
        logging.error("OpenAI APITimeoutError (consolidated report).")
        return None
    except openai.BadRequestError as e:
        st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ï¼Œç„¡æ³•ç”ŸæˆåŒ¯ç¸½å ±å‘Šã€‚", icon="â—")
        logging.error(f"OpenAI BadRequestError (consolidated report): {e}")
        return None
    except Exception as e:
        st.warning(f"ç”Ÿæˆ AI åŒ¯ç¸½å»ºè­°æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}", icon="âš ï¸")
        logging.error(f"Unknown error during consolidated report generation: {e}", exc_info=True)
        return None

def get_chat_context(session_state, max_rows=100):
    """Get combined report and dataframe context for chat."""
    context = {
        "report": _get_combined_report_context(session_state),
        "dataframe": _get_dataframe_context(session_state, max_rows)
    }
    return context

def _get_combined_report_context(session_state):
    """Combines markdown reports from all subjects."""
    full_report = ""
    
    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¦åˆ†æ•¸/ç™¾åˆ†ä½è³‡è¨Šä¸¦æ·»åŠ åˆ°å ±å‘Šé–‹é ­
    if hasattr(session_state, 'total_score') and session_state.total_score:
        full_report += "## ç¸½é«”åˆ†æ•¸èˆ‡ç™¾åˆ†ä½\n\n"
        
        # æ·»åŠ å„ç§‘ç›®åˆ†æ•¸
        scores_text = []
        if hasattr(session_state, 'q_score') and session_state.q_score:
            scores_text.append(f"Q (Quantitative): {session_state.q_score}")
        if hasattr(session_state, 'v_score') and session_state.v_score:
            scores_text.append(f"V (Verbal): {session_state.v_score}")
        if hasattr(session_state, 'di_score') and session_state.di_score:
            scores_text.append(f"DI (Data Insights): {session_state.di_score}")
        if hasattr(session_state, 'total_score') and session_state.total_score:
            scores_text.append(f"Total: {session_state.total_score}")
            
        if scores_text:
            full_report += "### å„ç§‘ç›®åˆ†æ•¸\n\n"
            for score in scores_text:
                full_report += f"- {score}\n"
            full_report += "\n"
    
    # æ·»åŠ AIç¸½çµå»ºè­°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(session_state, 'consolidated_report_text') and session_state.consolidated_report_text:
        full_report += "## AI ç¸½çµå»ºè­°\n\n"
        full_report += session_state.consolidated_report_text
        full_report += "\n\n---\n\n"
    
    # æ·»åŠ å„ç§‘ç›®è¨ºæ–·å ±å‘Š
    if session_state.report_dict:
        for subject in ['Q', 'V', 'DI']:  # æ˜ç¢ºå®šç¾©ç§‘ç›®é †åº
            report = session_state.report_dict.get(subject)
            if report:
                full_report += f"## {subject} ç§‘è¨ºæ–·å ±å‘Š\n\n{report}\n\n---\n\n"
                
    # å¦‚æœå·²ç¶“å­˜åœ¨æ–°è¨ºæ–·å ±å‘Šï¼ˆå¾ç·¨è¼¯æ¨™ç±¤ç”Ÿæˆï¼‰ï¼Œå‰‡æ·»åŠ åˆ°å ±å‘Šä¸­
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None:
        try:
            # ä½¿ç”¨å…§éƒ¨å‡½æ•¸ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šï¼Œé¿å…å¾ªç’°å°å…¥
            df = session_state.editable_diagnostic_df
            if 'generated_new_diagnostic_report' in session_state:
                # ä½¿ç”¨å·²ç”Ÿæˆçš„å ±å‘Š
                new_report = session_state.generated_new_diagnostic_report
                full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                full_report += new_report
                full_report += "\n\n"
            else:
                # å˜—è©¦ç”Ÿæˆæ–°å ±å‘Š
                logging.info("å˜—è©¦ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Š...")
                # æˆ‘å€‘å°‡ä½¿ç”¨å‹•æ…‹å°å…¥ä¾†é¿å…å¾ªç’°å°å…¥å•é¡Œ
                import importlib
                try:
                    results_display_module = importlib.import_module('gmat_diagnosis_app.ui.results_display')
                    generate_report_func = getattr(results_display_module, 'generate_new_diagnostic_report')
                    
                    new_report = generate_report_func(df)
                    if new_report:
                        # ä¿å­˜åˆ°session_stateä»¥ä¾¿é‡è¤‡ä½¿ç”¨
                        session_state.generated_new_diagnostic_report = new_report
                        full_report += "## æ–°æ¨™ç±¤åˆ†é¡å ±å‘Š\n\n"
                        full_report += new_report
                        full_report += "\n\n"
                except (ImportError, AttributeError) as ie:
                    logging.error(f"ç„¡æ³•å°å…¥æˆ–ä½¿ç”¨generate_new_diagnostic_reportå‡½æ•¸: {ie}")
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ–°è¨ºæ–·å ±å‘Šæ™‚å‡ºéŒ¯: {e}")
    
    return full_report.strip()

def _get_dataframe_context(session_state, max_rows=100):
    """Converts the processed dataframe to a string format (markdown) for context."""
    # å„ªå…ˆä½¿ç”¨ä¿®å‰ªæ¨™ç±¤å¾Œçš„æ•¸æ“šè¡¨æ ¼ (å¦‚æœå­˜åœ¨)
    if hasattr(session_state, 'editable_diagnostic_df') and session_state.editable_diagnostic_df is not None and not session_state.editable_diagnostic_df.empty:
        df_context = session_state.editable_diagnostic_df.copy()
        logging.info(f"æº–å‚™è½‰æ›ä¿®å‰ªæ¨™ç±¤å¾Œçš„è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    elif session_state.processed_df is not None and not session_state.processed_df.empty:
        df_context = session_state.processed_df.copy()
        logging.info(f"æº–å‚™è½‰æ›åŸå§‹è¨ºæ–·è©¦ç®—è¡¨ï¼ŒåŸå§‹åˆ—æ•¸: {len(df_context)}, åˆ—å: {', '.join(df_context.columns)}")
    else:
        logging.warning("è¨ºæ–·è©¦ç®—è¡¨ç‚ºç©ºæˆ–ä¸å­˜åœ¨")
        return "(ç„¡è©³ç´°æ•¸æ“šè¡¨æ ¼)"
        
    try:
        # é¸æ“‡é—œéµåˆ—ä»¥æé«˜å¯è®€æ€§ï¼Œæ’é™¤ä¸å¿…è¦çš„åˆ—
        important_cols = [
            'Subject', 'question_position', 'question_type', 'question_fundamental_skill',
            'content_domain', 'is_invalid', 'is_correct', 'question_time',
            'time_performance_category', 'diagnostic_params_list'
        ]
        
        # åªä¿ç•™å­˜åœ¨æ–¼ DataFrame ä¸­çš„åˆ—
        cols_to_use = [col for col in important_cols if col in df_context.columns]
        
        # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—ï¼ˆä½†å„ªå…ˆä½¿ç”¨ä¸Šé¢å®šç¾©çš„é—œéµåˆ—ï¼‰
        for col in df_context.columns:
            if col not in cols_to_use and col not in ['raw_content']:
                cols_to_use.append(col)
        
        logging.info(f"é¸æ“‡çš„åˆ—: {', '.join(cols_to_use)}")
        
        # å¦‚æœæœ‰åˆ—å¯ç”¨ï¼Œå‰‡ä½¿ç”¨é€™äº›åˆ—ï¼›å¦å‰‡ä½¿ç”¨æ‰€æœ‰åˆ—
        if cols_to_use:
            df_context = df_context[cols_to_use]
        
        # Convert boolean columns to Yes/No for better readability for the LLM
        bool_cols = df_context.select_dtypes(include=bool).columns
        for col in bool_cols:
            df_context[col] = df_context[col].map({True: 'Yes', False: 'No'})
            logging.info(f"å·²å°‡å¸ƒçˆ¾åˆ— '{col}' è½‰æ›ç‚º Yes/No æ ¼å¼")

        # Convert list column to string
        if 'diagnostic_params_list' in df_context.columns:
            df_context['diagnostic_params_list'] = df_context['diagnostic_params_list'].apply(
                lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x)
            )
            logging.info("å·²å°‡ 'diagnostic_params_list' åˆ—è½‰æ›ç‚ºå­—ç¬¦ä¸²æ ¼å¼")

        # Limit rows to avoid excessive context length
        if len(df_context) > max_rows:
            logging.info(f"æ•¸æ“šè¡Œæ•¸è¶…éé™åˆ¶ ({len(df_context)} > {max_rows})ï¼Œåªå–å‰ {max_rows} è¡Œ")
            df_context_str = df_context.head(max_rows).to_markdown(index=False)
            df_context_str += f"\n... (åªé¡¯ç¤ºå‰ {max_rows} è¡Œï¼Œå…± {len(df_context)} è¡Œ)"
        else:
            logging.info(f"è½‰æ›å…¨éƒ¨ {len(df_context)} è¡Œæ•¸æ“šç‚º markdown æ ¼å¼")
            df_context_str = df_context.to_markdown(index=False)
        
        logging.info(f"æˆåŠŸè½‰æ›è¨ºæ–·è©¦ç®—è¡¨ï¼Œè¼¸å‡ºé•·åº¦ç´„ {len(df_context_str)} å­—ç¬¦")
        return df_context_str
    except Exception as e:
        error_msg = f"Error converting dataframe to markdown context: {e}"
        logging.error(error_msg, exc_info=True)
        return f"(ç„¡æ³•è½‰æ›è©³ç´°æ•¸æ“šè¡¨æ ¼: {str(e)})"

def get_openai_response(current_chat_history, report_context, dataframe_context, api_key):
    """Get response from OpenAI based on chat history and context, using o4-mini.

    Args:
        current_chat_history (list): The current conversation history.
        report_context (str): Markdown string of the GMAT report.
        dataframe_context (str): Markdown string of the diagnostic dataframe.
        api_key (str): OpenAI API key.

    Returns:
        tuple: (ai_response_text, response_id) or (error_message_string, None)
    """
    if not api_key:
        logging.error("OpenAI API key is missing for chat.")
        return "éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°æœªè¨­å®šã€‚", None

    client = openai.OpenAI(api_key=api_key)

    # Determine if this is the first user message in the session for this specific chat.
    # A simple heuristic: if chat history has only one user message (the current one being processed)
    # or two messages (system (if we decide to add one initially), user), it's likely the first real interaction.
    # For robustness, let's count user messages.
    user_message_count = sum(1 for msg in current_chat_history if msg['role'] == 'user')

    is_first_exchange = user_message_count <= 1

    if is_first_exchange:
        # First exchange: include the full report and dataframe context
        system_prompt_content = f"""You are an expert GMAT diagnostic assistant. 
Your goal is to help the user understand their GMAT performance based on the provided diagnostic report and data.

Here is the user's GMAT diagnostic report:
--- BEGIN REPORT ---
{report_context}
--- END REPORT ---

Here is the user's detailed diagnostic data (questions, answers, time spent, etc.):
--- BEGIN DATA ---
{dataframe_context}
--- END DATA ---

Answer the user's questions based on this report and data. Be concise, helpful, and refer to specific details from the report or data when possible.
If the user asks for something not in the report or data, state that the information is unavailable.
Maintain a conversational and supportive tone.
"""
    else:
        # Subsequent exchanges: use a simpler system prompt, assuming context is maintained by conversation history.
        system_prompt_content = """You are an expert GMAT diagnostic assistant. 
Continue the conversation based on the provided history. Be concise, helpful, and refer to specific details from the report or data if they were mentioned earlier or are relevant.
If the user asks for something not in the report or data that wasn't previously established, state that the information is unavailable.
Maintain a conversational and supportive tone.
"""

    messages_for_api = [
        {"role": "system", "content": system_prompt_content}
    ]
    # Append the actual chat history (user and assistant messages)
    # We need to ensure the history format is correct for the API
    for message in current_chat_history:
        role = message.get("role")
        content = message.get("content", "") # Default to empty string if content is missing
        if not isinstance(content, str):
            content = str(content) # Ensure content is a string

        if role:
            # Only include 'role' and 'content' keys in the message sent to the API
            messages_for_api.append({"role": role, "content": content})
        # No longer need the separate case for `elif message.get("role")` with empty content,
        # as `message.get("content", "")` handles it.

    # Get previous response_id if available from the last assistant message
    previous_response_id = None
    if len(current_chat_history) > 1:
        # Iterate backwards to find the last assistant message with a response_id
        for i in range(len(current_chat_history) - 1, -1, -1):
            msg = current_chat_history[i]
            if msg.get("role") == "assistant" and msg.get("response_id"):
                previous_response_id = msg.get("response_id")
                logging.info(f"Found previous_response_id: {previous_response_id[:10]}...")
                break 

    try:
        logging.info(f"Calling OpenAI o4-mini. is_first_exchange: {is_first_exchange}. History length: {len(messages_for_api)}. Previous_id: {previous_response_id[:10] if previous_response_id else 'None'}")
        
        # Debug: Log the system prompt being used
        if is_first_exchange:
            logging.info("Sending request with full context in system prompt.")
        else:
            logging.info("Sending request with simplified system prompt.")

        response = client.responses.create(
            model="o4-mini", 
            input=messages_for_api, # Pass the constructed messages list
            # The 'o4-mini' model expects input as a list of messages, not a single prompt string.
            # `prompt` parameter is not used with this model for chat-like interactions.
            previous_response_id=previous_response_id, # Pass the ID for context
            # max_output_tokens=1000 # Optional, if needed for controlling response length
        )
        logging.info(f"OpenAI response status: {response.status}")

        if response.status == 'completed' and response.output:
            ai_response_text = ""
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break
            
            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        ai_response_text += content_block.text

            if not ai_response_text:
                logging.error("OpenAI response completed but no text found.")
                return "AI å›æ‡‰ç‚ºç©ºï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", None
            
            new_response_id = response.id
            logging.info(f"OpenAI response successful. New response_id: {new_response_id[:10] if new_response_id else 'N/A'}...")
            return ai_response_text.strip(), new_response_id
        
        elif response.status == 'error':
            error_details = response.error if response.error else "Unknown error"
            logging.error(f"OpenAI API error: {error_details}")
            return f"AI æœå‹™å‡ºéŒ¯: {error_details}", None
        else:
            logging.error(f"OpenAI response status not completed or output empty. Status: {response.status}")
            return f"AI æœªèƒ½æˆåŠŸå›æ‡‰ (ç‹€æ…‹: {response.status})ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", None

    except openai.AuthenticationError:
        error_msg = "OpenAI APIå¯†é‘°ç„¡æ•ˆ"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.RateLimitError:
        error_msg = "OpenAI APIè«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.APIConnectionError as e:
        error_msg = f"é€£æ¥OpenAI APIæ™‚å‡ºéŒ¯: {e}"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.APITimeoutError:
        error_msg = "OpenAI APIè«‹æ±‚è¶…æ™‚"
        logging.error(error_msg)
        return error_msg, None
        
    except openai.BadRequestError as e:
        error_msg = f"OpenAI APIè«‹æ±‚ç„¡æ•ˆ: {e}"
        logging.error(error_msg)
        return error_msg, None
        
    except Exception as e:
        error_msg = f"èˆ‡OpenAI APIé€šè¨Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
        logging.error(error_msg, exc_info=True)
        return error_msg, None

def trim_diagnostic_tags_with_openai(original_tags_str: str, user_description: str, api_key: str) -> str:
    """
    Uses OpenAI to suggest a trimmed list of 1-2 most relevant diagnostic tags
    based on original tags and user's description of their difficulty.

    Args:
        original_tags_str: A string containing the original diagnostic tags (e.g., comma-separated or list-like).
        user_description: User's description of their difficulty with the question.
        api_key: OpenAI API key.

    Returns:
        A string containing the AI's suggested trimmed tags, or an error message.
    """
    if not api_key:
        logging.warning("OpenAI API key is missing for tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°æœªæä¾›ã€‚"
    if not original_tags_str.strip():
        return "éŒ¯èª¤ï¼šåŸå§‹è¨ºæ–·æ¨™ç±¤ä¸èƒ½ç‚ºç©ºã€‚"
    if not user_description.strip():
        return "éŒ¯èª¤ï¼šä½¿ç”¨è€…æè¿°ä¸èƒ½ç‚ºç©ºã€‚"

    client = openai.OpenAI(api_key=api_key)

    system_prompt = """
æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„ GMAT è¨ºæ–·æ¨™ç±¤ä¿®å‰ªåŠ©æ‰‹ã€‚
æ‚¨çš„ä»»å‹™æ˜¯åˆ†æä½¿ç”¨è€…æä¾›çš„ã€ŒåŸå§‹è¨ºæ–·æ¨™ç±¤ã€ï¼ˆä»¥é€—è™Ÿ,åˆ†éš”ï¼‰ä»¥åŠä»–å€‘ã€Œå°é¡Œç›®çš„æè¿°æˆ–é‡åˆ°çš„å›°é›£ã€ã€‚
æ ¹æ“šä½¿ç”¨è€…çš„æè¿°ï¼Œå¾åŸå§‹æ¨™ç±¤ä¸­ç¯©é¸å‡º 1 è‡³ 2 å€‹æœ€èƒ½ç›´æ¥å°æ‡‰ä¸¦è§£é‡‹ä½¿ç”¨è€…æ‰€è¿°å›°é›£çš„æ ¸å¿ƒè¨ºæ–·æ¨™ç±¤ã€‚

è¼¸å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¿”å›ä¿®å‰ªå¾Œå»ºè­°çš„ 1 æˆ– 2 å€‹æ¨™ç±¤å…¨åï¼Œä¸è¦çœç•¥æˆç°¡åã€‚
- å¦‚æœæœ‰å¤šå€‹å»ºè­°æ¨™ç±¤ï¼Œè«‹ç”¨é€—è™Ÿå’Œä¸€å€‹ç©ºæ ¼ï¼ˆä¾‹å¦‚ï¼šæ¨™ç±¤1, æ¨™ç±¤2ï¼‰åˆ†éš”ã€‚
- å¦‚æœåŸå§‹æ¨™ç±¤å·²ç¶“å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰1-2å€‹ï¼‰ä¸”èˆ‡ä½¿ç”¨è€…æè¿°ç›¸é—œï¼Œå¯ä»¥ç›´æ¥è¿”å›åŸå§‹æ¨™ç±¤ï¼ˆæˆ–ç›¸é—œçš„éƒ¨åˆ†ï¼‰ã€‚
- å¦‚æœæ ¹æ“šä½¿ç”¨è€…æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æ²’æœ‰æ˜é¡¯ç›´æ¥ç›¸é—œçš„æ¨™ç±¤ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€Œæ ¹æ“šæ‚¨çš„æè¿°ï¼ŒåŸå§‹æ¨™ç±¤ä¸­æœªæ‰¾åˆ°ç›´æ¥å°æ‡‰çš„é …ç›®ã€‚ã€
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€å¼•è¨€æˆ–é¡å¤–çš„å®¢å¥—è©±ï¼Œåªéœ€æä¾›å»ºè­°çš„æ¨™ç±¤æˆ–ä¸Šè¿°æ˜ç¢ºçš„æç¤ºã€‚
"""
    
    user_content = f"åŸå§‹è¨ºæ–·æ¨™ç±¤ï¼š\n{original_tags_str}\n\nä½¿ç”¨è€…æè¿°ï¼š\n{user_description}"

    try:
        logging.info("Calling OpenAI ChatCompletion for tag trimming with model gpt-3.5-turbo.")
        response = client.chat.completions.create(
            model="gpt-4.1-mini", # Using a cost-effective and capable model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.2, # Lower temperature for more deterministic output
            max_tokens=100 # Expecting a short list of tags
        )
        
        trimmed_suggestion = response.choices[0].message.content
        
        if trimmed_suggestion and trimmed_suggestion.strip():
            logging.info(f"OpenAI tag trimming successful. Suggestion: {trimmed_suggestion}")
            return trimmed_suggestion.strip()
        else:
            logging.warning("OpenAI returned empty response for tag trimming.")
            return "AI æœªèƒ½æä¾›ä¿®å‰ªå»ºè­°ï¼ˆè¿”å›ç©ºå…§å®¹ï¼‰ã€‚"

    except openai.AuthenticationError:
        logging.error("OpenAI AuthenticationError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API é‡‘é‘°ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ã€‚"
    except openai.RateLimitError:
        logging.error("OpenAI RateLimitError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    except openai.APIConnectionError as e:
        logging.error(f"OpenAI APIConnectionError during tag trimming: {e}")
        return f"éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ã€‚"
    except openai.APITimeoutError:
        logging.error("OpenAI APITimeoutError during tag trimming.")
        return "éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚è¶…æ™‚ã€‚"
    except openai.BadRequestError as e:
        logging.error(f"OpenAI BadRequestError during tag trimming: {e}")
        return f"éŒ¯èª¤ï¼šOpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ã€‚å¯èƒ½æ˜¯è¼¸å…¥å…§å®¹å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unknown OpenAI API error during tag trimming: {e}", exc_info=True)
        return f"èª¿ç”¨ OpenAI API æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚" 