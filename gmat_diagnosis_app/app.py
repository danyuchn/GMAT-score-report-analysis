import sys
import os

# Get the directory containing app.py
app_dir = os.path.dirname(os.path.abspath(__file__))
# Get the parent directory (project root)
project_root = os.path.dirname(app_dir)

# Add the project root to the beginning of sys.path
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import streamlit as st
import pandas as pd
from io import StringIO # Use io.StringIO directly
from gmat_diagnosis_app import irt_module as irt # Import using absolute path
from gmat_diagnosis_app.diagnosis_module import run_diagnosis # Import using absolute path
import os # For environment variables
import openai # Import OpenAI library

# --- Initialize Session State ---
# To track if analysis has been run and store results
if 'analysis_run' not in st.session_state:
    st.session_state.analysis_run = False
if 'report_string' not in st.session_state:
    st.session_state.report_string = None
if 'ai_summary' not in st.session_state:
    st.session_state.ai_summary = None
if 'final_thetas' not in st.session_state:
     st.session_state.final_thetas = {}


# --- Sidebar Settings (Keep as is) ---
st.sidebar.subheader("OpenAI è¨­å®š (é¸ç”¨)")
api_key_input = st.sidebar.text_input(
    "è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼š", 
    type="password", 
    help="æˆ–è€…è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸ã€‚ç”¨æ–¼ç”Ÿæˆæ–‡å­—æ‘˜è¦ã€‚"
)
openai_api_key = api_key_input if api_key_input else os.getenv("OPENAI_API_KEY")

st.sidebar.subheader("IRT æ¨¡æ“¬è¨­å®š")
initial_theta_q = st.sidebar.number_input("Q ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)
initial_theta_v = st.sidebar.number_input("V ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)
initial_theta_di = st.sidebar.number_input("DI ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)

BANK_SIZE = 1000 
TOTAL_QUESTIONS_Q = 21
TOTAL_QUESTIONS_V = 23
TOTAL_QUESTIONS_DI = 20
RANDOM_SEED = 1000 
# --- End Sidebar ---

st.title('GMAT æˆç¸¾è¨ºæ–·å¹³å°')

# --- Data Input Section (Using Tabs) ---
st.header("1. ä¸Šå‚³æˆ–è²¼ä¸Šå„ç§‘æˆç¸¾å–®")

# Initialize DataFrames
df_q = None
df_v = None
df_di = None
data_sources = {}

tab_q, tab_v, tab_di = st.tabs(["Quantitative (Q)", "Verbal (V)", "Data Insights (DI)"])

with tab_q:
    st.subheader("Quantitative (Q) è³‡æ–™è¼¸å…¥")
    uploaded_file_q = st.file_uploader("ä¸Šå‚³ Q ç§‘ç›® CSV æª”æ¡ˆ", type="csv", key="q_uploader")
    pasted_data_q = st.text_area("æˆ–å°‡ Q ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="q_paster")
    temp_df_q = None
    source_q = None
    if uploaded_file_q is not None:
        source_q = uploaded_file_q
    elif pasted_data_q:
        source_q = StringIO(pasted_data_q)
        
    if source_q is not None:
        try:
            temp_df_q = pd.read_csv(source_q, sep=None, engine='python')
            # Drop rows where all columns are NaN (completely empty rows)
            temp_df_q.dropna(how='all', inplace=True)

            # --- Standardize Columns (Handle potential 'Question' variations) ---
            rename_map_q = {
                # 'ï»¿Question': 'question_position', # Old logic
                'Performance': 'Correct',
                'Response Time (Minutes)': 'question_time',
                'Question Type': 'question_type',
                'Content Domain': 'content_domain',
                'Fundamental Skills': 'question_fundamental_skill'
            }
            # Dynamically add the position mapping
            if 'ï»¿Question' in temp_df_q.columns:
                rename_map_q['ï»¿Question'] = 'question_position'
            elif 'Question' in temp_df_q.columns: # Check for plain 'Question'
                rename_map_q['Question'] = 'question_position'

            # Apply only columns that exist
            cols_to_rename = {k: v for k, v in rename_map_q.items() if k in temp_df_q.columns}
            temp_df_q.rename(columns=cols_to_rename, inplace=True)
            
            # Standardize question_type AFTER renaming
            if 'question_type' in temp_df_q.columns:
                temp_df_q['question_type'] = temp_df_q['question_type'].astype(str).str.strip().str.upper()
            else:
                 st.warning("Q: ç¼ºå°‘ 'question_type' æ¬„ä½ã€‚") # Warn but continue

            if 'Correct' in temp_df_q.columns:
                 # Convert to boolean consistently
                 temp_df_q['Correct'] = temp_df_q['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                 temp_df_q.rename(columns={'Correct': 'is_correct'}, inplace=True) # Rename to is_correct
            else:
                st.error("Q è³‡æ–™ç¼ºå°‘ 'Performance' (Correct) æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚")
                temp_df_q = None

            if temp_df_q is not None:
                data_sources['Q'] = 'File Upload' if uploaded_file_q else 'Pasted Data'
                st.success(f"Q ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['Q']})ï¼")
                # Add subject identifier and rename
                temp_df_q['Subject'] = 'Q'

                # --- Ensure independent question_position for Q ---
                if 'question_position' not in temp_df_q.columns or pd.to_numeric(temp_df_q['question_position'], errors='coerce').isnull().any():
                    st.warning("Q: æ­£åœ¨æ ¹æ“šåŸå§‹é †åºé‡æ–°ç”Ÿæˆ question_positionã€‚")
                    temp_df_q = temp_df_q.reset_index(drop=True)
                    temp_df_q['question_position'] = temp_df_q.index + 1
                else:
                    # Ensure it's integer type after validation
                    temp_df_q['question_position'] = pd.to_numeric(temp_df_q['question_position'], errors='coerce').astype('Int64')
                # --- End Ensure ---
                df_q = temp_df_q # Assign to df_q here
                with st.expander("é¡¯ç¤º Q è³‡æ–™é è¦½"): # Optional Preview Expander
                     st.dataframe(df_q.head())
        except Exception as e:
            st.error(f"è™•ç† Q ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_q = None # Ensure df_q is None on error

with tab_v:
    st.subheader("Verbal (V) è³‡æ–™è¼¸å…¥")
    uploaded_file_v = st.file_uploader("ä¸Šå‚³ V ç§‘ç›® CSV æª”æ¡ˆ", type="csv", key="v_uploader")
    pasted_data_v = st.text_area("æˆ–å°‡ V ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="v_paster")
    temp_df_v = None
    source_v = None
    if uploaded_file_v is not None:
        source_v = uploaded_file_v
    elif pasted_data_v:
        source_v = StringIO(pasted_data_v)
        
    if source_v is not None:
        try:
            temp_df_v = pd.read_csv(source_v, sep=None, engine='python')
            # Drop rows where all columns are NaN (completely empty rows)
            temp_df_v.dropna(how='all', inplace=True)

            rename_map_v = {
                # 'ï»¿Question': 'question_position',
                'Performance': 'Correct',
                'Response Time (Minutes)': 'question_time',
                'Question Type': 'question_type',
                'Fundamental Skills': 'question_fundamental_skill'
            }
            # Dynamically add the position mapping
            if 'ï»¿Question' in temp_df_v.columns:
                rename_map_v['ï»¿Question'] = 'question_position'
            elif 'Question' in temp_df_v.columns:
                rename_map_v['Question'] = 'question_position'

            cols_to_rename = {k: v for k, v in rename_map_v.items() if k in temp_df_v.columns}
            temp_df_v.rename(columns=cols_to_rename, inplace=True)
            
            # Standardize question_type AFTER renaming (Remove .str.upper() for V)
            if 'question_type' in temp_df_v.columns:
                temp_df_v['question_type'] = temp_df_v['question_type'].astype(str).str.strip() # Keep original case for V
            else:
                 st.warning("V: ç¼ºå°‘ 'question_type' æ¬„ä½ã€‚") # Warn but continue

            if 'Correct' in temp_df_v.columns:
                 temp_df_v['Correct'] = temp_df_v['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                 temp_df_v.rename(columns={'Correct': 'is_correct'}, inplace=True) # Rename to is_correct
            else:
                 st.error("V è³‡æ–™ç¼ºå°‘ 'Performance' (Correct) æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚")
                 temp_df_v = None

            if temp_df_v is not None:
                data_sources['V'] = 'File Upload' if uploaded_file_v else 'Pasted Data'
                st.success(f"V ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['V']})ï¼")
                # Add subject identifier and rename
                temp_df_v['Subject'] = 'V'

                # --- Ensure independent question_position for V ---
                if 'question_position' not in temp_df_v.columns or pd.to_numeric(temp_df_v['question_position'], errors='coerce').isnull().any():
                    st.warning("V: æ­£åœ¨æ ¹æ“šåŸå§‹é †åºé‡æ–°ç”Ÿæˆ question_positionã€‚")
                    temp_df_v = temp_df_v.reset_index(drop=True)
                    temp_df_v['question_position'] = temp_df_v.index + 1
                else:
                    # Ensure it's integer type after validation
                    temp_df_v['question_position'] = pd.to_numeric(temp_df_v['question_position'], errors='coerce').astype('Int64')
                # --- End Ensure ---
                df_v = temp_df_v # Assign to df_v here
                with st.expander("é¡¯ç¤º V è³‡æ–™é è¦½"):
                     st.dataframe(df_v.head())
        except Exception as e:
            st.error(f"è™•ç† V ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_v = None # Ensure df_v is None on error

with tab_di:
    st.subheader("Data Insights (DI) è³‡æ–™è¼¸å…¥")
    uploaded_file_di = st.file_uploader("ä¸Šå‚³ DI ç§‘ç›® CSV æª”æ¡ˆ", type="csv", key="di_uploader")
    pasted_data_di = st.text_area("æˆ–å°‡ DI ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="di_paster")
    temp_df_di = None
    source_di = None
    if uploaded_file_di is not None:
        source_di = uploaded_file_di
    elif pasted_data_di:
        source_di = StringIO(pasted_data_di)

    if source_di is not None:
        try:
            temp_df_di = pd.read_csv(source_di, sep=None, engine='python')
            # Drop rows where all columns are NaN (completely empty rows)
            temp_df_di.dropna(how='all', inplace=True)

            rename_map_di = {
                # 'ï»¿Question': 'question_position',
                'Performance': 'Correct',
                'Response Time (Minutes)': 'question_time',
                'Question Type': 'question_type',
                'Content Domain': 'content_domain'
            }
            # Dynamically add the position mapping
            if 'ï»¿Question' in temp_df_di.columns:
                rename_map_di['ï»¿Question'] = 'question_position'
            elif 'Question' in temp_df_di.columns:
                rename_map_di['Question'] = 'question_position'

            cols_to_rename = {k: v for k, v in rename_map_di.items() if k in temp_df_di.columns}
            temp_df_di.rename(columns=cols_to_rename, inplace=True)
            
            # Standardize question_type AFTER renaming (Remove .str.upper() for DI)
            if 'question_type' in temp_df_di.columns:
                temp_df_di['question_type'] = temp_df_di['question_type'].astype(str).str.strip() # Keep original case for DI
            else:
                st.warning("DI: ç¼ºå°‘ 'question_type' æ¬„ä½ã€‚") # Warn but continue
            
            if 'Correct' in temp_df_di.columns:
                 temp_df_di['Correct'] = temp_df_di['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                 temp_df_di.rename(columns={'Correct': 'is_correct'}, inplace=True) # Rename to is_correct
            else:
                 st.error("DI è³‡æ–™ç¼ºå°‘ 'Performance' (Correct) æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚")
                 temp_df_di = None

            if temp_df_di is not None:
                data_sources['DI'] = 'File Upload' if uploaded_file_di else 'Pasted Data'
                st.success(f"DI ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['DI']})ï¼")
                # Add subject identifier and rename
                temp_df_di['Subject'] = 'DI'

                # --- Ensure independent question_position for DI ---
                if 'question_position' not in temp_df_di.columns or pd.to_numeric(temp_df_di['question_position'], errors='coerce').isnull().any():
                    st.warning("DI: æ­£åœ¨æ ¹æ“šåŸå§‹é †åºé‡æ–°ç”Ÿæˆ question_positionã€‚")
                    temp_df_di = temp_df_di.reset_index(drop=True)
                    temp_df_di['question_position'] = temp_df_di.index + 1
                else:
                    # Ensure it's integer type after validation
                    temp_df_di['question_position'] = pd.to_numeric(temp_df_di['question_position'], errors='coerce').astype('Int64')
                # --- End Ensure ---
                df_di = temp_df_di # Assign to df_di here
                with st.expander("é¡¯ç¤º DI è³‡æ–™é è¦½"):
                     st.dataframe(df_di.head())
        except Exception as e:
            st.error(f"è™•ç† DI ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_di = None # Ensure df_di is None on error

# --- Combine Input Data (After Tabs) ---
input_dfs = {'Q': df_q, 'V': df_v, 'DI': df_di}
loaded_subjects = {subj for subj, df in input_dfs.items() if df is not None}
df_combined_input = None
df_combined_input_list = [df for df in [df_q, df_v, df_di] if df is not None]

if df_combined_input_list:
     try:
         # Concatenate with ignore_index=True, but 'question_position' is already calculated per subject
         df_combined_input = pd.concat(df_combined_input_list, ignore_index=True)

         # Ensure 'question_position' column exists after concat (it should, unless all inputs failed processing)
         if 'question_position' not in df_combined_input.columns:
             st.error("åˆä½µå¾Œè³‡æ–™ç¼ºå°‘ 'question_position' æ¬„ä½ï¼Œç„¡æ³•ç¹¼çºŒã€‚æª¢æŸ¥å„ç§‘æ•¸æ“šè™•ç†ã€‚")
             df_combined_input = None
         elif df_combined_input['question_position'].isnull().any():
              st.error("åˆä½µå¾Œ 'question_position' æ¬„ä½åŒ…å«ç©ºå€¼ï¼Œç„¡æ³•ç¹¼çºŒã€‚æª¢æŸ¥å„ç§‘æ•¸æ“šè™•ç†ã€‚")
              df_combined_input = None
         # else:
         #     # Optional: Sort for better viewing/debugging, though later logic sorts per subject
         #     df_combined_input.sort_values(by=['Subject', 'question_position'], inplace=True)

     except Exception as e:
         st.error(f"åˆä½µè¼¸å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
         df_combined_input = None
elif loaded_subjects: # Data was loaded but failed during processing within tabs
     st.warning("éƒ¨åˆ†ç§‘ç›®æ•¸æ“šè™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤ä¿¡æ¯ã€‚")

# --- Analysis Trigger Button ---
st.divider() # Add a visual separator

if df_combined_input is not None:
    if st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary"):
        st.session_state.analysis_run = True
        # Reset previous results when starting new analysis
        st.session_state.report_string = None
        st.session_state.ai_summary = None
        st.session_state.final_thetas = {}
    else:
        # If button not clicked in this run, keep analysis_run as False unless it was already True
        # This prevents analysis from running just on widget interaction after first run
        # st.session_state.analysis_run = st.session_state.get('analysis_run', False) # Keep existing state if button not clicked
        pass # No need to explicitly set to false, just don't set to true

elif loaded_subjects: # Data loaded but failed combining
    st.error("ç„¡æ³•åˆä½µè¼¸å…¥è³‡æ–™ï¼Œè«‹æª¢æŸ¥å„ç§‘æ•¸æ“šæ ¼å¼ã€‚åˆ†æç„¡æ³•é€²è¡Œã€‚")
else:
    st.info("è«‹åœ¨ä¸Šæ–¹åˆ†é ä¸­ç‚ºè‡³å°‘ä¸€å€‹ç§‘ç›®ä¸Šå‚³æˆ–è²¼ä¸Šè³‡æ–™ã€‚")

# --- Simulation, Processing, Diagnosis, and Output Tabs (Conditional Execution) ---
if st.session_state.analysis_run and df_combined_input is not None:

    st.header("2. åŸ·è¡Œ IRT æ¨¡æ“¬èˆ‡è¨ºæ–·") # Combine headers
    all_simulation_histories = {} # Store histories per subject
    final_thetas = {}           # Store final theta per subject locally for this run
    df_final_for_diagnosis = None # Initialize

    # --- IRT Simulation ---
    simulation_success = True
    # with st.spinner("æ­£åœ¨åŸ·è¡Œ IRT æ¨¡æ“¬..."): # Replace with st.status
    with st.status("åŸ·è¡Œ IRT æ¨¡æ“¬...", expanded=True) as status:
        st.write("åˆå§‹åŒ–æ¨¡æ“¬é¡Œåº«...")
        question_banks = {}
        try:
            # Create banks only for loaded subjects
            if 'Q' in loaded_subjects: question_banks['Q'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED)
            if 'V' in loaded_subjects: question_banks['V'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED + 1)
            if 'DI' in loaded_subjects: question_banks['DI'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED + 2)
            # Check for failure after trying to create all needed banks
            if any(subj in loaded_subjects and question_banks.get(subj) is None for subj in loaded_subjects):
                 st.error("å‰µå»ºæ¨¡æ“¬é¡Œåº«å¤±æ•—ã€‚")
                 simulation_success = False
                 status.update(label="æ¨¡æ“¬é¡Œåº«å‰µå»ºå¤±æ•—", state="error", expanded=True)
            else:
                 st.write("æ¨¡æ“¬é¡Œåº«å‰µå»ºå®Œæˆã€‚")
        except Exception as e:
            st.error(f"å‰µå»ºæ¨¡æ“¬é¡Œåº«æ™‚å‡ºéŒ¯: {e}")
            simulation_success = False
            status.update(label=f"æ¨¡æ“¬é¡Œåº«å‰µå»ºå‡ºéŒ¯: {e}", state="error", expanded=True)

        if simulation_success:
            subject_params = {
                'Q': {'initial_theta': initial_theta_q, 'total_questions': TOTAL_QUESTIONS_Q},
                'V': {'initial_theta': initial_theta_v, 'total_questions': TOTAL_QUESTIONS_V},
                'DI': {'initial_theta': initial_theta_di, 'total_questions': TOTAL_QUESTIONS_DI}
            }

            for subject in loaded_subjects:
                st.write(f"åŸ·è¡Œ {subject} ç§‘ç›®æ¨¡æ“¬...")
                # Get parameters for simulation
                params = subject_params[subject]
                initial_theta = params['initial_theta']
                total_sim_questions = params['total_questions'] # Number of questions IN SIMULATION
                bank = question_banks[subject]
                
                # Get WRONG indices from the *original* user data for this subject
                user_df_subj = df_combined_input[df_combined_input['Subject'] == subject]
                wrong_indices = [] # Keep variable name for now, but it holds positions
                # Check for the final column name 'is_correct'
                if 'is_correct' in user_df_subj.columns:
                    # Sort by position before getting indices
                    user_df_subj_sorted = user_df_subj.sort_values(by='question_position')
                    # Filter using the final column name 'is_correct'
                    # --- CORRECTED LOGIC HERE ---
                    # Directly get the 'question_position' values of incorrect answers
                    wrong_positions = user_df_subj_sorted[user_df_subj_sorted['is_correct'] == False]['question_position'].tolist()
                    wrong_indices = wrong_positions # Assign to the variable expected by the simulation function
                    # --- END CORRECTION ---
                    st.write(f"  {subject}: å¾ç”¨æˆ¶æ•¸æ“šæå– {len(wrong_indices)} å€‹éŒ¯èª¤é¡Œç›®ä½ç½®: {wrong_indices}")
                else:
                    # This warning should now only appear if 'Performance' was truly missing initially
                    st.warning(f"  {subject}: ç”¨æˆ¶æ•¸æ“šç¼ºå°‘ 'is_correct' æ¬„ä½ (æºè‡ª 'Performance')ï¼Œå‡è¨­å…¨éƒ¨ç­”å°é€²è¡Œæ¨¡æ“¬ã€‚")
                    wrong_indices = []

                # Run the simulation
                try:
                    history_df = irt.simulate_cat_exam(
                        question_bank=bank,
                        wrong_question_indices=wrong_indices, # Pass the list of actual wrong positions
                        initial_theta=initial_theta,
                        total_questions=total_sim_questions # Use the simulation total questions
                    )
                    if history_df is not None and not history_df.empty:
                        all_simulation_histories[subject] = history_df
                        # Store final theta locally first
                        final_theta_subj = history_df['theta_est_after_answer'].iloc[-1]
                        final_thetas[subject] = final_theta_subj
                        st.write(f"  {subject}: æ¨¡æ“¬å®Œæˆã€‚æœ€å¾Œ Theta ä¼°è¨ˆ: {final_theta_subj:.3f}")
                    elif history_df is not None and history_df.empty:
                        st.warning(f"  {subject}: æ¨¡æ“¬åŸ·è¡Œäº†ï¼Œä½†æœªç”¢ç”Ÿæ­·å²è¨˜éŒ„ã€‚")
                        simulation_success = False # Treat empty history as failure for next steps
                    else:
                         st.error(f"  {subject}: æ¨¡æ“¬åŸ·è¡Œå¤±æ•—ï¼Œè¿”å› Noneã€‚")
                         simulation_success = False
                         break # Stop simulation for other subjects if one fails
                except Exception as e:
                    st.error(f"  {subject}: åŸ·è¡Œæ¨¡æ“¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    simulation_success = False
                    break # Stop simulation

        if simulation_success and all_simulation_histories:
            status.update(label="IRT æ¨¡æ“¬å®Œæˆï¼", state="complete", expanded=False)
            st.session_state.final_thetas = final_thetas # Store final thetas in session state
        elif simulation_success: # Banks created, but simulation failed for some reason
             status.update(label="IRT æ¨¡æ“¬éƒ¨åˆ†å¤±æ•—æˆ–æœªç”¢ç”Ÿçµæœ", state="error", expanded=True)
             simulation_success = False # Ensure it's false
        # Else: Bank creation failed, status already set to error

    # --- Prepare Data for Diagnosis ---
    # st.header("2. æº–å‚™è¨ºæ–·æ•¸æ“š (çµåˆç”¨æˆ¶æ•¸æ“šèˆ‡æ¨¡æ“¬é›£åº¦)") # Combined into header 2
    if simulation_success:
        # with st.spinner("æº–å‚™è¨ºæ–·æ•¸æ“šä¸­..."): # Replace with st.status
        with st.status("æº–å‚™è¨ºæ–·æ•¸æ“š...", expanded=True) as status_prep:
            df_final_for_diagnosis_list = []
            processing_error = False
            for subject in loaded_subjects:
                st.write(f"è™•ç† {subject} ç§‘ç›®...")
                user_df_subj = df_combined_input[df_combined_input['Subject'] == subject].copy()
                sim_history_df = all_simulation_histories.get(subject)
                final_theta = final_thetas.get(subject)

                if sim_history_df is None or sim_history_df.empty:
                    st.error(f"æ‰¾ä¸åˆ° {subject} ç§‘ç›®çš„æœ‰æ•ˆæ¨¡æ“¬çµæœï¼Œç„¡æ³•ç¹¼çºŒã€‚")
                    processing_error = True; status_prep.update(state="error"); break

                # Extract simulated b-values
                sim_b_values = sim_history_df['b'].tolist()
                
                # Sort user data by position
                user_df_subj_sorted = user_df_subj.sort_values(by='question_position')
                num_user_questions = len(user_df_subj_sorted)
                num_sim_b = len(sim_b_values)

                # Check for length mismatch between user data and simulation results
                if num_user_questions != num_sim_b:
                     st.warning(f"{subject}: ç”¨æˆ¶æ•¸æ“šé¡Œç›®æ•¸ ({num_user_questions}) èˆ‡æ¨¡æ“¬çµæœæ•¸ ({num_sim_b}) ä¸ç¬¦ã€‚" 
                               f"å°‡åƒ…ä½¿ç”¨å‰ {min(num_user_questions, num_sim_b)} å€‹æ•¸æ“šé€²è¡Œé›£åº¦è³¦å€¼ã€‚è¨ºæ–·å¯èƒ½ä¸å®Œæ•´ã€‚")
                     # Truncate to the minimum length
                     min_len = min(num_user_questions, num_sim_b)
                     user_df_subj_sorted = user_df_subj_sorted.iloc[:min_len]
                     sim_b_values = sim_b_values[:min_len]
                
                if not sim_b_values: # Check if list became empty after truncation
                     st.error(f"{subject}: ç„¡å¯ç”¨çš„æ¨¡æ“¬é›£åº¦å€¼ï¼Œç„¡æ³•ç¹¼çºŒè™•ç†ã€‚")
                     processing_error = True; status_prep.update(state="error"); break

                # Assign simulated b-values as 'question_difficulty'
                user_df_subj_sorted['question_difficulty'] = sim_b_values
                st.write(f"  {subject}: å·²å°‡æ¨¡æ“¬é›£åº¦è³¦å€¼çµ¦ {len(user_df_subj_sorted)} é“é¡Œç›®ã€‚")

                # Add final theta as context
                if final_theta is not None:
                     user_df_subj_sorted['estimated_ability'] = final_theta

                df_final_for_diagnosis_list.append(user_df_subj_sorted)
            
            if not processing_error and df_final_for_diagnosis_list:
                df_final_for_diagnosis = pd.concat(df_final_for_diagnosis_list, ignore_index=True)
                
                # st.subheader("è¨ºæ–·ç”¨æ•¸æ“šé è¦½ (å«æ¨¡æ“¬é›£åº¦)") # Optional: Move preview here?
                # st.dataframe(df_final_for_diagnosis.head())
                st.write("æ‰€æœ‰ç§‘ç›®æ•¸æ“šæº–å‚™å®Œæˆã€‚")
                status_prep.update(label="è¨ºæ–·æ•¸æ“šæº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            elif not processing_error: # List is empty but no specific error flagged?
                 st.warning("æœªèƒ½æº–å‚™ä»»ä½•è¨ºæ–·æ•¸æ“šã€‚")
                 status_prep.update(label="æœªèƒ½æº–å‚™è¨ºæ–·æ•¸æ“š", state="warning", expanded=True)
            # Else: error occurred, status already set
    
    # --- Diagnosis Section --- (Now uses df_final_for_diagnosis)
    # st.header("3. åŸ·è¡Œè¨ºæ–·åˆ†æ") # Combined into header 2
    if df_final_for_diagnosis is not None: # Check if data prep was successful
        report_string = None # Initialize report string for this run
        # with st.spinner("åŸ·è¡Œè¨ºæ–·åˆ†æä¸­..."): # Replace with st.status
        with st.status("åŸ·è¡Œè¨ºæ–·åˆ†æ...", expanded=True) as status_diag:
            try:
                # Ensure all required columns for the *markdown logic* are present
                # Markdown needs: 'Correct', 'question_difficulty', 'question_time', 'question_type', 
                # 'question_fundamental_skill', 'question_position'
                required_cols = ['is_correct', 'question_difficulty', 'question_time', 'question_type', 'question_position']
                # Check optional cols individually and add placeholders if missing
                if 'question_fundamental_skill' not in df_final_for_diagnosis.columns:
                     st.warning("æ•¸æ“šç¼ºå°‘ 'question_fundamental_skill'ï¼Œéƒ¨åˆ†è¨ºæ–·å¯èƒ½å—å½±éŸ¿ã€‚æ­£åœ¨æ·»åŠ å ä½ç¬¦ã€‚")
                     df_final_for_diagnosis['question_fundamental_skill'] = 'Unknown Skill' # Add placeholder
                if 'content_domain' not in df_final_for_diagnosis.columns:
                     st.warning("æ•¸æ“šç¼ºå°‘ 'content_domain'ï¼Œéƒ¨åˆ†è¨ºæ–·å¯èƒ½å—å½±éŸ¿ã€‚æ­£åœ¨æ·»åŠ å ä½ç¬¦ã€‚")
                     df_final_for_diagnosis['content_domain'] = 'Unknown Domain' # Add placeholder

                missing_cols = [col for col in required_cols if col not in df_final_for_diagnosis.columns]
                
                if not missing_cols:
                    st.write("æ­£åœ¨èª¿ç”¨è¨ºæ–·æ¨¡å¡Š...")
                    # Pass the combined data to the diagnosis module
                    # Consider passing total_test_time if collected
                    report_string = run_diagnosis(df_final_for_diagnosis) # Changed variable name
                    st.session_state.report_string = report_string # Store in session state

                    # Check if the report string is not None and not empty
                    if report_string:
                        st.write("è¨ºæ–·åˆ†æå®Œæˆã€‚")
                        status_diag.update(label="è¨ºæ–·åˆ†æå®Œæˆï¼", state="complete", expanded=False)
                    else:
                        st.warning("è¨ºæ–·åˆ†ææœªè¿”å›çµæœæˆ–çµæœç‚ºç©ºã€‚è«‹æª¢æŸ¥ `diagnosis_module.py`ã€‚") # Updated warning
                        status_diag.update(label="è¨ºæ–·åˆ†ææœªè¿”å›çµæœ", state="warning", expanded=True)
                else:
                    st.error(f"æº–å‚™ç”¨æ–¼è¨ºæ–·çš„è³‡æ–™ç¼ºå°‘å¿…éœ€æ¬„ä½: {missing_cols}ã€‚ç„¡æ³•åŸ·è¡Œè¨ºæ–·ã€‚")
                    status_diag.update(label=f"è¨ºæ–·æ‰€éœ€æ¬„ä½ç¼ºå¤±: {missing_cols}", state="error", expanded=True)
                    st.session_state.report_string = None # Ensure reset on error
            except Exception as e:
                st.error(f"åŸ·è¡Œè¨ºæ–·æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                status_diag.update(label=f"è¨ºæ–·åŸ·è¡Œå‡ºéŒ¯: {e}", state="error", expanded=True)
                st.session_state.report_string = None # Ensure reset on error

            # --- Generate OpenAI Summary --- (Still within button click block)
            # Check if report_string exists and is not empty
            if st.session_state.report_string: # Check session state
                if openai_api_key:
                    # with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡å­—æ‘˜è¦..."): # Replace with st.status
                    with st.status("ç”Ÿæˆ AI æ–‡å­—æ‘˜è¦...", expanded=False) as status_ai:
                        try:
                            client = openai.OpenAI(api_key=openai_api_key)
                            # Use the report string directly in the prompt
                            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ GMAT è¨ºæ–·å ±å‘Šï¼ˆåŸºæ–¼ç”¨æˆ¶å¯¦éš›è¡¨ç¾æ•¸æ“šï¼Œä½†ä½¿ç”¨äº†æ¨¡æ“¬é›£åº¦ä¼°è¨ˆï¼‰ï¼Œç”¢ç”Ÿä¸€æ®µç°¡æ½”çš„ç¸½çµå ±å‘Šï¼ˆç´„ 150-200 å­—ï¼‰ï¼Œèªªæ˜ä¸»è¦çš„å¼·é …ã€å¼±é …æˆ–å€¼å¾—æ³¨æ„çš„æ¨¡å¼ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚

è¨ºæ–·å ±å‘Šï¼š
{st.session_state.report_string}

ç¸½çµå ±å‘Šï¼š"""
                            st.write("æ­£åœ¨èª¿ç”¨ OpenAI API...")
                            response = client.chat.completions.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ“…é•·åˆ†æ GMAT è¨ºæ–·å ±å‘Šä¸¦ç”¢ç”Ÿç¸½çµçš„ AI åŠ©ç†ã€‚"},
                                    {"role": "user", "content": prompt}
                                ]
                            )
                            summary = response.choices[0].message.content
                            st.session_state.ai_summary = summary # Store in session state
                            st.write("AI æ‘˜è¦ç”ŸæˆæˆåŠŸã€‚")
                            status_ai.update(label="AI æ–‡å­—æ‘˜è¦ç”ŸæˆæˆåŠŸï¼", state="complete", expanded=False)
                        except Exception as e:
                            st.error(f"ç”Ÿæˆ AI æ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                            st.session_state.ai_summary = None # Reset on error
                            status_ai.update(label=f"AI æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}", state="error", expanded=True)
                else:
                     # Info message moved to output section
                     pass
            # --- End OpenAI Summary ---
        # elif not df_final_for_diagnosis_list: # Handled inside status_prep
        #      st.warning("æœªèƒ½æˆåŠŸæº–å‚™ä»»ä½•ç”¨æ–¼è¨ºæ–·çš„æ•¸æ“šã€‚")
    # elif not simulation_success: # Condition checked before data prep
    #     st.error("IRT æ¨¡æ“¬éç¨‹ä¸­æ–·æˆ–å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒåˆ†æã€‚")

# --- Display Results Section (Uses Session State) ---
st.divider()
if st.session_state.analysis_run: # Only show results area if analysis was attempted
    st.header("è¨ºæ–·çµæœ")

    # Display Final Thetas if available
    if st.session_state.final_thetas:
         theta_items = [f"{subj}: {theta:.3f}" for subj, theta in st.session_state.final_thetas.items()]
         st.success(f"æœ€çµ‚èƒ½åŠ›ä¼°è¨ˆ (Theta): {', '.join(theta_items)}")
    # else:
    #      st.warning("ç„¡æ³•ç²å–æœ€çµ‚èƒ½åŠ›ä¼°è¨ˆã€‚") # Maybe don't show warning if analysis failed earlier

    if st.session_state.report_string:
        report_tab, summary_tab = st.tabs(["è©³ç´°è¨ºæ–·å ±å‘Š", "AI æ–‡å­—æ‘˜è¦"])
        with report_tab:
            st.markdown(st.session_state.report_string)
        with summary_tab:
            if st.session_state.ai_summary:
                st.markdown(st.session_state.ai_summary)
            elif openai_api_key:
                 st.info("AI æ‘˜è¦å°šæœªç”Ÿæˆæˆ–ç”Ÿæˆå¤±æ•—ã€‚è«‹æª¢æŸ¥ä¸Šæ–¹ç‹€æ…‹ã€‚")
            else:
                st.info("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥ OpenAI API Key ä¸¦é‡æ–°é‹è¡Œåˆ†æä»¥ç”Ÿæˆ AI æ–‡å­—æ‘˜è¦ã€‚")
    else: # Analysis ran but report is None/empty
        st.error("åˆ†æéç¨‹æœªæˆåŠŸç”Ÿæˆè¨ºæ–·å ±å‘Šï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹çš„ç‹€æ…‹ä¿¡æ¯å’ŒéŒ¯èª¤æç¤ºã€‚")

# else: # Analysis was not run (button not clicked or no data)
#     st.info("é»æ“Š 'é–‹å§‹åˆ†æ' æŒ‰éˆ•ä»¥åŸ·è¡Œæ¨¡æ“¬èˆ‡è¨ºæ–·ã€‚") # Message now shown near button

# Final info message handled within sections
