import sys
import os
import re # Import regex module
import io # Ensure io is imported
import pandas as pd # Ensure pandas is imported

# Get the directory containing app.py
app_dir = os.path.dirname(os.path.abspath(__file__))
# Get the parent directory (project root)
project_root = os.path.dirname(app_dir)

# Add the project root to the beginning of sys.path
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import streamlit as st
from io import StringIO # Use io.StringIO directly
from gmat_diagnosis_app import irt_module as irt # Import using absolute path
# from gmat_diagnosis_app.diagnosis_module import run_diagnosis # Remove old import
import os # For environment variables
import openai # Import OpenAI library
from gmat_diagnosis_app.preprocess_helpers import suggest_invalid_questions # Import new preprocessor
# Import preprocessing constants
from gmat_diagnosis_app.preprocess_helpers import (
    MAX_ALLOWED_TIME_Q, TOTAL_QUESTIONS_Q, Q_FAST_END_THRESHOLD_MINUTES, 
    Q_TIME_DIFF_PRESSURE_THRESHOLD, LAST_THIRD_FRACTION_Q,
    MAX_ALLOWED_TIME_DI, TOTAL_QUESTIONS_DI, DI_TIME_PRESSURE_THRESHOLD_MINUTES,
    DI_INVALID_TIME_THRESHOLD_MINUTES, LAST_THIRD_FRACTION_DI,
    V_INVALID_TIME_HASTY_MIN, TOTAL_QUESTIONS_V
)
# Import the updated diagnostic functions
from gmat_diagnosis_app.diagnostics.v_diagnostic import run_v_diagnosis_processed
from gmat_diagnosis_app.diagnostics.di_diagnostic import run_di_diagnosis_processed
from gmat_diagnosis_app.diagnostics.q_diagnostic import run_q_diagnosis_processed

# --- Validation Rules and Messages ---
# Based on provided testset CSVs, case and space sensitive

ALLOWED_PERFORMANCE = ['Correct', 'Incorrect']
ALLOWED_CONTENT_DOMAIN = {
    'Q': ['Algebra', 'Arithmetic'],
    'V': ['N/A'], # Assuming only N/A based on v.csv example
    'DI': ['Math Related', 'Non-Math Related']
}
# Combine all possible domains for easier checking if needed, or use subject-specific
ALL_CONTENT_DOMAINS = list(set(cd for subj_cds in ALLOWED_CONTENT_DOMAIN.values() for cd in subj_cds))

ALLOWED_QUESTION_TYPE = {
    'Q': ['REAL', 'PURE'],
    'V': ['Critical Reasoning', 'Reading Comprehension'],
    'DI': ['Data Sufficiency', 'Two-part analysis', 'Multi-source reasoning', 'Graph and Table', 'Graphs and Tables']
}
ALL_QUESTION_TYPES = list(set(qt for subj_qts in ALLOWED_QUESTION_TYPE.values() for qt in subj_qts))

ALLOWED_FUNDAMENTAL_SKILLS = {
    'Q': ['Equal/Unequal/ALG', 'Rates/Ratio/Percent', 'Rates/Ratios/Percent', 'Value/Order/Factors', 'Counting/Sets/Series/Prob/Stats'],
    'V': ['Plan/Construct', 'Identify Stated Idea', 'Identify Inferred Idea', 'Analysis/Critique'],
    'DI': ['N/A'] # Assuming only N/A based on di.csv example
}
ALL_FUNDAMENTAL_SKILLS = list(set(fs for subj_fss in ALLOWED_FUNDAMENTAL_SKILLS.values() for fs in subj_fss))


VALIDATION_RULES = {
    # Original Column Name (before rename) : { rules }
    'Response Time (Minutes)': {'type': 'positive_float', 'error': "å¿…é ˆæ˜¯æ­£æ•¸ (ä¾‹å¦‚ 1.5, 2)ã€‚"},
    'Performance': {'allowed': ALLOWED_PERFORMANCE, 'error': f"å¿…é ˆæ˜¯ {ALLOWED_PERFORMANCE} å…¶ä¸­ä¹‹ä¸€ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Content Domain': {'allowed': ALL_CONTENT_DOMAINS, 'subject_specific': ALLOWED_CONTENT_DOMAIN, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Question Type': {'allowed': ALL_QUESTION_TYPES, 'subject_specific': ALLOWED_QUESTION_TYPE, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Fundamental Skills': {'allowed': ALL_FUNDAMENTAL_SKILLS, 'subject_specific': ALLOWED_FUNDAMENTAL_SKILLS, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    # We will validate the 'Question' column after potential rename
    'Question': {'type': 'positive_integer', 'error': "å¿…é ˆæ˜¯æ­£æ•´æ•¸ (ä¾‹å¦‚ 1, 2, 3)ã€‚"},
    '\ufeffQuestion': {'type': 'positive_integer', 'error': "å¿…é ˆæ˜¯æ­£æ•´æ•¸ (ä¾‹å¦‚ 1, 2, 3)ã€‚"}, # Handle BOM
    # Removed validation for b-values as they are not expected user inputs
    # 'DI_b': {'type': 'number', 'error': "å¿…é ˆæ˜¯æ•¸å­—ã€‚"},
    # 'Q_b': {'type': 'number', 'error': "å¿…é ˆæ˜¯æ•¸å­—ã€‚"},
    # 'V_b': {'type': 'number', 'error': "å¿…é ˆæ˜¯æ•¸å­—ã€‚"},
}

# --- End Validation Rules ---

# --- Validation Helper Function ---
def validate_dataframe(df, subject):
    """
    Validates the DataFrame rows based on predefined rules for the subject.

    Args:
        df (pd.DataFrame): The DataFrame to validate (potentially edited).
        subject (str): The subject ('Q', 'V', 'DI') to apply specific rules.

    Returns:
        list: A list of error message strings. Empty list if valid.
    """
    errors = []
    # Define required columns based on the *original* expected names from CSVs
    # We check these exist in the df passed *to* validation (which is the edited_df)
    required_original_columns = {
        'Q': ['Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type', 'Fundamental Skills'],
        'V': ['Question', 'Response Time (Minutes)', 'Performance', 'Question Type', 'Fundamental Skills'], # REMOVE Content Domain for V
        'DI': ['Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type'] # REMOVE Fundamental Skills for DI
    }
    # Adjust required based on BOM possibility for 'Question'
    actual_required = required_original_columns.get(subject, [])
    question_col_to_check = 'Question' # Default
    if '\ufeffQuestion' in df.columns and 'Question' not in df.columns:
         question_col_to_check = '\ufeffQuestion'
         actual_required = [col if col != 'Question' else '\ufeffQuestion' for col in actual_required]
    elif 'Question' not in df.columns and '\ufeffQuestion' not in df.columns:
         # If neither standard 'Question' nor BOM version exists, flag 'Question' as missing
         pass # Let the check below handle it

    # Helper function for preprocessing skill names (lowercase, strip, collapse spaces)
    def preprocess_skill(skill):
        return re.sub(r'\\s+', ' ', str(skill).strip()).lower()

    # 1. Check for missing required columns first (using the potentially BOM-adjusted list)
    missing_cols = [col for col in actual_required if col not in df.columns]
    if missing_cols:
        # If 'Question' is missing, but '\ufeffQuestion' was required and IS present, don't report 'Question' as missing.
        if 'Question' in missing_cols and question_col_to_check == '\ufeffQuestion':
             missing_cols.remove('Question') # Remove the standard one if BOM version is the one we check

        # Re-check if missing_cols is now empty
        if missing_cols:
             errors.append(f"è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½: {', '.join(missing_cols)}ã€‚è«‹æª¢æŸ¥ä¸Šå‚³/è²¼ä¸Šçš„è³‡æ–™æˆ–ç·¨è¼¯çµæœã€‚")
             return errors # Stop further validation if essential columns are missing

    # 2. Validate cell values row by row
    for index, row in df.iterrows():
        for col_name, rules in VALIDATION_RULES.items():
            # Determine the actual column name to check in the dataframe (handling BOM)
            current_col_name = col_name
            if col_name == 'Question' and question_col_to_check == '\ufeffQuestion':
                 current_col_name = question_col_to_check # Use BOM version if that's what df has

            if current_col_name not in df.columns:
                # Only skip if the original column name (non-BOM) wasn't in the required list
                # This prevents skipping validation for a required column just because it wasn't found (e.g., 'Performance')
                if col_name not in actual_required:
                    continue # Skip validation if column doesn't exist AND wasn't required

            # If the column IS required but wasn't found above, it would have returned errors already.
            # If it's not required and not present, we skipped. Now, if it *is* present:
            if current_col_name in df.columns:
                value = row[current_col_name]

                # Skip validation for truly empty/NaN cells for now
                if pd.isna(value): # Check for NaN first
                     continue
                # Convert to string and check if empty *after* stripping whitespace
                value_str_for_empty_check = str(value).strip()
                if value_str_for_empty_check == '':
                     continue # Also skip visibly empty cells

                is_valid = True
                error_detail = rules['error']
                correct_value = None # To store the value we might correct to

                # Type checks (apply strip here for robustness)
                if 'type' in rules:
                    value_str = str(value).strip() # Use stripped value for validation
                    if rules['type'] == 'positive_float':
                        try:
                            num = float(value_str)
                            if num <= 0: is_valid = False
                        except (ValueError, TypeError):
                            is_valid = False
                    elif rules['type'] == 'positive_integer':
                        try:
                            # Convert to float first to handle inputs like '1.0'
                            num_float = float(value_str)
                            # Check if positive, is an integer (no fractional part), and matches int conversion
                            if num_float > 0 and num_float == int(num_float):
                                is_valid = True
                            else:
                                is_valid = False # Not positive or has fractional part
                        except (ValueError, TypeError):
                             # If conversion to float fails, it's not a valid representation of an integer
                            is_valid = False
                    elif rules['type'] == 'number':
                         try:
                             float(value_str) # Just check if convertible
                         except (ValueError, TypeError):
                             is_valid = False

                # Allowed list checks (case and space sensitive - use original value)
                elif 'allowed' in rules:
                    allowed_values = rules['allowed']
                    # Use subject-specific list if available
                    if 'subject_specific' in rules and subject in rules['subject_specific']:
                        allowed_values = rules['subject_specific'][subject]

                    value_str_stripped = str(value).strip() # Value after stripping whitespace
                    value_str_lower = value_str_stripped.lower() # Lowercase version for comparison

                    # --- Start Specific Logic for Q: Question Type ---
                    if col_name == 'Question Type' and subject == 'Q':
                        if value_str_lower == 'real contexts':
                            correct_value = 'REAL'
                            is_valid = True
                        elif value_str_lower == 'pure contexts':
                            correct_value = 'PURE'
                            is_valid = True
                        else:
                            # Fallback to standard check if not the special context values
                            allowed_values_map = {str(v).lower(): v for v in allowed_values}
                            if value_str_lower in allowed_values_map:
                                correct_value = allowed_values_map[value_str_lower]
                                is_valid = True
                            else:
                                is_valid = False
                    # --- End Specific Logic for Q: Question Type ---

                    # --- Start Specific Logic for Fundamental Skills (Fuzzy Match + Canonical Mapping) ---
                    elif col_name == 'Fundamental Skills':
                        processed_input = preprocess_skill(value_str_stripped)
                        is_valid = False # Assume invalid initially
                        correct_value = None

                        # Build a map from preprocessed key to canonical value
                        skill_map = {}
                        for allowed_val in allowed_values: # allowed_values is subject-specific list
                            processed_allowed = preprocess_skill(allowed_val)
                            # Define the canonical value
                            if processed_allowed == 'rates/ratios/percent':
                                canonical_value = 'Rates/Ratio/Percent' # Standard singular form
                            else:
                                canonical_value = allowed_val # Use the value itself as canonical
                            skill_map[processed_allowed] = canonical_value

                        # Check if the preprocessed input exists as a key
                        if processed_input in skill_map:
                            correct_value = skill_map[processed_input] # Get the canonical value
                            is_valid = True
                    # --- End Specific Logic for Fundamental Skills ---

                    # --- Default Case-Insensitive Check (Handles case and specific variations like Graphs/Graph) ---
                    else:
                        # Build a map from lowercase key to canonical value
                        allowed_values_map = {}
                        for v in allowed_values:
                            key = str(v).lower()
                            # Map specific variations to their canonical form
                            if key == 'graphs and tables':
                                canonical_value = 'Graph and Table' # Standard form
                            else:
                                canonical_value = v # Use the value itself as canonical
                            allowed_values_map[key] = canonical_value

                        # Check if the lowercased input exists as a key in the map
                        if value_str_lower in allowed_values_map:
                            correct_value = allowed_values_map[value_str_lower] # Get the canonical value
                            is_valid = True
                        else:
                            is_valid = False
                    # --- End Default Check ---

                    # --- Correction Logic (Applies if a match was found above and correct_value is set) ---
                    if is_valid and correct_value is not None:
                        # If the stripped original value differs from the correct capitalization/format, update the DataFrame
                        if value_str_stripped != correct_value:
                            try:
                                # Use .loc for safe assignment
                                df.loc[index, current_col_name] = correct_value
                                # Optionally, log the correction?
                                # print(f"Corrected row {index}, col '{current_col_name}': '{value}' -> '{correct_value}'")
                            except Exception as e:
                                errors.append(f"ç¬¬ {index + 1} è¡Œ, æ¬„ä½ '{current_col_name}': å˜—è©¦è‡ªå‹•ä¿®æ­£å¤§å°å¯«/æ ¼å¼æ™‚å‡ºéŒ¯ ({e})ã€‚")
                                is_valid = False # Mark as invalid if correction fails
                    elif not is_valid:
                        # Format allowed values clearly for the error message if validation failed
                        allowed_values_str = ", ".join(f"'{v}'" for v in allowed_values)
                        error_detail += f" å…è¨±çš„å€¼: {allowed_values_str} (å¤§å°å¯«/ç©ºæ ¼ä¸æ•æ„ŸåŒ¹é…å¤±æ•—)ã€‚"
                    # --- End Correction Logic ---

                if not is_valid:
                    # Add 1 to index for user-friendly row numbering (Excel/CSV style)
                    # Use current_col_name in the error message as that's the column header the user sees
                    errors.append(f"ç¬¬ {index + 1} è¡Œ, æ¬„ä½ '{current_col_name}': å€¼ '{value}' ç„¡æ•ˆã€‚{error_detail}")

    return errors
# --- End Validation Helper ---


# --- Initialize Session State ---
# To track if analysis has been run and store results
if 'analysis_run' not in st.session_state:
    st.session_state.analysis_run = False
if 'report_dict' not in st.session_state:
    st.session_state.report_dict = {}
if 'ai_summary' not in st.session_state:
    st.session_state.ai_summary = None
if 'final_thetas' not in st.session_state:
     st.session_state.final_thetas = {}
# Remove state for validated dataframes
# if 'validated_df_q' not in st.session_state:
#     st.session_state.validated_df_q = None
# if 'validated_df_v' not in st.session_state:
#     st.session_state.validated_df_v = None
# if 'validated_df_di' not in st.session_state:
#     st.session_state.validated_df_di = None

# --- Sidebar Settings (Keep as is) ---
st.sidebar.subheader("OpenAI è¨­å®š (é¸ç”¨)")
api_key_input = st.sidebar.text_input(
    "è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼š", 
    type="password", 
    help="æˆ–è€…è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸ã€‚ç”¨æ–¼ç”Ÿæˆæ–‡å­—æ‘˜è¦ã€‚"
)
openai_api_key = api_key_input if api_key_input else os.getenv("OPENAI_API_KEY")

st.sidebar.subheader("IRT æ¨¡æ“¬è¨­å®š")
initial_theta_q = st.sidebar.number_input("Q ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)
initial_theta_v = st.sidebar.number_input("V ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)
initial_theta_di = st.sidebar.number_input("DI ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=0.0, step=0.1)

BANK_SIZE = 1000 
TOTAL_QUESTIONS_Q = 21
TOTAL_QUESTIONS_V = 23
TOTAL_QUESTIONS_DI = 20
RANDOM_SEED = 1000 
MAX_FILE_SIZE_MB = 1 # Define max file size in MB
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024 # Calculate size in bytes
# --- End Sidebar ---

st.title('GMAT æˆç¸¾è¨ºæ–·å¹³å°')

# --- Initialize session state variables ---
if 'data_loaded_q' not in st.session_state:
    st.session_state.data_loaded_q = False
if 'analysis_run' not in st.session_state:
    st.session_state.analysis_run = False
if 'report_dict' not in st.session_state:
    st.session_state.report_dict = {}
if 'df_processed' not in st.session_state:
    st.session_state.df_processed = None
if 'error_message' not in st.session_state: # Add initialization for error_message
    st.session_state.error_message = None
if 'pasted_q' not in st.session_state: st.session_state.pasted_q = ""
if 'pasted_v' not in st.session_state: st.session_state.pasted_v = ""
if 'pasted_di' not in st.session_state: st.session_state.pasted_di = ""
# Add initializations for edited dataframes if they are used before assigned in tabs
if 'edited_df_q' not in st.session_state: st.session_state.edited_df_q = None
if 'edited_df_v' not in st.session_state: st.session_state.edited_df_v = None
if 'edited_df_di' not in st.session_state: st.session_state.edited_df_di = None


# --- Initialize validation error lists ---
validation_errors_q = []
validation_errors_v = []
validation_errors_di = []


# --- Data Input Section (Using Tabs) ---
st.header("1. ä¸Šå‚³æˆ–è²¼ä¸Šå„ç§‘æˆç¸¾å–®")

st.info(f"æç¤ºï¼šä¸Šå‚³çš„ CSV æª”æ¡ˆå¤§å°è«‹å‹¿è¶…é {MAX_FILE_SIZE_MB}MBã€‚è²¼ä¸Šçš„è³‡æ–™æ²’æœ‰æ­¤é™åˆ¶ã€‚") # Add info message

# Initialize DataFrames
df_q = None
df_v = None
df_di = None
data_sources = {}

tab_q, tab_v, tab_di = st.tabs(["Quantitative (Q)", "Verbal (V)", "Data Insights (DI)"])

with tab_q:
    st.subheader("Quantitative (Q) è³‡æ–™è¼¸å…¥")
    uploaded_file_q = st.file_uploader(
        "ä¸Šå‚³ Q ç§‘ç›® CSV æª”æ¡ˆ", 
        type="csv", 
        key="q_uploader",
        help=f"æª”æ¡ˆå¤§å°é™åˆ¶ç‚º {MAX_FILE_SIZE_MB}MBã€‚"
    )
    pasted_data_q = st.text_area("æˆ–å°‡ Q ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="q_paster")
    
    # --- Add Header Requirement Info ---
    st.caption("è«‹ç¢ºä¿è³‡æ–™åŒ…å«ä»¥ä¸‹æ¬„ä½æ¨™é¡Œ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ): 'Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type', 'Fundamental Skills'")
    # --- End Header Requirement Info ---

    temp_df_q = None
    source_q = None
    data_source_type_q = None # Track source type

    if uploaded_file_q is not None:
        if uploaded_file_q.size > MAX_FILE_SIZE_BYTES:
            st.error(f"æª”æ¡ˆå¤§å° ({uploaded_file_q.size / (1024*1024):.2f} MB) è¶…é {MAX_FILE_SIZE_MB}MB é™åˆ¶ã€‚")
            # source_q remains None
        else:
            source_q = uploaded_file_q
            data_source_type_q = 'File Upload'
    elif pasted_data_q:
        source_q = StringIO(pasted_data_q)
        data_source_type_q = 'Pasted Data'

    if source_q is not None:
        try:
            temp_df_q = pd.read_csv(source_q, sep=None, engine='python')
            
            # --- Explicitly Remove *_b columns --- 
            cols_to_drop_q = [col for col in temp_df_q.columns if str(col).endswith('_b')]
            if cols_to_drop_q:
                temp_df_q.drop(columns=cols_to_drop_q, inplace=True)
                st.caption(f"å·²è‡ªå‹•å¿½ç•¥ä»¥ä¸‹æ¬„ä½: {', '.join(cols_to_drop_q)}")
            # --- End Remove --- 
            
            # --- Initial Cleaning: Drop empty rows and columns --- 
            initial_rows_q, initial_cols_q = temp_df_q.shape
            temp_df_q.dropna(how='all', inplace=True) # Drop rows where ALL are NaN
            temp_df_q.dropna(axis=1, how='all', inplace=True) # Drop columns where ALL are NaN
            temp_df_q.reset_index(drop=True, inplace=True) # Reset index
            cleaned_rows_q, cleaned_cols_q = temp_df_q.shape
            if initial_rows_q > cleaned_rows_q or initial_cols_q > cleaned_cols_q:
                 st.caption(f"å·²è‡ªå‹•ç§»é™¤ {initial_rows_q - cleaned_rows_q} å€‹ç©ºè¡Œå’Œ {initial_cols_q - cleaned_cols_q} å€‹ç©ºåˆ—ã€‚")
            # --- End Initial Cleaning ---

            # --- Add Manual Invalid Column BEFORE editor ---
            if not temp_df_q.empty:
                temp_df_q['is_manually_invalid'] = False
            # --- End Add ---

            # --- Editable Preview --- 
            validation_errors_q = [] # Initialize error list for Q
            if not temp_df_q.empty:
                st.write("é è¦½èˆ‡ç·¨è¼¯è³‡æ–™ (ä¿®æ”¹å¾Œè«‹ç¢ºä¿æ¬„ä½ç¬¦åˆä¸‹æ–¹è¦æ±‚)ï¼š")
                try:
                    # Define desired column order, putting manual invalid first
                    q_column_order = ['is_manually_invalid'] + [col for col in temp_df_q.columns if col != 'is_manually_invalid']
                    
                    # --- Apply preprocessing to detect invalid responses ---
                    # Calculate time pressure for Q section
                    if 'question_time' in temp_df_q.columns:
                        # Simple time pressure check for preview (can be adjusted based on your specific logic)
                        q_time_values = pd.to_numeric(temp_df_q['question_time'], errors='coerce')
                        time_pressure_q = (q_time_values.max() - q_time_values.min() > Q_TIME_DIFF_PRESSURE_THRESHOLD) if len(q_time_values) > 0 else False
                    else:
                        time_pressure_q = False
                    
                    # Create time pressure map for suggest_invalid_questions
                    temp_time_pressure_map = {'Q': time_pressure_q}
                    
                    # Add Subject column for preprocessing
                    temp_df_q['Subject'] = 'Q'
                    
                    # Rename necessary columns for preprocessing
                    temp_rename_map = {}
                    if 'Response Time (Minutes)' in temp_df_q.columns:
                        temp_rename_map['Response Time (Minutes)'] = 'question_time'
                    if 'Question' in temp_df_q.columns:
                        temp_rename_map['Question'] = 'question_position'
                    elif '\ufeffQuestion' in temp_df_q.columns:
                        temp_rename_map['\ufeffQuestion'] = 'question_position'
                    
                    # Apply temporary rename for preprocessing
                    temp_preprocessing_df = temp_df_q.copy()
                    if temp_rename_map:
                        temp_preprocessing_df.rename(columns=temp_rename_map, inplace=True)
                    
                    # Apply suggestion logic
                    try:
                        processed_df_q = suggest_invalid_questions(temp_preprocessing_df, temp_time_pressure_map)
                        # Update is_manually_invalid to match auto-suggested values
                        if 'is_auto_suggested_invalid' in processed_df_q.columns:
                            temp_df_q['is_manually_invalid'] = processed_df_q['is_auto_suggested_invalid']
                    except Exception as preprocess_err:
                        st.warning(f"è‡ªå‹•æª¢æ¸¬ç„¡æ•ˆé¡Œç›®æ™‚å‡ºéŒ¯ï¼Œå°‡ä½¿ç”¨é è¨­å€¼: {preprocess_err}")
                    # --- End preprocessing ---
                    
                    # Use data_editor for viewing all data and allowing edits
                    edited_temp_df_q = st.data_editor(
                        temp_df_q,
                        key="editor_q",
                        num_rows="dynamic", # Allow adding/deleting rows
                        use_container_width=True,
                        column_order=q_column_order, # Set the column order here
                        # --- Add column config for the new checkbox ---
                        column_config={
                            "is_manually_invalid": st.column_config.CheckboxColumn(
                                "æ˜¯å¦è‰ç‡åšé¡Œï¼Ÿ (æ‰‹å‹•æ¨™è¨˜)",
                                help="å‹¾é¸æ­¤æ¡†è¡¨ç¤ºæ‚¨æ‰‹å‹•åˆ¤æ–·æ­¤é¡Œç‚ºç„¡æ•ˆï¼ˆä¾‹å¦‚å› å€‰ä¿ƒ/æ…Œäº‚ï¼‰ã€‚æ­¤æ¨™è¨˜å°‡å„ªå…ˆæ–¼ç³»çµ±è‡ªå‹•å»ºè­°ã€‚",
                                default=False,
                            )
                        }
                        # --- End config ---
                    )
                    # --- START VALIDATION ---
                    validation_errors_q = validate_dataframe(edited_temp_df_q, 'Q')
                    if validation_errors_q:
                        st.error("Qç§‘ç›®: ç™¼ç¾ä»¥ä¸‹è¼¸å…¥éŒ¯èª¤ï¼Œè«‹ä¿®æ­£ï¼š")
                        for error in validation_errors_q:
                            st.error(f"- {error}")
                        temp_df_q = None # Mark as invalid if errors found
                    else:
                        temp_df_q = edited_temp_df_q # Use the validated, edited dataframe
                    # --- END VALIDATION ---

                except Exception as editor_e:
                    st.error(f"ç·¨è¼¯å™¨è¼‰å…¥æˆ–æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{editor_e}")
                    temp_df_q = None # Stop processing this tab on editor error
            else:
                st.warning("è®€å–çš„è³‡æ–™åœ¨æ¸…ç†ç©ºè¡Œ/ç©ºåˆ—å¾Œç‚ºç©ºã€‚")
                temp_df_q = None # Set to None if empty after cleaning

            # Proceed with standardization ONLY if temp_df_q is not None (i.e., no validation errors)
            if temp_df_q is not None:
                # --- Standardize Columns (Handle potential 'Question' variations) --- 
                rename_map_q = {
                    # 'ï»¿Question': 'question_position', # Old logic
                    'Performance': 'Correct',
                    'Response Time (Minutes)': 'question_time',
                    'Question Type': 'question_type',
                    'Content Domain': 'content_domain',
                    'Fundamental Skills': 'question_fundamental_skill'
                }
                # Dynamically add the position mapping based on columns found in the *edited* df
                if '\ufeffQuestion' in temp_df_q.columns: # Check for BOM + Question first
                    rename_map_q['\ufeffQuestion'] = 'question_position'
                elif 'Question' in temp_df_q.columns: # Check for plain 'Question'
                    rename_map_q['Question'] = 'question_position'

                # Apply only columns that exist in the edited dataframe
                cols_to_rename = {k: v for k, v in rename_map_q.items() if k in temp_df_q.columns}
                if cols_to_rename:
                     temp_df_q.rename(columns=cols_to_rename, inplace=True)

                # Standardize question_type AFTER renaming
                if 'question_type' in temp_df_q.columns:
                    temp_df_q['question_type'] = temp_df_q['question_type'].astype(str).str.strip().str.upper()
                else:
                     st.warning("Q: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'question_type' (æˆ–åŸå§‹å°æ‡‰) æ¬„ä½ã€‚", icon="âš ï¸")

                if 'Correct' in temp_df_q.columns:
                     # Convert to boolean consistently
                     temp_df_q['Correct'] = temp_df_q['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                     temp_df_q.rename(columns={'Correct': 'is_correct'}, inplace=True) # Rename to is_correct
                else:
                    st.error("Q: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'Performance'/'Correct' æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚", icon="ğŸš¨")
                    temp_df_q = None # Mark as invalid

                if temp_df_q is not None:
                    data_sources['Q'] = data_source_type_q
                    st.success(f"Q ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['Q']})ï¼")
                    temp_df_q['Subject'] = 'Q' # Add subject identifier

                    # --- Ensure independent question_position for Q --- 
                    # Check if position exists AND is usable after potential edits/renaming
                    if 'question_position' not in temp_df_q.columns or pd.to_numeric(temp_df_q['question_position'], errors='coerce').isnull().any():
                        st.warning("Q: ç¼ºå°‘ 'Question'/'question_position' æ¬„ä½æˆ–åŒ…å«ç„¡æ•ˆå€¼ï¼Œæ­£åœ¨æ ¹æ“šç•¶å‰é †åºé‡æ–°ç”Ÿæˆé¡Œè™Ÿã€‚", icon="âš ï¸")
                        # Use reset_index again to ensure consecutive numbering after edits
                        temp_df_q = temp_df_q.reset_index(drop=True)
                        temp_df_q['question_position'] = temp_df_q.index + 1
                    else:
                        # Ensure it's integer type after validation
                        temp_df_q['question_position'] = pd.to_numeric(temp_df_q['question_position'], errors='coerce').astype('Int64')
                    # --- End Ensure --- 
                    df_q = temp_df_q # Assign the fully processed df to df_q

            # Reset df_q if temp_df_q ended up being None after standardization/validation
            if temp_df_q is None:
                df_q = None
        except Exception as e:
            st.error(f"è™•ç† Q ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_q = None # Ensure df_q is None on error

with tab_v:
    st.subheader("Verbal (V) è³‡æ–™è¼¸å…¥")
    uploaded_file_v = st.file_uploader(
        "ä¸Šå‚³ V ç§‘ç›® CSV æª”æ¡ˆ", 
        type="csv", 
        key="v_uploader",
        help=f"æª”æ¡ˆå¤§å°é™åˆ¶ç‚º {MAX_FILE_SIZE_MB}MBã€‚"
    )
    pasted_data_v = st.text_area("æˆ–å°‡ V ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="v_paster")
    
    # --- Add Header Requirement Info ---
    st.caption("è«‹ç¢ºä¿è³‡æ–™åŒ…å«ä»¥ä¸‹æ¬„ä½æ¨™é¡Œ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ): 'Question', 'Response Time (Minutes)', 'Performance', 'Question Type', 'Fundamental Skills'") # Note: Content Domain is often N/A for Verbal but check if your specific source requires it.
    # --- End Header Requirement Info ---

    temp_df_v = None
    source_v = None
    data_source_type_v = None

    if uploaded_file_v is not None:
        if uploaded_file_v.size > MAX_FILE_SIZE_BYTES:
            st.error(f"æª”æ¡ˆå¤§å° ({uploaded_file_v.size / (1024*1024):.2f} MB) è¶…é {MAX_FILE_SIZE_MB}MB é™åˆ¶ã€‚")
        else:
            source_v = uploaded_file_v
            data_source_type_v = 'File Upload'
    elif pasted_data_v:
        source_v = StringIO(pasted_data_v)
        data_source_type_v = 'Pasted Data'

    if source_v is not None:
        try:
            temp_df_v = pd.read_csv(source_v, sep=None, engine='python')
            
            # --- Explicitly Remove *_b columns --- 
            cols_to_drop_v = [col for col in temp_df_v.columns if str(col).endswith('_b')]
            if cols_to_drop_v:
                temp_df_v.drop(columns=cols_to_drop_v, inplace=True)
                st.caption(f"å·²è‡ªå‹•å¿½ç•¥ä»¥ä¸‹æ¬„ä½: {', '.join(cols_to_drop_v)}")
            # --- End Remove --- 
            
            # --- Initial Cleaning --- 
            initial_rows_v, initial_cols_v = temp_df_v.shape
            temp_df_v.dropna(how='all', inplace=True)
            temp_df_v.dropna(axis=1, how='all', inplace=True)
            temp_df_v.reset_index(drop=True, inplace=True)
            cleaned_rows_v, cleaned_cols_v = temp_df_v.shape
            if initial_rows_v > cleaned_rows_v or initial_cols_v > cleaned_cols_v:
                 st.caption(f"å·²è‡ªå‹•ç§»é™¤ {initial_rows_v - cleaned_rows_v} å€‹ç©ºè¡Œå’Œ {initial_cols_v - cleaned_cols_v} å€‹ç©ºåˆ—ã€‚")
            # --- End Initial Cleaning ---

            # --- Add Manual Invalid Column BEFORE editor ---
            if not temp_df_v.empty:
                temp_df_v['is_manually_invalid'] = False
            # --- End Add ---

            # --- Editable Preview --- 
            validation_errors_v = [] # Initialize error list for V
            if not temp_df_v.empty:
                 st.write("é è¦½èˆ‡ç·¨è¼¯è³‡æ–™ (ä¿®æ”¹å¾Œè«‹ç¢ºä¿æ¬„ä½ç¬¦åˆä¸‹æ–¹è¦æ±‚)ï¼š")
                 try:
                    # Define desired column order, putting manual invalid first
                    v_column_order = ['is_manually_invalid'] + [col for col in temp_df_v.columns if col != 'is_manually_invalid']
                    
                    # --- Apply preprocessing to detect invalid responses ---
                    # Add Subject column for preprocessing
                    temp_df_v['Subject'] = 'V'
                    
                    # Rename necessary columns for preprocessing
                    temp_rename_map = {}
                    if 'Response Time (Minutes)' in temp_df_v.columns:
                        temp_rename_map['Response Time (Minutes)'] = 'question_time'
                    if 'Question' in temp_df_v.columns:
                        temp_rename_map['Question'] = 'question_position'
                    elif '\ufeffQuestion' in temp_df_v.columns:
                        temp_rename_map['\ufeffQuestion'] = 'question_position'
                    
                    # Apply temporary rename for preprocessing
                    temp_preprocessing_df = temp_df_v.copy()
                    if temp_rename_map:
                        temp_preprocessing_df.rename(columns=temp_rename_map, inplace=True)
                    
                    # Create an empty time pressure map (V uses simpler logic)
                    temp_time_pressure_map = {'V': False}  # Væ£€æµ‹ç®€å•è§„åˆ™ä¸ä¾èµ–äºæ—¶é—´å‹åŠ›çŠ¶æ€
                    
                    # Apply suggestion logic
                    try:
                        processed_df_v = suggest_invalid_questions(temp_preprocessing_df, temp_time_pressure_map)
                        # Update is_manually_invalid to match auto-suggested values
                        if 'is_auto_suggested_invalid' in processed_df_v.columns:
                            temp_df_v['is_manually_invalid'] = processed_df_v['is_auto_suggested_invalid']
                    except Exception as preprocess_err:
                        st.warning(f"è‡ªå‹•æª¢æ¸¬ç„¡æ•ˆé¡Œç›®æ™‚å‡ºéŒ¯ï¼Œå°‡ä½¿ç”¨é è¨­å€¼: {preprocess_err}")
                    # --- End preprocessing ---
                    
                    edited_temp_df_v = st.data_editor(
                        temp_df_v,
                        key="editor_v",
                        num_rows="dynamic",
                        use_container_width=True,
                        column_order=v_column_order, # Set the column order here
                        # --- Add column config for the new checkbox ---
                        column_config={
                            "is_manually_invalid": st.column_config.CheckboxColumn(
                                "æ˜¯å¦è‰ç‡åšé¡Œï¼Ÿ (æ‰‹å‹•æ¨™è¨˜)",
                                help="å‹¾é¸æ­¤æ¡†è¡¨ç¤ºæ‚¨æ‰‹å‹•åˆ¤æ–·æ­¤é¡Œç‚ºç„¡æ•ˆï¼ˆä¾‹å¦‚å› å€‰ä¿ƒ/æ…Œäº‚ï¼‰ã€‚æ­¤æ¨™è¨˜å°‡å„ªå…ˆæ–¼ç³»çµ±è‡ªå‹•å»ºè­°ã€‚",
                                default=False,
                            )
                        }
                        # --- End config ---
                    )
                    # --- START VALIDATION --- 
                    validation_errors_v = validate_dataframe(edited_temp_df_v, 'V')
                    if validation_errors_v:
                        st.error("Vç§‘ç›®: ç™¼ç¾ä»¥ä¸‹è¼¸å…¥éŒ¯èª¤ï¼Œè«‹ä¿®æ­£ï¼š")
                        for error in validation_errors_v:
                            st.error(f"- {error}")
                        temp_df_v = None # Mark as invalid
                    else:
                        temp_df_v = edited_temp_df_v # Use validated df
                    # --- END VALIDATION --- 
                 except Exception as editor_e:
                     st.error(f"ç·¨è¼¯å™¨è¼‰å…¥æˆ–æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{editor_e}")
                     temp_df_v = None
            else:
                 st.warning("è®€å–çš„è³‡æ–™åœ¨æ¸…ç†ç©ºè¡Œ/ç©ºåˆ—å¾Œç‚ºç©ºã€‚")
                 temp_df_v = None

            # Proceed with standardization ONLY if temp_df_v is not None
            if temp_df_v is not None:
                # --- Standardize Columns --- 
                rename_map_v = {
                    # 'ï»¿Question': 'question_position',
                    'Performance': 'Correct',
                    'Response Time (Minutes)': 'question_time',
                    'Question Type': 'question_type',
                    'Fundamental Skills': 'question_fundamental_skill'
                }
                if '\ufeffQuestion' in temp_df_v.columns:
                    rename_map_v['\ufeffQuestion'] = 'question_position'
                elif 'Question' in temp_df_v.columns:
                    rename_map_v['Question'] = 'question_position'
                
                cols_to_rename = {k: v for k, v in rename_map_v.items() if k in temp_df_v.columns}
                if cols_to_rename:
                    temp_df_v.rename(columns=cols_to_rename, inplace=True)

                # Standardize question_type AFTER renaming (Remove .str.upper() for V)
                if 'question_type' in temp_df_v.columns:
                    temp_df_v['question_type'] = temp_df_v['question_type'].astype(str).str.strip() # Keep original case for V
                else:
                     st.warning("V: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'question_type' (æˆ–åŸå§‹å°æ‡‰) æ¬„ä½ã€‚", icon="âš ï¸")

                if 'Correct' in temp_df_v.columns:
                     temp_df_v['Correct'] = temp_df_v['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                     temp_df_v.rename(columns={'Correct': 'is_correct'}, inplace=True)
                else:
                     st.error("V: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'Performance'/'Correct' æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚", icon="ğŸš¨")
                     temp_df_v = None

                if temp_df_v is not None:
                    data_sources['V'] = data_source_type_v
                    st.success(f"V ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['V']})ï¼")
                    temp_df_v['Subject'] = 'V'

                    # --- Ensure independent question_position for V --- 
                    if 'question_position' not in temp_df_v.columns or pd.to_numeric(temp_df_v['question_position'], errors='coerce').isnull().any():
                        st.warning("V: ç¼ºå°‘ 'Question'/'question_position' æ¬„ä½æˆ–åŒ…å«ç„¡æ•ˆå€¼ï¼Œæ­£åœ¨æ ¹æ“šç•¶å‰é †åºé‡æ–°ç”Ÿæˆé¡Œè™Ÿã€‚", icon="âš ï¸")
                        temp_df_v = temp_df_v.reset_index(drop=True)
                        temp_df_v['question_position'] = temp_df_v.index + 1
                    else:
                        temp_df_v['question_position'] = pd.to_numeric(temp_df_v['question_position'], errors='coerce').astype('Int64')
                    # --- End Ensure --- 
                    df_v = temp_df_v

            # Reset df_v if temp_df_v ended up being None after standardization/validation
            if temp_df_v is None:
                df_v = None
        except Exception as e:
            st.error(f"è™•ç† V ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_v = None

with tab_di:
    st.subheader("Data Insights (DI) è³‡æ–™è¼¸å…¥")
    uploaded_file_di = st.file_uploader(
        "ä¸Šå‚³ DI ç§‘ç›® CSV æª”æ¡ˆ", 
        type="csv", 
        key="di_uploader",
        help=f"æª”æ¡ˆå¤§å°é™åˆ¶ç‚º {MAX_FILE_SIZE_MB}MBã€‚"
    )
    pasted_data_di = st.text_area("æˆ–å°‡ DI ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š", height=150, key="di_paster")
    
    # --- Add Header Requirement Info ---
    st.caption("è«‹ç¢ºä¿è³‡æ–™åŒ…å«ä»¥ä¸‹æ¬„ä½æ¨™é¡Œ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ): 'Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type'") # Note: Fundamental Skills is often N/A for DI but check if your specific source requires it.
    # --- End Header Requirement Info ---
    
    temp_df_di = None
    source_di = None
    data_source_type_di = None

    if uploaded_file_di is not None:
        if uploaded_file_di.size > MAX_FILE_SIZE_BYTES:
            st.error(f"æª”æ¡ˆå¤§å° ({uploaded_file_di.size / (1024*1024):.2f} MB) è¶…é {MAX_FILE_SIZE_MB}MB é™åˆ¶ã€‚")
        else:
            source_di = uploaded_file_di
            data_source_type_di = 'File Upload'
    elif pasted_data_di:
        source_di = StringIO(pasted_data_di)
        data_source_type_di = 'Pasted Data'

    if source_di is not None:
        try:
            temp_df_di = pd.read_csv(source_di, sep=None, engine='python')
            
            # --- Explicitly Remove *_b columns --- 
            cols_to_drop_di = [col for col in temp_df_di.columns if str(col).endswith('_b')]
            if cols_to_drop_di:
                temp_df_di.drop(columns=cols_to_drop_di, inplace=True)
                st.caption(f"å·²è‡ªå‹•å¿½ç•¥ä»¥ä¸‹æ¬„ä½: {', '.join(cols_to_drop_di)}")
            # --- End Remove --- 
            
            # --- Initial Cleaning --- 
            initial_rows_di, initial_cols_di = temp_df_di.shape
            temp_df_di.dropna(how='all', inplace=True)
            temp_df_di.dropna(axis=1, how='all', inplace=True)
            temp_df_di.reset_index(drop=True, inplace=True)
            cleaned_rows_di, cleaned_cols_di = temp_df_di.shape
            if initial_rows_di > cleaned_rows_di or initial_cols_di > cleaned_cols_di:
                 st.caption(f"å·²è‡ªå‹•ç§»é™¤ {initial_rows_di - cleaned_rows_di} å€‹ç©ºè¡Œå’Œ {initial_cols_di - cleaned_cols_di} å€‹ç©ºåˆ—ã€‚")
            # --- End Initial Cleaning ---

            # --- Add Manual Invalid Column BEFORE editor ---
            if not temp_df_di.empty:
                temp_df_di['is_manually_invalid'] = False
            # --- End Add ---

            # --- Editable Preview --- 
            validation_errors_di = [] # Initialize error list for DI
            if not temp_df_di.empty:
                 st.write("é è¦½èˆ‡ç·¨è¼¯è³‡æ–™ (ä¿®æ”¹å¾Œè«‹ç¢ºä¿æ¬„ä½ç¬¦åˆä¸‹æ–¹è¦æ±‚)ï¼š")
                 try:
                     # Define desired column order, putting manual invalid first
                     di_column_order = ['is_manually_invalid'] + [col for col in temp_df_di.columns if col != 'is_manually_invalid']
                     
                     # --- Apply preprocessing to detect invalid responses ---
                     # Add Subject column for preprocessing
                     temp_df_di['Subject'] = 'DI'
                     
                     # Calculate time pressure for DI section
                     if 'Response Time (Minutes)' in temp_df_di.columns:
                         # Simple time pressure check for preview
                         di_time_values = pd.to_numeric(temp_df_di['Response Time (Minutes)'], errors='coerce')
                         time_pressure_di = (di_time_values.max() - di_time_values.min() > DI_TIME_PRESSURE_THRESHOLD_MINUTES) if len(di_time_values) > 0 else False
                     else:
                         time_pressure_di = False
                     
                     # Create time pressure map for suggest_invalid_questions
                     temp_time_pressure_map = {'DI': time_pressure_di}
                     
                     # Rename necessary columns for preprocessing
                     temp_rename_map = {}
                     if 'Response Time (Minutes)' in temp_df_di.columns:
                         temp_rename_map['Response Time (Minutes)'] = 'question_time'
                     if 'Question' in temp_df_di.columns:
                         temp_rename_map['Question'] = 'question_position'
                     elif '\ufeffQuestion' in temp_df_di.columns:
                         temp_rename_map['\ufeffQuestion'] = 'question_position'
                     
                     # Apply temporary rename for preprocessing
                     temp_preprocessing_df = temp_df_di.copy()
                     if temp_rename_map:
                         temp_preprocessing_df.rename(columns=temp_rename_map, inplace=True)
                     
                     # Apply suggestion logic
                     try:
                         processed_df_di = suggest_invalid_questions(temp_preprocessing_df, temp_time_pressure_map)
                         # Update is_manually_invalid to match auto-suggested values
                         if 'is_auto_suggested_invalid' in processed_df_di.columns:
                             temp_df_di['is_manually_invalid'] = processed_df_di['is_auto_suggested_invalid']
                     except Exception as preprocess_err:
                         st.warning(f"è‡ªå‹•æª¢æ¸¬ç„¡æ•ˆé¡Œç›®æ™‚å‡ºéŒ¯ï¼Œå°‡ä½¿ç”¨é è¨­å€¼: {preprocess_err}")
                     # --- End preprocessing ---
                     
                     edited_temp_df_di = st.data_editor(
                         temp_df_di,
                         key="editor_di",
                         num_rows="dynamic",
                         use_container_width=True,
                         column_order=di_column_order, # Set the column order here
                         # --- Add column config for the new checkbox ---
                        column_config={
                            "is_manually_invalid": st.column_config.CheckboxColumn(
                                "æ˜¯å¦è‰ç‡åšé¡Œï¼Ÿ (æ‰‹å‹•æ¨™è¨˜)",
                                help="å‹¾é¸æ­¤æ¡†è¡¨ç¤ºæ‚¨æ‰‹å‹•åˆ¤æ–·æ­¤é¡Œç‚ºç„¡æ•ˆï¼ˆä¾‹å¦‚å› å€‰ä¿ƒ/æ…Œäº‚ï¼‰ã€‚æ­¤æ¨™è¨˜å°‡å„ªå…ˆæ–¼ç³»çµ±è‡ªå‹•å»ºè­°ã€‚",
                                default=False,
                            )
                        }
                        # --- End config ---
                     )
                     # --- START VALIDATION --- 
                     validation_errors_di = validate_dataframe(edited_temp_df_di, 'DI')
                     if validation_errors_di:
                         st.error("DIç§‘ç›®: ç™¼ç¾ä»¥ä¸‹è¼¸å…¥éŒ¯èª¤ï¼Œè«‹ä¿®æ­£ï¼š")
                         for error in validation_errors_di:
                             st.error(f"- {error}")
                         temp_df_di = None # Mark as invalid
                     else:
                         temp_df_di = edited_temp_df_di # Use validated df
                     # --- END VALIDATION --- 
                 except Exception as editor_e:
                     st.error(f"ç·¨è¼¯å™¨è¼‰å…¥æˆ–æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{editor_e}")
                     temp_df_di = None
            else:
                 st.warning("è®€å–çš„è³‡æ–™åœ¨æ¸…ç†ç©ºè¡Œ/ç©ºåˆ—å¾Œç‚ºç©ºã€‚")
                 temp_df_di = None

            # Proceed with standardization ONLY if temp_df_di is not None
            if temp_df_di is not None:
                # --- Standardize Columns --- 
                rename_map_di = {
                    # 'ï»¿Question': 'question_position',
                    'Performance': 'Correct',
                    'Response Time (Minutes)': 'question_time',
                    'Question Type': 'question_type',
                    'Content Domain': 'content_domain'
                }
                if '\ufeffQuestion' in temp_df_di.columns:
                    rename_map_di['\ufeffQuestion'] = 'question_position'
                elif 'Question' in temp_df_di.columns:
                    rename_map_di['Question'] = 'question_position'

                cols_to_rename = {k: v for k, v in rename_map_di.items() if k in temp_df_di.columns}
                if cols_to_rename:
                    temp_df_di.rename(columns=cols_to_rename, inplace=True)
                
                # Standardize question_type AFTER renaming (Remove .str.upper() for DI)
                if 'question_type' in temp_df_di.columns:
                    temp_df_di['question_type'] = temp_df_di['question_type'].astype(str).str.strip() # Keep original case for DI
                else:
                    st.warning("DI: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'question_type' (æˆ–åŸå§‹å°æ‡‰) æ¬„ä½ã€‚", icon="âš ï¸")
                
                if 'Correct' in temp_df_di.columns:
                     temp_df_di['Correct'] = temp_df_di['Correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
                     temp_df_di.rename(columns={'Correct': 'is_correct'}, inplace=True) # Rename to is_correct
                else:
                     st.error("DI: ç·¨è¼¯å¾Œçš„è³‡æ–™ç¼ºå°‘ 'Performance'/'Correct' æ¬„ä½ï¼Œç„¡æ³•ç¢ºå®šéŒ¯èª¤é¡Œç›®ã€‚", icon="ğŸš¨")
                     temp_df_di = None

                if temp_df_di is not None:
                    data_sources['DI'] = data_source_type_di
                    st.success(f"DI ç§‘ç›®è³‡æ–™è®€å–æˆåŠŸ ({data_sources['DI']})ï¼")
                    temp_df_di['Subject'] = 'DI'

                    # --- Ensure independent question_position for DI --- 
                    if 'question_position' not in temp_df_di.columns or pd.to_numeric(temp_df_di['question_position'], errors='coerce').isnull().any():
                        st.warning("DI: ç¼ºå°‘ 'Question'/'question_position' æ¬„ä½æˆ–åŒ…å«ç„¡æ•ˆå€¼ï¼Œæ­£åœ¨æ ¹æ“šç•¶å‰é †åºé‡æ–°ç”Ÿæˆé¡Œè™Ÿã€‚", icon="âš ï¸")
                        temp_df_di = temp_df_di.reset_index(drop=True)
                        temp_df_di['question_position'] = temp_df_di.index + 1
                    else:
                        temp_df_di['question_position'] = pd.to_numeric(temp_df_di['question_position'], errors='coerce').astype('Int64')
                    # --- End Ensure --- 
                    df_di = temp_df_di

            # Reset df_di if temp_df_di ended up being None after standardization/validation
            if temp_df_di is None:
                 df_di = None
        except Exception as e:
            st.error(f"è™•ç† DI ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            df_di = None

# --- Combine Input Data (After Tabs) ---
# Use the potentially edited and VALIDATED dataframes (df_q, df_v, df_di)
input_dfs = {'Q': df_q, 'V': df_v, 'DI': df_di}
# Filter out subjects where df is None (either initially or due to validation error)
loaded_subjects = {subj for subj, df in input_dfs.items() if df is not None}
# Create list of valid dataframes only
df_combined_input_list = [df for df in [df_q, df_v, df_di] if df is not None]
df_combined_input = None # Initialize

# Check if *any* data was loaded initially, even if it failed validation later
any_data_attempted = bool(data_sources) # Check if any source was identified

if df_combined_input_list: # Only proceed if there's at least one *valid* df
     try:
         # Concatenate with ignore_index=True, but 'question_position' is already calculated per subject
         df_combined_input = pd.concat(df_combined_input_list, ignore_index=True)

         # Ensure 'question_position' column exists after concat (it should, unless all inputs failed processing)
         if 'question_position' not in df_combined_input.columns:
             st.error("åˆä½µå¾Œè³‡æ–™ç¼ºå°‘ 'question_position' æ¬„ä½ï¼Œç„¡æ³•ç¹¼çºŒã€‚æª¢æŸ¥å„ç§‘æ•¸æ“šè™•ç†ã€‚")
             df_combined_input = None
         elif df_combined_input['question_position'].isnull().any():
              st.error("åˆä½µå¾Œ 'question_position' æ¬„ä½åŒ…å«ç©ºå€¼ï¼Œç„¡æ³•ç¹¼çºŒã€‚æª¢æŸ¥å„ç§‘æ•¸æ“šè™•ç†ã€‚")
              df_combined_input = None
         # else:
         #     # Optional: Sort for better viewing/debugging, though later logic sorts per subject
         #     df_combined_input.sort_values(by=['Subject', 'question_position'], inplace=True)

     except Exception as e:
         st.error(f"åˆä½µ *æœ‰æ•ˆ* è¼¸å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
         df_combined_input = None
elif any_data_attempted: # Data was loaded/pasted but ALL failed validation or processing
     st.warning("æ‰€æœ‰è¼¸å…¥çš„ç§‘ç›®æ•¸æ“šå‡æœªèƒ½é€šéé©—è­‰æˆ–è™•ç†ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹å„åˆ†é çš„éŒ¯èª¤ä¿¡æ¯ã€‚åˆ†æç„¡æ³•é€²è¡Œã€‚", icon="âš ï¸") # Changed icon
# else: No data was ever loaded or pasted, message handled below

# --- Analysis Trigger Button ---
st.divider() # Add a visual separator

# Check if any validation errors occurred across all tabs
any_validation_errors = bool(validation_errors_q) or bool(validation_errors_v) or bool(validation_errors_di)

# Display the button differently based on whether data is ready AND valid
if df_combined_input is not None and not any_validation_errors:
    # Case 1: Data is combined and NO validation errors exist - Enable button
    if st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary", key="analyze_button"):
        st.session_state.analysis_run = True
        # Reset previous results when starting new analysis
        st.session_state.report_dict = {}
        st.session_state.ai_summary = None
        st.session_state.final_thetas = {}
    else:
        # If button not clicked in this run, keep analysis_run as False unless it was already True
        # This prevents analysis from running just on widget interaction after first run
        # st.session_state.analysis_run = st.session_state.get('analysis_run', False) # Keep existing state if button not clicked
        pass # No need to explicitly set to false, just don't set to true

elif any_data_attempted:
    # Case 2: Data was attempted, but either failed combination OR had validation errors
    st.error("æ•¸æ“šé©—è­‰å¤±æ•—æˆ–ç„¡æ³•åˆä½µï¼Œè«‹ä¿®æ­£ä¸Šæ–¹æ¨™ç¤ºçš„éŒ¯èª¤å¾Œå†è©¦ã€‚")
    st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary", disabled=True, key="analyze_button_disabled_invalid") # Disable button

else: # Case 3: No data was ever loaded or pasted
    st.info("è«‹åœ¨ä¸Šæ–¹åˆ†é ä¸­ç‚ºè‡³å°‘ä¸€å€‹ç§‘ç›®ä¸Šå‚³æˆ–è²¼ä¸Šè³‡æ–™ã€‚")
    st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary", disabled=True, key="analyze_button_disabled_no_data") # Disable button

# --- Simulation, Processing, Diagnosis, and Output Tabs (Conditional Execution) ---
if st.session_state.analysis_run and df_combined_input is not None:

    st.header("2. åŸ·è¡Œ IRT æ¨¡æ“¬èˆ‡è¨ºæ–·") # Combine headers
    all_simulation_histories = {} # Store histories per subject
    final_thetas_local = {}     # Store final theta per subject locally for this run
    df_final_for_diagnosis = None # Initialize
    df_processed_after_suggestion = None # Initialize df after preprocessing

    # --- Calculate Time Pressure (using combined validated input data) ---
    time_pressure_map = {}
    try:
        # Your existing time pressure calculation logic should go here
        # Example placeholder:
        st.write("Calculating Time Pressure...") # Placeholder
        time_pressure_q = False # Replace with actual calculation for Q
        time_pressure_v = False # Replace with actual calculation for V
        time_pressure_di = False # Replace with actual calculation for DI
        time_pressure_map = {'Q': time_pressure_q, 'V': time_pressure_v, 'DI': time_pressure_di}
        st.write(f"Time Pressure Map: {time_pressure_map}") # Placeholder
    except Exception as e:
        st.error(f"è¨ˆç®—æ™‚é–“å£“åŠ›æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        st.session_state.analysis_run = False # Stop analysis
        st.stop()

    # --- Apply Preprocessing to Suggest Invalid Questions --- 
    try:
        st.write("æ‡‰ç”¨é è™•ç†è¦å‰‡ä»¥å»ºè­°ç„¡æ•ˆé¡Œç›®...")
        df_processed_after_suggestion = suggest_invalid_questions(df_combined_input, time_pressure_map)
        st.write("é è™•ç†å®Œæˆã€‚å»ºè­°çš„ç„¡æ•ˆç‹€æ…‹å·²åˆä½µåˆ° 'is_invalid' æ¬„ä½ã€‚")
        # Optional: Display a preview of the suggestions if helpful for debugging
        # st.write("é è™•ç†å¾Œæ•¸æ“šé è¦½ (å‰5è¡Œ):")
        # st.dataframe(df_processed_after_suggestion.head())
    except Exception as e:
        st.error(f"é è™•ç†å»ºè­°ç„¡æ•ˆé¡Œç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        st.session_state.analysis_run = False # Stop analysis
        st.stop()

    # --- IRT Simulation --- 
    simulation_success = True
    # with st.spinner("æ­£åœ¨åŸ·è¡Œ IRT æ¨¡æ“¬..."): # Replace with st.status
    with st.status("åŸ·è¡Œ IRT æ¨¡æ“¬...", expanded=True) as status:
        st.write("åˆå§‹åŒ–æ¨¡æ“¬é¡Œåº«...")
        question_banks = {}
        try:
            # Create banks only for loaded subjects
            if 'Q' in loaded_subjects: question_banks['Q'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED)
            if 'V' in loaded_subjects: question_banks['V'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED + 1)
            if 'DI' in loaded_subjects: question_banks['DI'] = irt.initialize_question_bank(BANK_SIZE, seed=RANDOM_SEED + 2)
            # Check for failure after trying to create all needed banks
            if any(subj in loaded_subjects and question_banks.get(subj) is None for subj in loaded_subjects):
                 st.error("å‰µå»ºæ¨¡æ“¬é¡Œåº«å¤±æ•—ã€‚")
                 simulation_success = False
                 status.update(label="æ¨¡æ“¬é¡Œåº«å‰µå»ºå¤±æ•—", state="error", expanded=True)
            else:
                 st.write("æ¨¡æ“¬é¡Œåº«å‰µå»ºå®Œæˆã€‚")
        except Exception as e:
            st.error(f"å‰µå»ºæ¨¡æ“¬é¡Œåº«æ™‚å‡ºéŒ¯: {e}")
            simulation_success = False
            status.update(label=f"æ¨¡æ“¬é¡Œåº«å‰µå»ºå‡ºéŒ¯: {e}", state="error", expanded=True)

        if simulation_success:
            subject_params = {
                'Q': {'initial_theta': initial_theta_q, 'total_questions': TOTAL_QUESTIONS_Q},
                'V': {'initial_theta': initial_theta_v, 'total_questions': TOTAL_QUESTIONS_V},
                'DI': {'initial_theta': initial_theta_di, 'total_questions': TOTAL_QUESTIONS_DI}
            }

            for subject in loaded_subjects:
                st.write(f"åŸ·è¡Œ {subject} ç§‘ç›®æ¨¡æ“¬...")
                # Get parameters for simulation
                params = subject_params[subject]
                initial_theta = params['initial_theta']
                total_sim_questions = params['total_questions'] # Number of questions IN SIMULATION
                bank = question_banks[subject]
                
                # Get WRONG indices from the *original* user data for this subject
                user_df_subj = df_combined_input[df_combined_input['Subject'] == subject]
                wrong_indices = [] # Keep variable name for now, but it holds positions
                # Check for the final column name 'is_correct'
                if 'is_correct' in user_df_subj.columns:
                    # Sort by position before getting indices
                    user_df_subj_sorted = user_df_subj.sort_values(by='question_position')
                    # Filter using the final column name 'is_correct'
                    # --- CORRECTED LOGIC HERE ---
                    # Directly get the 'question_position' values of incorrect answers
                    # Use the dataframe AFTER preprocessing suggestion for simulation input
                    user_df_subj_processed = df_processed_after_suggestion[df_processed_after_suggestion['Subject'] == subject].sort_values(by='question_position')
                    wrong_positions = user_df_subj_processed[user_df_subj_processed['is_correct'] == False]['question_position'].tolist()
                    wrong_indices = wrong_positions # Assign to the variable expected by the simulation function
                    # --- END CORRECTION ---
                    st.write(f"  {subject}: å¾ç”¨æˆ¶æ•¸æ“šæå– {len(wrong_indices)} å€‹éŒ¯èª¤é¡Œç›®ä½ç½®: {wrong_indices}")
                else:
                    # This warning should now only appear if 'Performance' was truly missing initially
                    st.warning(f"  {subject}: ç”¨æˆ¶æ•¸æ“šç¼ºå°‘ 'is_correct' æ¬„ä½ (æºè‡ª 'Performance')ï¼Œå‡è¨­å…¨éƒ¨ç­”å°é€²è¡Œæ¨¡æ“¬ã€‚")
                    wrong_indices = []

                # Run the simulation
                try:
                    history_df = irt.simulate_cat_exam(
                        question_bank=bank,
                        wrong_question_indices=wrong_indices, # Pass the list of actual wrong positions
                        initial_theta=initial_theta,
                        total_questions=total_sim_questions # Use the simulation total questions
                    )
                    if history_df is not None and not history_df.empty:
                        all_simulation_histories[subject] = history_df
                        # Store final theta locally first
                        final_theta_subj = history_df['theta_est_after_answer'].iloc[-1] # Corrected variable name
                        final_thetas_local[subject] = final_theta_subj # Corrected variable name
                        st.write(f"  {subject}: æ¨¡æ“¬å®Œæˆã€‚æœ€å¾Œ Theta ä¼°è¨ˆ: {final_theta_subj:.3f}")
                    elif history_df is not None and history_df.empty:
                        st.warning(f"  {subject}: æ¨¡æ“¬åŸ·è¡Œäº†ï¼Œä½†æœªç”¢ç”Ÿæ­·å²è¨˜éŒ„ã€‚")
                        simulation_success = False # Treat empty history as failure for next steps
                    else:
                         st.error(f"  {subject}: æ¨¡æ“¬åŸ·è¡Œå¤±æ•—ï¼Œè¿”å› Noneã€‚")
                         simulation_success = False
                         break # Stop simulation for other subjects if one fails
                except Exception as e:
                    st.error(f"  {subject}: åŸ·è¡Œæ¨¡æ“¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    simulation_success = False
                    break # Stop simulation

        if simulation_success and all_simulation_histories:
            status.update(label="IRT æ¨¡æ“¬å®Œæˆï¼", state="complete", expanded=False)
            st.session_state.final_thetas = final_thetas_local # Store final thetas in session state
        elif simulation_success: # Banks created, but simulation failed for some reason
             status.update(label="IRT æ¨¡æ“¬éƒ¨åˆ†å¤±æ•—æˆ–æœªç”¢ç”Ÿçµæœ", state="error", expanded=True)
             simulation_success = False # Ensure it's false
        # Else: Bank creation failed, status already set to error

    # --- Prepare Data for Diagnosis ---
    # st.header("2. æº–å‚™è¨ºæ–·æ•¸æ“š (çµåˆç”¨æˆ¶æ•¸æ“šèˆ‡æ¨¡æ“¬é›£åº¦)") # Combined into header 2
    if simulation_success and df_processed_after_suggestion is not None:
        # with st.spinner("æº–å‚™è¨ºæ–·æ•¸æ“šä¸­..."): # Replace with st.status
        with st.status("æº–å‚™è¨ºæ–·æ•¸æ“š...", expanded=True) as status_prep:
            df_final_for_diagnosis_list = []
            processing_error = False
            for subject in loaded_subjects:
                st.write(f"è™•ç† {subject} ç§‘ç›®...")
                # Use the dataframe that went through invalid suggestion
                user_df_subj_processed = df_processed_after_suggestion[df_processed_after_suggestion['Subject'] == subject].copy()
                sim_history_df = all_simulation_histories.get(subject)
                final_theta = final_thetas_local.get(subject)

                if sim_history_df is None or sim_history_df.empty:
                    st.error(f"æ‰¾ä¸åˆ° {subject} ç§‘ç›®çš„æœ‰æ•ˆæ¨¡æ“¬çµæœï¼Œç„¡æ³•ç¹¼çºŒã€‚")
                    processing_error = True; status_prep.update(state="error"); break

                # Extract simulated b-values
                sim_b_values = sim_history_df['b'].tolist()
                
                # Sort user data by position
                user_df_subj_sorted = user_df_subj_processed.sort_values(by='question_position')
                num_user_questions = len(user_df_subj_sorted)
                num_sim_b = len(sim_b_values)

                # Check for length mismatch between user data and simulation results
                if num_user_questions != num_sim_b:
                     st.warning(f"{subject}: ç”¨æˆ¶æ•¸æ“šé¡Œç›®æ•¸ ({num_user_questions}) èˆ‡æ¨¡æ“¬çµæœæ•¸ ({num_sim_b}) ä¸ç¬¦ã€‚" 
                               f"å°‡åƒ…ä½¿ç”¨å‰ {min(num_user_questions, num_sim_b)} å€‹æ•¸æ“šé€²è¡Œé›£åº¦è³¦å€¼ã€‚è¨ºæ–·å¯èƒ½ä¸å®Œæ•´ã€‚")
                     # Truncate to the minimum length
                     min_len = min(num_user_questions, num_sim_b)
                     user_df_subj_sorted = user_df_subj_sorted.iloc[:min_len]
                     sim_b_values = sim_b_values[:min_len]
                
                if not sim_b_values: # Check if list became empty after truncation
                     st.error(f"{subject}: ç„¡å¯ç”¨çš„æ¨¡æ“¬é›£åº¦å€¼ï¼Œç„¡æ³•ç¹¼çºŒè™•ç†ã€‚")
                     processing_error = True; status_prep.update(state="error"); break

                # Assign simulated b-values as 'question_difficulty'
                user_df_subj_sorted['question_difficulty'] = sim_b_values
                st.write(f"  {subject}: å·²å°‡æ¨¡æ“¬é›£åº¦è³¦å€¼çµ¦ {len(user_df_subj_sorted)} é“é¡Œç›®ã€‚")

                # Add final theta as context
                if final_theta is not None:
                     user_df_subj_sorted['estimated_ability'] = final_theta

                df_final_for_diagnosis_list.append(user_df_subj_sorted)
            
            if not processing_error and df_final_for_diagnosis_list:
                df_final_for_diagnosis = pd.concat(df_final_for_diagnosis_list, ignore_index=True)
                
                # Add required columns if they were missing before diagnosis, using defaults
                if 'question_fundamental_skill' not in df_final_for_diagnosis.columns:
                    df_final_for_diagnosis['question_fundamental_skill'] = 'Unknown Skill'
                if 'content_domain' not in df_final_for_diagnosis.columns:
                    df_final_for_diagnosis['content_domain'] = 'Unknown Domain'

                # st.subheader("è¨ºæ–·ç”¨æ•¸æ“šé è¦½ (å«æ¨¡æ“¬é›£åº¦)") # Optional: Move preview here?
                # st.dataframe(df_final_for_diagnosis.head())
                st.write("æ‰€æœ‰ç§‘ç›®æ•¸æ“šæº–å‚™å®Œæˆã€‚")
                status_prep.update(label="è¨ºæ–·æ•¸æ“šæº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            elif not processing_error: # List is empty but no specific error flagged?
                 st.warning("æœªèƒ½æº–å‚™ä»»ä½•è¨ºæ–·æ•¸æ“šã€‚")
                 status_prep.update(label="æœªèƒ½æº–å‚™è¨ºæ–·æ•¸æ“š", state="warning", expanded=True)
            # Else: error occurred, status already set
    
    # --- Diagnosis Section --- (Now uses df_final_for_diagnosis)
    diagnosis_expander = st.expander('è¨ºæ–·éç¨‹', expanded=False)
    
    if simulation_success and df_final_for_diagnosis is not None:
        # Define required columns immediately after confirming df_final_for_diagnosis exists
        # Ensure 'overtime' and 'is_invalid' might be needed by diagnosis functions
        required_cols = ['question_position', 'is_correct', 'question_difficulty', 
                         'question_time', 'question_type', 'question_fundamental_skill', 
                         'Subject', 'is_invalid', 'overtime'] # Add overtime, is_invalid
 
        with diagnosis_expander:
            st.write("é–‹å§‹é€²è¡Œè¨ºæ–·åˆ†æ...")
            progress_bar = st.progress(0, text="åˆå§‹åŒ–è¨ºæ–·éç¨‹...")
            
            # --- Display Subject Distribution and Difficulty Column Type Check --- 
            subject_counts = df_final_for_diagnosis['Subject'].value_counts()
            
            if 'question_fundamental_skill' not in df_final_for_diagnosis.columns:
                st.warning(f"è­¦å‘Š: è³‡æ–™ä¸­ç¼ºå°‘ 'question_fundamental_skill' æ¬„ä½ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ã€‚")
                df_final_for_diagnosis['question_fundamental_skill'] = 'Unknown Skill' # Add placeholder
            if 'content_domain' not in df_final_for_diagnosis.columns:
                st.warning(f"è­¦å‘Š: è³‡æ–™ä¸­ç¼ºå°‘ 'content_domain' æ¬„ä½ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ã€‚")
                df_final_for_diagnosis['content_domain'] = 'Unknown Domain' # Add placeholder

            # Define required columns inside the 'with' block, right before use
            required_cols = ['question_position', 'is_correct', 'question_difficulty', 'question_time', 'question_type', 'question_fundamental_skill', 'Subject']

            missing_cols = [col for col in required_cols if col not in df_final_for_diagnosis.columns]
            if missing_cols:
                st.error(f"ç¼ºå°‘å¿…é ˆçš„æ¬„ä½: {', '.join(missing_cols)}")
                st.info("è«‹ç¢ºä¿æ‚¨çš„è³‡æ–™åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½ã€‚")
                st.session_state.diagnosis_complete = False
            else:
                # Run diagnosis if all required columns are present
                try:
                    # --- Run Diagnosis per Subject ---
                    st.session_state.report_dict = {} # Reset results
                    all_diagnosed_dfs = []
                    diagnosis_success = True
                    progress_bar.progress(25, text="é–‹å§‹åˆ†ç§‘è¨ºæ–·...") # Update progress

                    # Calculate avg time per type needed for V diagnosis
                    v_avg_time_per_type = {}
                    if 'V' in loaded_subjects:
                        df_v_temp = df_final_for_diagnosis[df_final_for_diagnosis['Subject'] == 'V']
                        if not df_v_temp.empty and 'question_time' in df_v_temp.columns:
                            v_avg_time_per_type = df_v_temp.groupby('question_type')['question_time'].mean().to_dict()

                    # Calculate progress increment
                    progress_increment = 70 / len(loaded_subjects) if loaded_subjects else 0
                    current_progress = 25

                    for subject in loaded_subjects:
                        st.write(f"  åŸ·è¡Œ {subject} ç§‘è¨ºæ–·...")
                        progress_bar.progress(int(current_progress), text=f"æ­£åœ¨è¨ºæ–· {subject} ç§‘...")
                        df_subj = df_final_for_diagnosis[df_final_for_diagnosis['Subject'] == subject].copy()
                        time_pressure = time_pressure_map.get(subject, False)

                        subj_results, subj_report, df_subj_diagnosed = None, None, None

                        if subject == 'Q':
                            subj_results, subj_report, df_subj_diagnosed = run_q_diagnosis_processed(df_subj, time_pressure)
                        elif subject == 'DI':
                            subj_results, subj_report, df_subj_diagnosed = run_di_diagnosis_processed(df_subj, time_pressure)
                        elif subject == 'V':
                            subj_results, subj_report, df_subj_diagnosed = run_v_diagnosis_processed(df_subj, time_pressure, v_avg_time_per_type)

                        if subj_results is not None and subj_report is not None and df_subj_diagnosed is not None:
                            st.session_state.report_dict[subject] = subj_report
                            all_diagnosed_dfs.append(df_subj_diagnosed)
                            st.write(f"    {subject} ç§‘è¨ºæ–·å®Œæˆã€‚")
                        else:
                            st.error(f"    {subject} ç§‘è¨ºæ–·å¤±æ•—æˆ–æœªè¿”å›é æœŸçµæœã€‚")
                            diagnosis_success = False
                            break # Stop if one subject fails
                        current_progress += progress_increment


                    if diagnosis_success and all_diagnosed_dfs:
                        # Combine all diagnosed dataframes
                        df_processed = pd.concat(all_diagnosed_dfs, ignore_index=True)
                        st.session_state.processed_df = df_processed # Store combined processed df
                    else:
                        st.error("éƒ¨åˆ†æˆ–å…¨éƒ¨ç§‘ç›®è¨ºæ–·å¤±æ•—ã€‚")
                        diagnosis_success = False

                    # --- End Run Diagnosis per Subject ---

                    if diagnosis_success:
                        progress_bar.progress(100, text="è¨ºæ–·åˆ†æå®Œæˆï¼")
                        st.session_state.diagnosis_complete = True
                        st.success("è¨ºæ–·åˆ†ææˆåŠŸå®Œæˆã€‚è«‹é¸æ“‡ä¸‹æ–¹é¸é …å¡æŸ¥çœ‹çµæœæˆ–åŸå§‹è³‡æ–™ã€‚")
                    else:
                         progress_bar.progress(100, text="è¨ºæ–·åˆ†æåŒ…å«éŒ¯èª¤ã€‚")
                         st.session_state.diagnosis_complete = False # Mark as incomplete due to errors

                except Exception as e:
                    st.error(f"è¨ºæ–·éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
                    st.session_state.diagnosis_complete = False

# --- Display Results Section (Uses Session State) ---
st.divider()
if st.session_state.analysis_run: # Only show results area if analysis was attempted
    st.header("è¨ºæ–·çµæœ")

    # --- Constants for Styling ---
    # Define colors matching the apply_styles function (or close approximations)
    ERROR_FONT_COLOR = '#D32F2F' # Red for errors
    OVERTIME_FILL_COLOR = '#FFCDD2' # Light red fill for overtime rows/cells

    # --- Helper function to convert DataFrame to styled Excel bytes ---
    # Modified to accept df and a column name mapping dictionary
    def to_excel(df, column_map):
        output = io.BytesIO()
        df_copy = df.copy()

        # Select only columns present in the map keys and rename them
        columns_to_keep = [col for col in column_map.keys() if col in df_copy.columns]
        df_renamed = df_copy[columns_to_keep].rename(columns=column_map)

        # Use pandas ExcelWriter with XlsxWriter engine
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_renamed.to_excel(writer, index=False, sheet_name='Sheet1')

            # Get XlsxWriter objects
            workbook = writer.book
            worksheet = writer.sheets['Sheet1']

            # --- Define formats ---
            error_format = workbook.add_format({'font_color': ERROR_FONT_COLOR})
            overtime_format = workbook.add_format({'bg_color': OVERTIME_FILL_COLOR})

            # --- Get column indices (0-based) AFTER renaming ---
            # Find the indices based on the DISPLAYED (renamed) headers
            header_list = list(df_renamed.columns)
            try:
                # Use display names from the map's values
                correct_col_name = column_map.get('is_correct', 'is_correct') # Get the display name
                time_col_name = column_map.get('question_time', 'question_time')
                overtime_col_name = column_map.get('overtime', 'overtime') # Need this for the condition

                correct_col_idx = header_list.index(correct_col_name)
                time_col_idx = header_list.index(time_col_name)
                overtime_col_idx = header_list.index(overtime_col_name)

                # Convert 0-based index to Excel column letters for formulas
                correct_col_letter = chr(ord('A') + correct_col_idx)
                time_col_letter = chr(ord('A') + time_col_idx)
                overtime_col_letter = chr(ord('A') + overtime_col_idx)

                # Define data range (assuming header is row 1, data starts row 2)
                # Adjust max_row if necessary, or use a large number
                max_row = len(df_renamed) + 1
                data_range = f'A2:{chr(ord("A") + len(header_list)-1)}{max_row}' # Apply error format to all columns
                time_col_range = f'{time_col_letter}2:{time_col_letter}{max_row}'

                # --- Apply conditional formatting ---
                # 1. Red text for incorrect answers (apply to whole row)
                # Formula checks the 'is_correct' column (using its letter) for FALSE
                worksheet.conditional_format(data_range, {'type': 'formula',
                                                           'criteria': f'=${correct_col_letter}2=FALSE', # Check the correct column's value
                                                           'format': error_format})

                # 2. Red background for overtime time cells
                # Formula checks the (hidden) 'overtime' column for TRUE
                worksheet.conditional_format(time_col_range, {'type': 'formula',
                                                              'criteria': f'=${overtime_col_letter}2=TRUE', # Check the overtime column's value
                                                              'format': overtime_format})

                # 3. Hide the 'overtime' column
                worksheet.set_column(overtime_col_idx, overtime_col_idx, None, None, {'hidden': True})

            except (ValueError, IndexError) as e:
                print(f"Warning: Could not find columns for styling in Excel export: {e}")
                # Handle cases where expected columns are missing gracefully
                pass # Continue without styling if columns aren't found

        processed_data = output.getvalue()
        return processed_data
    # --- End Helper ---

    # --- Styling Helper Function (for st.dataframe) ---
    def apply_styles(row):
        # Default styles (no special styling)
        styles = [''] * len(row)
        # Apply red text to entire row if incorrect
        if 'is_correct' in row.index and not row['is_correct']:
            styles = [f'color: {ERROR_FONT_COLOR}'] * len(row)
        
        # Apply red background to time cell if overtime
        if 'overtime' in row.index and 'question_time' in row.index:
            if row['overtime']:
                try:
                    # Attempt to get the index of the 'question_time' column
                    time_col_idx = row.index.get_loc('question_time')
                    # Apply light red background
                    styles[time_col_idx] += f'; background-color: {OVERTIME_FILL_COLOR}' # Use += to combine with potential font color
                    styles[time_col_idx] = styles[time_col_idx].lstrip('; ') # Clean up if it starts with ;
                except (KeyError, IndexError):
                    pass # Column not found or index issue, do nothing
        
        return styles
    # --- End Styling Helper ---

    # --- Centralized Column Configuration and Mapping ---
    # Define the desired display names and configurations once
    # This should match the column_config used in st.dataframe
    COLUMN_DISPLAY_CONFIG = {
        "question_position": st.column_config.NumberColumn("é¡Œè™Ÿ", help="åŸå§‹å ±å‘Šä¸­çš„é¡Œç›®é †åº"),
        "is_manually_invalid": st.column_config.CheckboxColumn("æ‰‹å‹•æ¨™è¨˜ç„¡æ•ˆ?", help="å‹¾é¸æ­¤æ¡†å°‡æ­¤é¡Œæ¨™è¨˜ç‚ºæ‰‹å‹•ç„¡æ•ˆ"),
        "Subject": st.column_config.TextColumn("ç§‘ç›®"),
        "question_type": st.column_config.TextColumn("é¡Œå‹"),
        "question_fundamental_skill": st.column_config.TextColumn("è€ƒå¯Ÿèƒ½åŠ›(å°åˆ†é¡)"),
        "is_correct": st.column_config.CheckboxColumn("ç­”å°?", help="é¡Œç›®æ˜¯å¦å›ç­”æ­£ç¢º"),
        "question_difficulty": st.column_config.NumberColumn("é›£åº¦(æ¨¡æ“¬)", help="ç³»çµ±æ¨¡æ“¬çš„é¡Œç›®é›£åº¦"),
        "question_time": st.column_config.NumberColumn("ç”¨æ™‚(åˆ†)", format="%.2f"),
        "estimated_ability": st.column_config.NumberColumn("èƒ½åŠ›ä¼°è¨ˆ(Theta)", format="%.2f"),
        "time_performance_category": st.column_config.TextColumn("æ™‚é–“è¡¨ç¾åˆ†é¡"),
        "is_sfe": st.column_config.CheckboxColumn("SFE?", help="æ˜¯å¦ç‚ºSpecial Focus Error (åœ¨å·²æŒæ¡æŠ€èƒ½ç¯„åœå…§åšéŒ¯)"),
        "diagnostic_params_list": st.column_config.ListColumn("è¨ºæ–·æ¨™ç±¤", help="åˆæ­¥è¨ºæ–·æ¨™ç±¤åˆ—è¡¨"),
        # Include columns needed for styling/logic even if not always displayed prominently
        "overtime": None, # Keep overtime for styling logic, hide later
        "is_invalid": None # Keep is_invalid for context? Maybe hide too.
        # "suspiciously_fast": None # Add if needed
    }

    # Create a simple mapping from internal name to display name for Excel export
    EXCEL_COLUMN_MAP = {
        "question_position": "é¡Œè™Ÿ",
        # "is_manually_invalid": "æ‰‹å‹•æ¨™è¨˜ç„¡æ•ˆ?", # Maybe don't include this in final download? Or keep it? Let's keep it for now.
        "Subject": "ç§‘ç›®",
        "question_type": "é¡Œå‹",
        "question_fundamental_skill": "è€ƒå¯Ÿèƒ½åŠ›(å°åˆ†é¡)",
        "is_correct": "ç­”å°?", # Keep the boolean representation for formula
        "question_difficulty": "é›£åº¦(æ¨¡æ“¬)",
        "question_time": "ç”¨æ™‚(åˆ†)",
        "estimated_ability": "èƒ½åŠ›ä¼°è¨ˆ(Theta)",
        "time_performance_category": "æ™‚é–“è¡¨ç¾åˆ†é¡",
        "is_sfe": "SFE?",
        "diagnostic_params_list": "è¨ºæ–·æ¨™ç±¤",
        "overtime": "overtime_flag", # Give it a name for Excel, will be hidden
        "is_invalid": "is_invalid_flag" # Keep for context, maybe hide
    }

    # --- Display Final Thetas if available ---
    if st.session_state.final_thetas:
        st.subheader("æœ€çµ‚èƒ½åŠ›ä¼°è¨ˆ (Final Thetas)")
        theta_data = {"ç§‘ç›®": list(st.session_state.final_thetas.keys()), "èƒ½åŠ›ä¼°è¨ˆå€¼ (Theta)": list(st.session_state.final_thetas.values())}
        st.dataframe(pd.DataFrame(theta_data), hide_index=True)

    # --- Display Subject Reports and DataFrames ---
    # Check if results exist in session state
    if st.session_state.report_dict and st.session_state.processed_df is not None:
        subject_reports = st.session_state.report_dict # Use report_dict
        df_processed_display = st.session_state.processed_df # Use the stored df

        # Determine which subjects have data
        subjects_with_data = df_processed_display['Subject'].unique()
        # --- DEBUG PRINT --- 
        print(f"DEBUG: Subjects found in processed_df: {subjects_with_data}")
        # --- END DEBUG --- 

        if not subjects_with_data.any():
             st.warning("è¨ºæ–·å·²åŸ·è¡Œï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•ç§‘ç›®çš„æœ‰æ•ˆè™•ç†æ•¸æ“šã€‚")
        else:
            # Create tabs for each subject with data
            subject_tabs = st.tabs([f"{subj} ç§‘è¨ºæ–·å ±å‘Šèˆ‡æ•¸æ“š" for subj in subjects_with_data])

            for i, subject in enumerate(subjects_with_data):
                # --- DEBUG PRINT --- 
                print(f"DEBUG: Processing tab for subject: {subject}")
                # --- END DEBUG --- 
                with subject_tabs[i]:
                    st.subheader(f"{subject} ç§‘è¨ºæ–·å ±å‘Š")
                    # Display the markdown report for the subject
                    report_md = subject_reports.get(subject, f"æœªæ‰¾åˆ° {subject} ç§‘çš„è¨ºæ–·å ±å‘Šã€‚")
                    # --- BEGIN ADDED DEBUG ---
                    if subject == 'DI':
                         print(f"DEBUG (app.py): DI report from session state BEFORE markdown (first 200 chars):\\n'''{report_md[:200]}'''")
                    # --- END ADDED DEBUG ---
                    st.markdown(report_md)

                    st.subheader(f"{subject} ç§‘è©³ç´°æ•¸æ“š (å«è¨ºæ–·æ¨™ç±¤)")
                    # Filter DataFrame for the current subject
                    df_to_display = df_processed_display[df_processed_display['Subject'] == subject].copy()
                    # --- DEBUG PRINT --- 
                    if subject == 'Q':
                        print(f"DEBUG: Q df_to_display is empty: {df_to_display.empty}")
                        if not df_to_display.empty:
                            print(f"DEBUG: Q df_to_display columns: {df_to_display.columns.tolist()}")
                    # --- END DEBUG --- 

                    # Prepare the list of columns to display based on COLUMN_DISPLAY_CONFIG keys
                    # Filter out keys with None values unless they are needed internally (like 'overtime')
                    columns_for_display = [k for k, v in COLUMN_DISPLAY_CONFIG.items() if v is not None and k in df_to_display.columns]


                    if not df_to_display.empty:
                         # Apply styling (assuming apply_styles function exists and is correct)
                         try:
                             # Ensure 'overtime' column exists for styling function
                             if 'overtime' not in df_to_display.columns:
                                 df_to_display['overtime'] = False # Add dummy if missing, though it should come from diagnosis_module
                                 print(f"Warning: 'overtime' column added artificially for {subject} display.")
                             if 'is_correct' not in df_to_display.columns:
                                 df_to_display['is_correct'] = True # Add dummy if missing
                                 print(f"Warning: 'is_correct' column added artificially for {subject} display.")


                             # Apply styling step-by-step to avoid parenthesis issues
                             styler = df_to_display.style
                             styler = styler.set_properties(**{'text-align': 'left'})
                             styler = styler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])
                             styled_df = styler.apply(apply_styles, axis=1) # Apply custom styling
                             
                             st.dataframe(
                                 styled_df,
                                 column_config=COLUMN_DISPLAY_CONFIG, # Use the central config
                                 column_order=columns_for_display, # Control display order
                                 hide_index=True,
                                 use_container_width=True
                             )
                         except Exception as e:
                              st.error(f"ç„¡æ³•æ‡‰ç”¨æ¨£å¼æˆ–é¡¯ç¤º {subject} ç§‘æ•¸æ“š: {e}")
                              st.dataframe(df_to_display, hide_index=True, use_container_width=True) # Fallback to unstyled

                         # --- Download Button (Modified) ---
                         try:
                             # Convert the *original* df_to_display to Excel bytes using the modified helper
                             excel_bytes = to_excel(df_to_display, EXCEL_COLUMN_MAP)
                             st.download_button(
                                label=f"ä¸‹è¼‰ {subject} ç§‘è©³ç´°æ•¸æ“š (Excel)",
                                data=excel_bytes,
                                file_name=f"gmat_diag_{subject}_detailed_data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key=f"download_excel_{subject}"
                             )
                         except Exception as e:
                              st.error(f"ç„¡æ³•ç”Ÿæˆ {subject} ç§‘çš„ Excel ä¸‹è¼‰æ–‡ä»¶: {e}")

                    else:
                        st.write(f"æ²’æœ‰æ‰¾åˆ° {subject} ç§‘çš„è©³ç´°æ•¸æ“šå¯ä¾›é¡¯ç¤ºã€‚")
    elif st.session_state.error_message:
         st.error(f"è¨ºæ–·éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {st.session_state.error_message}")
    else:
        st.info('è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é»æ“Š"é–‹å§‹è¨ºæ–·"æŒ‰éˆ•ã€‚')

# --- Footer or other UI elements ---
