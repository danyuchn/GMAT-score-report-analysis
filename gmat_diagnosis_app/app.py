# -*- coding: utf-8 -*- # Ensure UTF-8 encoding for comments/strings
import sys
import os
import re # Import regex module
import io # Ensure io is imported
import pandas as pd # Ensure pandas is imported
import streamlit as st
from io import StringIO # Use io.StringIO directly
import traceback # For detailed error logging
import numpy as np
import logging
import openpyxl # Required by pandas for Excel export
from openpyxl.styles import Font, Border, Side, Alignment, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter
import plotly.graph_objects as go # Add plotly import
import openai # Keep openai import

# --- Project Path Setup --- (Keep as is)
try:
    app_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(app_dir)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
except NameError:
    # Handle cases where __file__ is not defined (e.g., interactive environments)
    st.warning("Could not automatically determine project root. Assuming modules are available.", icon="âš ï¸")
    project_root = os.getcwd() # Fallback

# --- Module Imports --- (Keep as is, consider try-except for robustness)
try:
    from gmat_diagnosis_app import irt_module as irt
    from gmat_diagnosis_app.preprocess_helpers import suggest_invalid_questions, calculate_overtime, THRESHOLDS
    from gmat_diagnosis_app.diagnostics.v_diagnostic import run_v_diagnosis_processed
    from gmat_diagnosis_app.diagnostics.di_diagnostic import run_di_diagnosis_processed
    from gmat_diagnosis_app.diagnostics.q_diagnostic import run_q_diagnosis_processed
except ImportError as e:
    st.error(f"å°å…¥æ¨¡çµ„æ™‚å‡ºéŒ¯: {e}. è«‹ç¢ºä¿ç’°å¢ƒè¨­å®šæ­£ç¢ºï¼Œä¸” gmat_diagnosis_app åœ¨ Python è·¯å¾‘ä¸­ã€‚")
    st.stop()

# --- Constants ---
# Validation Rules (Case and space sensitive for allowed values)
ALLOWED_PERFORMANCE = ['Correct', 'Incorrect']
ALLOWED_CONTENT_DOMAIN = {
    'Q': ['Algebra', 'Arithmetic'],
    'V': ['N/A'],
    'DI': ['Math Related', 'Non-Math Related']
}
ALLOWED_QUESTION_TYPE = {
    'Q': ['REAL', 'PURE'],
    'V': ['Critical Reasoning', 'Reading Comprehension'],
    'DI': ['Data Sufficiency', 'Two-part analysis', 'Multi-source reasoning', 'Graph and Table', 'Graphs and Tables']
}
ALLOWED_FUNDAMENTAL_SKILLS = {
    'Q': ['Equal/Unequal/ALG', 'Rates/Ratio/Percent', 'Rates/Ratios/Percent', 'Value/Order/Factors', 'Counting/Sets/Series/Prob/Stats'],
    'V': ['Plan/Construct', 'Identify Stated Idea', 'Identify Inferred Idea', 'Analysis/Critique'],
    'DI': ['N/A']
}
ALL_CONTENT_DOMAINS = list(set(cd for subj_cds in ALLOWED_CONTENT_DOMAIN.values() for cd in subj_cds))
ALL_QUESTION_TYPES = list(set(qt for subj_qts in ALLOWED_QUESTION_TYPE.values() for qt in subj_qts))
ALL_FUNDAMENTAL_SKILLS = list(set(fs for subj_fss in ALLOWED_FUNDAMENTAL_SKILLS.values() for fs in subj_fss))

# Validation Rules Dictionary (Original Column Name : { rules })
VALIDATION_RULES = {
    'Response Time (Minutes)': {'type': 'positive_float', 'error': "å¿…é ˆæ˜¯æ­£æ•¸ (ä¾‹å¦‚ 1.5, 2)ã€‚"},
    'Performance': {'allowed': ALLOWED_PERFORMANCE, 'error': f"å¿…é ˆæ˜¯ {ALLOWED_PERFORMANCE} å…¶ä¸­ä¹‹ä¸€ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Content Domain': {'allowed': ALL_CONTENT_DOMAINS, 'subject_specific': ALLOWED_CONTENT_DOMAIN, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Question Type': {'allowed': ALL_QUESTION_TYPES, 'subject_specific': ALLOWED_QUESTION_TYPE, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Fundamental Skills': {'allowed': ALL_FUNDAMENTAL_SKILLS, 'subject_specific': ALLOWED_FUNDAMENTAL_SKILLS, 'error': "å€¼ç„¡æ•ˆæˆ–èˆ‡ç§‘ç›®ä¸ç¬¦ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ)ã€‚"},
    'Question': {'type': 'positive_integer', 'error': "å¿…é ˆæ˜¯æ­£æ•´æ•¸ (ä¾‹å¦‚ 1, 2, 3)ã€‚"},
    '\ufeffQuestion': {'type': 'positive_integer', 'error': "å¿…é ˆæ˜¯æ­£æ•´æ•¸ (ä¾‹å¦‚ 1, 2, 3)ã€‚"}, # Handle BOM
}

# Base Column Rename Map (Original CSV Header -> Internal Name)
BASE_RENAME_MAP = {
    'Performance': 'is_correct', # Rename to is_correct early
    'Response Time (Minutes)': 'question_time',
    'Question Type': 'question_type',
    'Content Domain': 'content_domain',
    'Fundamental Skills': 'question_fundamental_skill'
    # 'Question' handled dynamically based on BOM
}

# Required Original Columns per Subject (Used by validation)
REQUIRED_ORIGINAL_COLS = {
    'Q': ['Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type', 'Fundamental Skills'],
    'V': ['Question', 'Response Time (Minutes)', 'Performance', 'Question Type', 'Fundamental Skills'], # Removed Content Domain for V
    'DI': ['Question', 'Response Time (Minutes)', 'Performance', 'Content Domain', 'Question Type'] # Removed Fundamental Skills for DI
}

# Columns to keep for final diagnosis dataframe preparation
# Ensure all columns needed by diagnosis functions are listed here
FINAL_DIAGNOSIS_INPUT_COLS = [
    'Subject', 'question_position', 'is_correct', 'question_time',
    'question_type', 'content_domain', 'question_fundamental_skill',
    'is_invalid', 'overtime', 'is_manually_invalid', # Keep manual flag for reference if needed
    # Add simulation/calculated columns later:
    'question_difficulty', 'estimated_ability'
]

# IRT Simulation Constants
BANK_SIZE = 1000
RANDOM_SEED = 1000
SUBJECT_SIM_PARAMS = {
    'Q': {'initial_theta_key': 'initial_theta_q', 'total_questions': THRESHOLDS['Q']['TOTAL_QUESTIONS'], 'seed_offset': 0},
    'V': {'initial_theta_key': 'initial_theta_v', 'total_questions': THRESHOLDS['V']['TOTAL_QUESTIONS'], 'seed_offset': 1},
    'DI': {'initial_theta_key': 'initial_theta_di', 'total_questions': THRESHOLDS['DI']['TOTAL_QUESTIONS'], 'seed_offset': 2}
}

# Other Constants
MAX_FILE_SIZE_MB = 1
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
SUBJECTS = ['Q', 'V', 'DI'] # Define subjects for iteration

# --- Styling Constants & Helpers ---
ERROR_FONT_COLOR = '#D32F2F' # Red for errors
OVERTIME_FILL_COLOR = '#FFCDD2' # Light red fill for overtime

def apply_styles(row):
    """Applies styling for invalid rows, incorrect answers, and overtime."""
    styles = [''] * len(row)
    INVALID_FONT_COLOR = '#A9A9A9' # DarkGray
    ERROR_FONT_COLOR = '#D32F2F' # Red for errors
    OVERTIME_FILL_COLOR = '#FFCDD2' # Light red fill for overtime

    try:
        # Grey text for invalid rows (overrides other text styles)
        if 'is_invalid' in row.index and row['is_invalid']:
            styles = [f'color: {INVALID_FONT_COLOR}'] * len(row)
            # Apply overtime background even if invalid
            if 'overtime' in row.index and row['overtime'] and 'question_time' in row.index:
                time_col_idx = row.index.get_loc('question_time')
                styles[time_col_idx] = f'{styles[time_col_idx]}; background-color: {OVERTIME_FILL_COLOR}'.lstrip('; ')
            return styles # Return early if invalid

        # Red text for incorrect (only if not invalid)
        if 'is_correct' in row.index and not row['is_correct']:
            styles = [f'color: {ERROR_FONT_COLOR}'] * len(row)

        # Red background for overtime time cell (applies to correct or incorrect, but not invalid text color)
        if 'overtime' in row.index and row['overtime'] and 'question_time' in row.index:
            time_col_idx = row.index.get_loc('question_time')
            current_style = styles[time_col_idx]
            # Add background without overriding potential error text color
            styles[time_col_idx] = f'{current_style}; background-color: {OVERTIME_FILL_COLOR}'.lstrip('; ')

    except (KeyError, IndexError):
        pass # Ignore styling errors if columns are missing
    return styles

def to_excel(df, column_map):
    """Converts DataFrame to styled Excel bytes, hiding overtime flag."""
    output = io.BytesIO()
    df_copy = df.copy()

    # Select only columns present in the map keys and rename them
    columns_to_keep = [col for col in column_map.keys() if col in df_copy.columns]
    df_renamed = df_copy[columns_to_keep].rename(columns=column_map)

    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_renamed.to_excel(writer, index=False, sheet_name='Sheet1')
        workbook = writer.book
        worksheet = writer.sheets['Sheet1']

        # Define formats
        error_format = workbook.add_format({'font_color': ERROR_FONT_COLOR})
        overtime_format = workbook.add_format({'bg_color': OVERTIME_FILL_COLOR})
        invalid_format = workbook.add_format({'font_color': '#A9A9A9'})

        # --- Apply Conditional Formatting ---
        header_list = list(df_renamed.columns)
        max_row = len(df_renamed) + 1
        try:
            # Find columns by *display* names (values in column_map)
            correct_col_disp = next(v for k, v in column_map.items() if k == 'is_correct')
            time_col_disp = next(v for k, v in column_map.items() if k == 'question_time')
            overtime_col_disp = next(v for k, v in column_map.items() if k == 'overtime') # Name of the overtime flag column
            invalid_col_disp = next(v for k, v in column_map.items() if k == 'is_invalid') # Name of the invalid flag column

            correct_col_idx = header_list.index(correct_col_disp)
            time_col_idx = header_list.index(time_col_disp)
            overtime_col_idx = header_list.index(overtime_col_disp)
            invalid_col_idx = header_list.index(invalid_col_disp)

            correct_col_letter = chr(ord('A') + correct_col_idx)
            time_col_letter = chr(ord('A') + time_col_idx)
            overtime_col_letter = chr(ord('A') + overtime_col_idx)
            invalid_col_letter = chr(ord('A') + invalid_col_idx)

            data_range = f'A2:{chr(ord("A") + len(header_list)-1)}{max_row}'
            time_col_range = f'{time_col_letter}2:{time_col_letter}{max_row}'

            # Grey text for invalid rows (apply first or ensure it overrides)
            worksheet.conditional_format(data_range, {'type': 'formula',
                                                     'criteria': f'=${invalid_col_letter}2="True"', # Check text value
                                                     'format': invalid_format})

            # Red text for incorrect answers (whole row) - might be overridden by grey if invalid
            worksheet.conditional_format(data_range, {'type': 'formula',
                                                     'criteria': f'=${correct_col_letter}2="False"', # Check text value
                                                     'format': error_format})

            # Red background for overtime time cells
            worksheet.conditional_format(time_col_range, {'type': 'formula',
                                                          'criteria': f'=${overtime_col_letter}2=TRUE', # Check the flag column (still bool maybe?)
                                                          'format': overtime_format})

            # Hide the overtime flag column ONLY
            worksheet.set_column(overtime_col_idx, overtime_col_idx, None, None, {'hidden': True})
            # Do NOT hide the invalid column

        except (StopIteration, ValueError, IndexError) as e:
            st.warning(f"ç„¡æ³•æ‡‰ç”¨ Excel æ¨£å¼æˆ–éš±è—æ¬„ä½: {e}", icon="âš ï¸") # Use warning

    processed_data = output.getvalue()
    return processed_data

# --- Validation Helper Function ---
def preprocess_skill(skill):
    """Lowercase, strip, collapse spaces for skill matching."""
    return re.sub(r'\s+', ' ', str(skill).strip()).lower()

def validate_dataframe(df, subject):
    """Validates the DataFrame rows based on predefined rules for the subject."""
    errors = []
    warnings = []
    required_original = REQUIRED_ORIGINAL_COLS.get(subject, [])

    # Determine actual required columns considering potential BOM in 'Question'
    question_col_in_df = '\ufeffQuestion' if '\ufeffQuestion' in df.columns and 'Question' not in df.columns else 'Question'
    actual_required = [col if col != 'Question' else question_col_in_df for col in required_original]

    # 1. Check for missing required columns
    missing_cols = [col for col in actual_required if col not in df.columns]
    if 'Question' in missing_cols and question_col_in_df == '\ufeffQuestion':
        missing_cols.remove('Question') # Don't report 'Question' if BOM version is present and required

    if missing_cols:
        errors.append(f"è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½: {', '.join(missing_cols)}ã€‚è«‹æª¢æŸ¥æ¬„ä½æ¨™é ­ã€‚")
        return errors, warnings # Stop if essential columns are missing

    # 2. Validate cell values row by row
    for index, row in df.iterrows():
        for original_col_name, rules in VALIDATION_RULES.items():
            # Determine the actual column name to check in the dataframe (handling BOM)
            current_col_name = original_col_name
            if original_col_name == 'Question' and question_col_in_df == '\ufeffQuestion':
                current_col_name = question_col_in_df

            if current_col_name not in df.columns:
                # Skip validation only if the column wasn't required for this subject
                if original_col_name not in required_original:
                     continue
                # If it was required but not found, the previous check caught it.

            # Proceed if column exists
            if current_col_name in df.columns:
                value = row[current_col_name]

                # Skip validation for empty/NaN cells
                if pd.isna(value) or str(value).strip() == '':
                    continue

                is_valid = True
                error_detail = rules['error']
                correct_value = None # To store potential auto-corrected value

                # --- Type Checks ---
                if 'type' in rules:
                    value_str = str(value).strip()
                    if rules['type'] == 'positive_float':
                        try:
                            num = float(value_str)
                            if num <= 0: is_valid = False
                        except (ValueError, TypeError): is_valid = False
                    elif rules['type'] == 'positive_integer':
                        try:
                            num_float = float(value_str)
                            if not (num_float > 0 and num_float == int(num_float)):
                                is_valid = False
                        except (ValueError, TypeError): is_valid = False
                    elif rules['type'] == 'number':
                        try: float(value_str)
                        except (ValueError, TypeError): is_valid = False

                # --- Allowed List Checks (with auto-correction) ---
                elif 'allowed' in rules:
                    allowed_values_list = rules['allowed']
                    # Use subject-specific list if available
                    if 'subject_specific' in rules and subject in rules['subject_specific']:
                        allowed_values_list = rules['subject_specific'][subject]

                    value_str_stripped = str(value).strip()
                    value_str_lower = value_str_stripped.lower()

                    # Specific Logic for Q: Question Type ('real contexts'/'pure contexts')
                    if original_col_name == 'Question Type' and subject == 'Q':
                        if value_str_lower == 'real contexts': correct_value = 'REAL'; is_valid = True
                        elif value_str_lower == 'pure contexts': correct_value = 'PURE'; is_valid = True
                        else: # Fallback to general check for 'REAL', 'PURE'
                            allowed_map = {str(v).lower(): v for v in allowed_values_list}
                            if value_str_lower in allowed_map:
                                correct_value = allowed_map[value_str_lower]
                                is_valid = True
                            else: is_valid = False

                    # Specific Logic for Fundamental Skills (Fuzzy Match + Canonical Mapping)
                    elif original_col_name == 'Fundamental Skills':
                        processed_input = preprocess_skill(value_str_stripped)
                        is_valid = False # Assume invalid initially
                        skill_map = {}
                        for allowed_val in allowed_values_list:
                            processed_allowed = preprocess_skill(allowed_val)
                            # Canonical mapping for Rates/Ratio/Percent(s)
                            canonical_value = 'Rates/Ratio/Percent' if processed_allowed == 'rates/ratios/percent' else allowed_val
                            skill_map[processed_allowed] = canonical_value
                        if processed_input in skill_map:
                            correct_value = skill_map[processed_input]
                            is_valid = True

                    # Default Case-Insensitive Check (Handles 'Graphs and Tables' -> 'Graph and Table')
                    else:
                        allowed_map = {}
                        for v in allowed_values_list:
                            key = str(v).lower()
                            canonical_value = 'Graph and Table' if key == 'graphs and tables' else v
                            allowed_map[key] = canonical_value
                        if value_str_lower in allowed_map:
                            correct_value = allowed_map[value_str_lower]
                            is_valid = True
                        else: is_valid = False

                    # --- Auto-Correction Application ---
                    if is_valid and correct_value is not None and value_str_stripped != correct_value:
                        try:
                            # Use .loc for safe assignment BACK into the original DataFrame
                            df.loc[index, current_col_name] = correct_value
                            # st.toast(f"Row {index+1}, Col '{current_col_name}': Auto-corrected '{value_str_stripped}' to '{correct_value}'") # Optional user feedback
                        except Exception as e:
                            errors.append(f"ç¬¬ {index + 1} è¡Œ, æ¬„ä½ '{current_col_name}': è‡ªå‹•ä¿®æ­£æ™‚å‡ºéŒ¯ ({e})ã€‚")
                            is_valid = False # Mark invalid if correction fails

                    elif not is_valid:
                        # Improve error message for list failures
                        allowed_str = ", ".join(f"'{v}'" for v in allowed_values_list)
                        error_detail += f" å…è¨±çš„å€¼: {allowed_str} (å¤§å°å¯«/æ ¼å¼å¯èƒ½ä¸ç¬¦)ã€‚"

                # --- Record Error ---
                if not is_valid:
                    errors.append(f"ç¬¬ {index + 1} è¡Œ, æ¬„ä½ '{current_col_name}': å€¼ '{value}' ç„¡æ•ˆã€‚{error_detail}")

    return errors, warnings
# --- End Validation Helper ---

# --- NEW Plotting Function ---
def create_theta_plot(theta_history_df, subject_name):
    """Creates a Plotly line chart of theta estimation history, starting from question 0 (initial theta)."""
    if theta_history_df is None or theta_history_df.empty:
        logging.warning(f"ç„¡æ³•ç‚º {subject_name} ç”Ÿæˆ Theta åœ–è¡¨ï¼Œå› ç‚ºæ­·å²æ•¸æ“šç‚ºç©ºã€‚")
        return None

    # Check if we can get the initial theta from the 'before' column
    if 'theta_est_before_answer' in theta_history_df.columns and not theta_history_df['theta_est_before_answer'].empty:
        initial_theta = theta_history_df['theta_est_before_answer'].iloc[0]
        # Prepare data starting from question 0
        x_values = [0] + (theta_history_df.index + 1).tolist()
        y_values = [initial_theta] + theta_history_df['theta_est_after_answer'].tolist()
        hover_text = [f"åˆå§‹ Theta: {initial_theta:.3f}<extra></extra>"] + \
                     [f"é¡Œè™Ÿ {i+1}<br>Theta: {theta:.3f}<extra></extra>" for i, theta in enumerate(theta_history_df['theta_est_after_answer'])]

    elif 'theta_est_after_answer' in theta_history_df.columns and not theta_history_df['theta_est_after_answer'].empty:
        # Fallback: Start from question 1 if 'before' data is missing
        logging.warning(f"åœ¨ {subject_name} çš„æ­·å²æ•¸æ“šä¸­æ‰¾ä¸åˆ° 'theta_est_before_answer'ï¼Œåœ–è¡¨å°‡å¾é¡Œè™Ÿ 1 é–‹å§‹ã€‚")
        x_values = (theta_history_df.index + 1).tolist()
        y_values = theta_history_df['theta_est_after_answer'].tolist()
        hover_text = [f"é¡Œè™Ÿ {i+1}<br>Theta: {theta:.3f}<extra></extra>" for i, theta in enumerate(y_values)]
    else:
        logging.warning(f"ç„¡æ³•ç‚º {subject_name} ç”Ÿæˆ Theta åœ–è¡¨ï¼Œå› ç‚ºæ­·å²æ•¸æ“šç¼ºå°‘å¿…è¦çš„ Theta åˆ—ã€‚")
        return None

    fig = go.Figure()
    # Add trace using the prepared x and y values
    fig.add_trace(go.Scatter(
        x=x_values,
        y=y_values,
        mode='lines+markers',
        name='Theta ä¼°è¨ˆå€¼',
        line=dict(color='royalblue', width=2),
        marker=dict(color='royalblue', size=5),
        hovertemplate=hover_text # Use the combined hover text
    ))

    fig.update_layout(
        title=f"{subject_name} ç§‘ç›®èƒ½åŠ› (Theta) ä¼°è¨ˆè®ŠåŒ–",
        xaxis_title="é¡Œè™Ÿ (0=åˆå§‹å€¼)", # Update axis title
        yaxis_title="Theta (èƒ½åŠ›ä¼°è¨ˆå€¼)",
        xaxis=dict(
            showgrid=True,
            zeroline=False,
            # Adjust dtick for potentially starting from 0
            dtick=max(1, (len(x_values)-1) // 10 if len(x_values) > 1 else 1),
            tick0=0 # Ensure ticks start nicely from 0
        ),
        yaxis=dict(
            showgrid=True,
            zeroline=True,
            range=[-4, 4] # FIX: Set fixed y-axis range
        ),
        hovermode="x unified",
        legend_title_text='ä¼°è¨ˆå€¼é¡å‹'
    )
    return fig
# --- End NEW Plotting Function ---

# --- NEW OpenAI Summarization Helper ---
def summarize_report_with_openai(report_markdown, api_key):
    """Attempts to summarize/reformat a report using OpenAI API."""
    if not api_key:
        logging.warning("OpenAI API key is missing. Skipping summarization.")
        return report_markdown # Return original if no key

    try:
        # Ensure openai library is accessible here
        if 'openai' not in sys.modules:
             st.warning("OpenAI library not loaded correctly.", icon="âš ï¸")
             return report_markdown

        client = openai.OpenAI(api_key=api_key) # Initialize client
        system_prompt = """You are an assistant that reformats GMAT diagnostic reports. Use uniform heading levels: Level-2 headings for all major sections (## Section Title). Level-3 headings for subsections (### Subsection Title). Reformat content into clear Markdown tables where appropriate. Fix minor grammatical errors or awkward phrasing for readability. IMPORTANT: Do NOT add or remove any substantive information or conclusions. Only reformat and polish the existing text. Output strictly in Markdown format, without code blocks.  """

        response = client.chat.completions.create(
            model="gpt-4.1-nano", # Use the specified model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": report_markdown}
            ],
            # temperature=0.2 # Removed: o4-mini only supports default (1)
        )
        summarized_report = response.choices[0].message.content
        # Basic check if response seems valid (not empty)
        if summarized_report and summarized_report.strip():
             logging.info(f"OpenAI summarization successful for report starting with: {report_markdown[:50]}...")
             return summarized_report.strip()
        else:
             logging.warning("OpenAI returned empty response. Using original report.")
             st.warning("AI æ•´ç†å ±å‘Šæ™‚è¿”å›äº†ç©ºå…§å®¹ï¼Œå°‡ä½¿ç”¨åŸå§‹å ±å‘Šã€‚", icon="âš ï¸")
             return report_markdown

    except openai.AuthenticationError:
        st.warning("OpenAI API Key ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³ï¼Œç„¡æ³•æ•´ç†å ±å‘Šæ–‡å­—ã€‚è«‹æª¢æŸ¥å´é‚Šæ¬„è¼¸å…¥æˆ–ç’°å¢ƒè®Šæ•¸ã€‚", icon="ğŸ”‘")
        logging.error("OpenAI AuthenticationError.")
        return report_markdown
    except openai.RateLimitError:
        st.warning("OpenAI API è«‹æ±‚é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â³")
        logging.error("OpenAI RateLimitError.")
        return report_markdown
    except openai.APIConnectionError as e:
        st.warning(f"ç„¡æ³•é€£æ¥è‡³ OpenAI API ({e})ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="ğŸŒ")
        logging.error(f"OpenAI APIConnectionError: {e}")
        return report_markdown
    except openai.APITimeoutError:
         st.warning("OpenAI API è«‹æ±‚è¶…æ™‚ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â±ï¸")
         logging.error("OpenAI APITimeoutError.")
         return report_markdown
    except openai.BadRequestError as e:
         # Often happens with context length issues or invalid requests
         st.warning(f"OpenAI API è«‹æ±‚ç„¡æ•ˆ ({e})ã€‚å¯èƒ½æ˜¯å ±å‘Šéé•·æˆ–æ ¼å¼å•é¡Œã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="â—") # Use valid emoji
         logging.error(f"OpenAI BadRequestError: {e}")
         return report_markdown
    except Exception as e:
        st.warning(f"èª¿ç”¨ OpenAI API æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚æš«æ™‚ä½¿ç”¨åŸå§‹å ±å‘Šæ–‡å­—ã€‚", icon="âš ï¸")
        logging.error(f"Unknown OpenAI API error: {e}", exc_info=True)
        return report_markdown
# --- End OpenAI Summarization Helper ---

# --- Helper functions for AI Chat ---
def _get_combined_report_context():
    """Combines markdown reports from all subjects."""
    full_report = ""
    if st.session_state.report_dict:
        for subject in SUBJECTS:
            report = st.session_state.report_dict.get(subject)
            if report:
                full_report += f"\n\n## {subject} ç§‘è¨ºæ–·å ±å‘Š\n\n{report}"
    return full_report.strip()

def _get_dataframe_context(max_rows=50):
    """Converts the processed dataframe to a string format (markdown) for context."""
    if st.session_state.processed_df is not None and not st.session_state.processed_df.empty:
        df_context = st.session_state.processed_df.copy()
        # Optionally select/rename columns for clarity in context
        # For now, let's convert a sample to markdown
        try:
            # Convert boolean columns to Yes/No for better readability for the LLM
            bool_cols = df_context.select_dtypes(include=bool).columns
            for col in bool_cols:
                df_context[col] = df_context[col].map({True: 'Yes', False: 'No'})
                
            # Convert list column to string
            if 'diagnostic_params_list' in df_context.columns:
                df_context['diagnostic_params_list'] = df_context['diagnostic_params_list'].apply(
                    lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x)
                )
                
            # Limit rows to avoid excessive context length
            if len(df_context) > max_rows:
                df_context_str = df_context.head(max_rows).to_markdown(index=False)
                df_context_str += f"\n... (åªé¡¯ç¤ºå‰ {max_rows} è¡Œ)"
            else:
                df_context_str = df_context.to_markdown(index=False)
            return df_context_str
        except Exception as e:
            logging.error(f"Error converting dataframe to markdown context: {e}")
            return "(ç„¡æ³•è½‰æ›è©³ç´°æ•¸æ“šè¡¨æ ¼)"
    return "(ç„¡è©³ç´°æ•¸æ“šè¡¨æ ¼)"

def _get_openai_response(current_chat_history, report_context, dataframe_context):
    """Gets response from OpenAI using the responses endpoint and handles conversation history.
    
    Args:
        current_chat_history (list): The current chat history list.
        report_context (str): The combined markdown report context.
        dataframe_context (str): The dataframe context string.

    Returns:
        tuple: (ai_response_text, response_id) or raises an exception.
    """
    if not st.session_state.openai_api_key:
        raise ValueError("OpenAI API Key is missing.")
        
    client = openai.OpenAI(api_key=st.session_state.openai_api_key)
    
    # Find the last user prompt
    last_user_prompt = ""
    if current_chat_history and current_chat_history[-1]["role"] == "user":
        last_user_prompt = current_chat_history[-1]["content"]
    else:
        raise ValueError("Could not find the last user prompt in history.")
        
    # Find the previous response ID from the last assistant message
    previous_response_id = None
    if len(current_chat_history) > 1:
        for i in range(len(current_chat_history) - 2, -1, -1):
            if current_chat_history[i]["role"] == "assistant":
                previous_response_id = current_chat_history[i].get("response_id")
                break
                
    # Construct the input for the API using standard multi-line string and .format()
    input_template = '''You are a GMAT diagnostic assistant. Analyze the provided report summary and detailed data table (excerpt) to answer the user's question accurately and concisely. If the information is not present in the provided context, say so.

REPORT SUMMARY:
{report}

DETAILED DATA EXCERPT:
{data}

USER QUESTION:
{prompt}
'''
    api_input = input_template.format(
        report=report_context,
        data=dataframe_context,
        prompt=last_user_prompt
    )
    
    try:
        logging.info(f"Calling OpenAI responses.create with model gpt-4.1-mini. Previous ID: {previous_response_id}")
        response = client.responses.create(
            model="o4-mini", # Using the model from user's example
            input=api_input,
            previous_response_id=previous_response_id,
            # Add other parameters if needed, e.g., temperature, max_tokens based on API docs
            # max_output_tokens=500, # Example: limit output tokens
        )
        logging.info(f"OpenAI response received. Status: {response.status}")

        if response.status == 'completed' and response.output:
            # Extract text from the first output message content
            response_text = ""
            # Iterate through the output list to find the message block
            message_block = None
            for item in response.output:
                if hasattr(item, 'type') and item.type == 'message':
                    message_block = item
                    break # Found the message block
            
            if message_block and hasattr(message_block, 'content') and message_block.content:
                for content_block in message_block.content:
                    # Ensure the content_block itself has a type attribute before checking
                    if hasattr(content_block, 'type') and content_block.type == 'output_text':
                        response_text += content_block.text
            
            if not response_text:
                 # --- Log the output structure before raising error ---
                 logging.error(f"OpenAI response completed but no text found after iterating. Response output structure: {response.output}")
                 # --- End Log ---
                 raise ValueError("OpenAI response completed but contained no text output after iterating.")
                 
            return response_text.strip(), response.id
        elif response.status == 'error':
             error_details = response.error if response.error else "Unknown error"
             raise Exception(f"OpenAI API error: {error_details}")
        else:
             # --- Log the output structure before raising error ---
             logging.error(f"OpenAI response status not completed or output empty. Status: {response.status}. Response output structure: {response.output}")
             # --- End Log ---
             raise Exception(f"OpenAI response status was not 'completed' or output was empty. Status: {response.status}")

    except Exception as e:
        logging.error(f"Error calling OpenAI responses API: {e}", exc_info=True)
        # Re-raise the exception to be caught by the sidebar UI
        raise e
# Place this function near the other helper functions
# ... existing code ...

# --- Refactored Data Input Tab Function ---
def process_subject_tab(subject, tab_container, base_rename_map):
    """Handles data input, cleaning, validation, and standardization for a subject tab."""
    st.subheader(f"{subject} è³‡æ–™è¼¸å…¥")
    subject_key = subject.lower()

    uploaded_file = tab_container.file_uploader(
        f"ä¸Šå‚³ {subject} ç§‘ç›® CSV æª”æ¡ˆ",
        type="csv",
        key=f"{subject_key}_uploader",
        help=f"æª”æ¡ˆå¤§å°é™åˆ¶ç‚º {MAX_FILE_SIZE_MB}MBã€‚"
    )
    pasted_data = tab_container.text_area(
        f"æˆ–å°‡ {subject} ç§‘ç›® Excel è³‡æ–™è²¼åœ¨æ­¤è™•ï¼š",
        height=150,
        key=f"{subject_key}_paster"
    )

    # Display required headers
    req_cols_str = ", ".join(f"'{c}'" for c in REQUIRED_ORIGINAL_COLS.get(subject, []))
    tab_container.caption(f"è«‹ç¢ºä¿è³‡æ–™åŒ…å«ä»¥ä¸‹æ¬„ä½æ¨™é¡Œ (å¤§å°å¯«/ç©ºæ ¼æ•æ„Ÿ): {req_cols_str}")

    temp_df = None
    source = None
    data_source_type = None
    validation_errors = []

    # Determine data source
    if uploaded_file is not None:
        if uploaded_file.size > MAX_FILE_SIZE_BYTES:
            tab_container.error(f"æª”æ¡ˆå¤§å° ({uploaded_file.size / (1024*1024):.2f} MB) è¶…é {MAX_FILE_SIZE_MB}MB é™åˆ¶ã€‚")
            return None, 'File Upload', [] # Return error state
        else:
            source = uploaded_file
            data_source_type = 'File Upload'
    elif pasted_data:
        source = StringIO(pasted_data)
        data_source_type = 'Pasted Data'

    if source is not None:
        try:
            # Read data, attempt flexible separator detection
            temp_df = pd.read_csv(source, sep=None, engine='python', skip_blank_lines=True)

            # --- Initial Cleaning & Prep ---
            initial_rows, initial_cols = temp_df.shape
            # Drop *_b columns silently
            cols_to_drop = [col for col in temp_df.columns if str(col).endswith('_b')]
            if cols_to_drop:
                temp_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')

            # Drop fully empty rows/columns
            temp_df.dropna(how='all', axis=0, inplace=True)
            temp_df.dropna(how='all', axis=1, inplace=True)
            temp_df.reset_index(drop=True, inplace=True)
            cleaned_rows, cleaned_cols = temp_df.shape
            if initial_rows > cleaned_rows or initial_cols > cleaned_cols:
                 tab_container.caption(f"å·²è‡ªå‹•ç§»é™¤ {initial_rows - cleaned_rows} å€‹ç©ºè¡Œå’Œ {initial_cols - cleaned_cols} å€‹ç©ºåˆ—ã€‚")

            if temp_df.empty:
                 tab_container.warning("è®€å–çš„è³‡æ–™åœ¨æ¸…ç†ç©ºè¡Œ/ç©ºåˆ—å¾Œç‚ºç©ºã€‚")
                 return None, data_source_type, []

            # Add manual invalid column *before* editor
            temp_df['is_manually_invalid'] = False # Default to False

            # --- Preprocessing for Invalid Suggestion (for default checkbox state) ---
            try:
                # Create a temporary structure for suggestion function
                temp_suggest_df = temp_df.copy()
                temp_suggest_df['Subject'] = subject
                # Basic time pressure guess for suggestion (actual pressure calculated later)
                time_col_name = 'Response Time (Minutes)'
                pressure_guess = False
                if time_col_name in temp_suggest_df.columns:
                    times = pd.to_numeric(temp_suggest_df[time_col_name], errors='coerce').dropna()
                    if len(times) > 1:
                         # Use a simple threshold diff for the *guess*
                         diff_threshold = THRESHOLDS['Q']['TIME_DIFF_PRESSURE'] if subject == 'Q' else (THRESHOLDS['DI']['TIME_PRESSURE_DIFF_MIN'] if subject == 'DI' else 3.0) # Guess for V
                         pressure_guess = (times.max() - times.min()) > diff_threshold

                temp_pressure_map = {subject: pressure_guess}

                # Rename required columns for suggestion function
                temp_rename_map = {}
                if time_col_name in temp_suggest_df.columns: temp_rename_map[time_col_name] = 'question_time'
                question_col = '\ufeffQuestion' if '\ufeffQuestion' in temp_suggest_df.columns else 'Question'
                if question_col in temp_suggest_df.columns: temp_rename_map[question_col] = 'question_position'
                if 'Performance' in temp_suggest_df.columns: temp_rename_map['Performance'] = 'is_correct' # Need correct format for suggest

                # Apply temporary renames if needed
                if temp_rename_map:
                    temp_suggest_df.rename(columns=temp_rename_map, inplace=True)
                    # Convert 'is_correct' column format if it exists after renaming
                    if 'is_correct' in temp_suggest_df.columns:
                       temp_suggest_df['is_correct'] = temp_suggest_df['is_correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)


                # Call suggestion function IF necessary columns are present
                suggest_cols_present = all(c in temp_suggest_df.columns for c in ['question_time', 'question_position', 'is_correct'])
                if suggest_cols_present:
                    processed_suggest_df = suggest_invalid_questions(temp_suggest_df, temp_pressure_map)
                    # Update the manual invalid flag based on suggestion
                    if 'is_auto_suggested_invalid' in processed_suggest_df.columns:
                        # Align indices before assigning
                        temp_df['is_manually_invalid'] = processed_suggest_df['is_auto_suggested_invalid'].reindex(temp_df.index).fillna(False)
                else:
                     tab_container.caption("ç„¡æ³•è‡ªå‹•å»ºè­°ç„¡æ•ˆé¡Œç›®ï¼Œç¼ºå°‘å¿…è¦æ¬„ä½(æ™‚é–“, é¡Œè™Ÿ, æ­£ç¢ºæ€§)ã€‚è«‹æ‰‹å‹•å‹¾é¸ã€‚")


            except Exception as suggest_err:
                tab_container.warning(f"è‡ªå‹•æª¢æ¸¬ç„¡æ•ˆé¡Œç›®æ™‚å‡ºéŒ¯ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥: {suggest_err}", icon="âš ï¸")

            # --- Editable Preview ---
            tab_container.write("é è¦½èˆ‡ç·¨è¼¯è³‡æ–™ (ä¿®æ”¹å¾Œè«‹ç¢ºä¿æ¬„ä½ç¬¦åˆè¦æ±‚)ï¼š")
            column_order = ['is_manually_invalid'] + [col for col in temp_df.columns if col != 'is_manually_invalid']

            # Ensure the checkbox column uses boolean type before editor
            temp_df['is_manually_invalid'] = temp_df['is_manually_invalid'].astype(bool)

            edited_df = tab_container.data_editor(
                temp_df,
                key=f"editor_{subject_key}",
                num_rows="dynamic",
                use_container_width=True,
                column_order=column_order,
                column_config={
                    "is_manually_invalid": st.column_config.CheckboxColumn(
                        "æ˜¯å¦è‰ç‡åšé¡Œï¼Ÿ (æ‰‹å‹•æ¨™è¨˜)",
                        help="å‹¾é¸æ­¤æ¡†è¡¨ç¤ºæ‚¨æ‰‹å‹•åˆ¤æ–·æ­¤é¡Œç‚ºç„¡æ•ˆï¼ˆä¾‹å¦‚å› å€‰ä¿ƒ/æ…Œäº‚ï¼‰ã€‚æ­¤æ¨™è¨˜å°‡å„ªå…ˆæ–¼ç³»çµ±è‡ªå‹•å»ºè­°ã€‚",
                        default=False,
                    )
                    # Add other specific configs here if needed (e.g., number formats)
                }
            )

            # --- Post-Edit Validation ---
            # Create a fresh copy for validation to avoid modifying editor's state directly if validation fails mid-way
            df_to_validate = edited_df.copy()
            validation_errors, warnings = validate_dataframe(df_to_validate, subject) # validate_dataframe now modifies df_to_validate in place for corrections

            if validation_errors:
                tab_container.error(f"{subject} ç§‘ç›®: ç™¼ç¾ä»¥ä¸‹è¼¸å…¥éŒ¯èª¤ï¼Œè«‹ä¿®æ­£ï¼š")
                for error in validation_errors:
                    tab_container.error(f"- {error}")
                return None, data_source_type, validation_errors # Indicate validation failure

            # --- Final Standardization (using the validated and potentially auto-corrected df_to_validate) ---
            final_df = df_to_validate
            # Handle 'Question' column naming for standardization
            question_col_name = '\ufeffQuestion' if '\ufeffQuestion' in final_df.columns else 'Question'
            current_rename_map = base_rename_map.copy()
            if question_col_name in final_df.columns: # Ensure the correct one is mapped
                current_rename_map[question_col_name] = 'question_position'

            # Apply renames for columns that exist
            cols_to_rename = {k: v for k, v in current_rename_map.items() if k in final_df.columns}
            if cols_to_rename:
                final_df.rename(columns=cols_to_rename, inplace=True)

            # Subject-specific standardization AFTER renaming
            if 'question_type' in final_df.columns:
                final_df['question_type'] = final_df['question_type'].astype(str).str.strip()
                if subject == 'Q':
                    final_df['question_type'] = final_df['question_type'].str.upper() # Uppercase only for Q

            # Convert performance ('is_correct') to boolean (if it exists)
            if 'is_correct' in final_df.columns:
                 # Validation already ensured 'Correct'/'Incorrect', so simple check is fine
                 final_df['is_correct'] = final_df['is_correct'].apply(lambda x: True if str(x).strip().lower() == 'correct' else False)
            else:
                 # This case should be caught by validation, but as a safeguard:
                 tab_container.error(f"{subject}: ç·¨è¼¯/é©—è­‰å¾Œä»ç¼ºå°‘ 'Performance'/'is_correct' æ¬„ä½ã€‚", icon="ğŸš¨")
                 return None, data_source_type, ["Missing 'Performance' column after edits."]

            # Ensure question_position is numeric and sequential if missing/invalid
            if 'question_position' not in final_df.columns or pd.to_numeric(final_df['question_position'], errors='coerce').isnull().any():
                tab_container.warning(f"{subject}: 'Question'/'question_position' ç¼ºå¤±æˆ–ç„¡æ•ˆï¼Œå°‡æ ¹æ“šç•¶å‰é †åºé‡æ–°ç”Ÿæˆé¡Œè™Ÿã€‚", icon="âš ï¸")
                final_df = final_df.reset_index(drop=True) # Ensure index is clean before assigning
                final_df['question_position'] = final_df.index + 1
            else:
                # Convert valid ones to integer
                final_df['question_position'] = pd.to_numeric(final_df['question_position'], errors='coerce').astype('Int64')

            # Ensure 'is_manually_invalid' is boolean and set final 'is_invalid'
            if 'is_manually_invalid' not in final_df.columns: # Should exist from editor
                final_df['is_manually_invalid'] = False
            final_df['is_manually_invalid'] = final_df['is_manually_invalid'].fillna(False).astype(bool)
            final_df['is_invalid'] = final_df['is_manually_invalid'] # Final invalid status based on user edit

            # Add Subject identifier
            final_df['Subject'] = subject

            tab_container.success(f"{subject} ç§‘ç›®è³‡æ–™è®€å–èˆ‡é©—è­‰æˆåŠŸ ({data_source_type})ï¼")
            return final_df, data_source_type, warnings

        except pd.errors.ParserError as pe:
             tab_container.error(f"ç„¡æ³•è§£æ {subject} è³‡æ–™ã€‚è«‹æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ CSV æˆ– Tab åˆ†éš”æ ¼å¼ï¼Œä¸”æ¨™é ­æ­£ç¢ºã€‚éŒ¯èª¤: {pe}")
             return None, data_source_type, [f"ParserError: {pe}"]
        except Exception as e:
            tab_container.error(f"è™•ç† {subject} ç§‘ç›®è³‡æ–™æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}")
            # tab_container.code(traceback.format_exc()) # Optional: show full traceback for debugging
            return None, data_source_type, [f"Unexpected error: {e}"]

    return None, None, [] # No data source provided
# --- End Data Input Function ---

# --- Refactored Results Display Function ---
def display_subject_results(subject, tab_container, report_md, df_subject, col_config, excel_map):
    """Displays the diagnosis report, styled DataFrame, and download button for a subject."""
    tab_container.subheader(f"{subject} ç§‘è¨ºæ–·å ±å‘Š")
    # This line renders the markdown report with wrapping
    tab_container.markdown(report_md if report_md else f"æœªæ‰¾åˆ° {subject} ç§‘çš„è¨ºæ–·å ±å‘Šã€‚", unsafe_allow_html=True) # Added unsafe_allow_html=True just in case AI uses basic HTML

    tab_container.subheader(f"{subject} ç§‘è©³ç´°æ•¸æ“š (å«è¨ºæ–·æ¨™ç±¤)")

    if df_subject is None or df_subject.empty:
        tab_container.write(f"æ²’æœ‰æ‰¾åˆ° {subject} ç§‘çš„è©³ç´°æ•¸æ“šå¯ä¾›é¡¯ç¤ºã€‚")
        return

    # Prepare DataFrame for Display
    # 1. Select columns based on keys in col_config that exist in the data
    cols_available = [k for k in col_config.keys() if k in df_subject.columns]
    df_to_display = df_subject[cols_available].copy()

    # 2. Define column order for st.dataframe (exclude those with None config value, like 'overtime')
    columns_for_st_display_order = [k for k in cols_available if col_config.get(k) is not None]

    # 3. Display styled DataFrame
    try:
        # Ensure necessary columns for styling exist with defaults
        if 'overtime' not in df_to_display.columns: df_to_display['overtime'] = False
        if 'is_correct' not in df_to_display.columns: df_to_display['is_correct'] = True # Assume correct if missing for styling

        styled_df = df_to_display.style.set_properties(**{'text-align': 'left'}) \
                                       .set_table_styles([dict(selector='th', props=[('text-align', 'left')])]) \
                                       .apply(apply_styles, axis=1)

        tab_container.dataframe(
            styled_df,
            column_config=col_config,
            column_order=columns_for_st_display_order,
            hide_index=True,
            use_container_width=True
        )
    except Exception as e:
        tab_container.error(f"ç„¡æ³•æ‡‰ç”¨æ¨£å¼æˆ–é¡¯ç¤º {subject} ç§‘æ•¸æ“š: {e}")
        # Fallback: Display without styling, only showing configured columns
        try:
             tab_container.dataframe(
                 df_to_display[columns_for_st_display_order], # Use only displayable columns
                 column_config=col_config,
                 hide_index=True,
                 use_container_width=True
             )
        except Exception as fallback_e:
             tab_container.error(f"é¡¯ç¤ºå›é€€æ•¸æ“šæ™‚ä¹Ÿç™¼ç”ŸéŒ¯èª¤: {fallback_e}")


    # 4. Download Button
    try:
        # Prepare a copy specifically for Excel export using excel_map
        df_for_excel = df_subject[[k for k in excel_map.keys() if k in df_subject.columns]].copy()

        # Apply number formatting *before* calling to_excel if needed
        if 'question_difficulty' in df_for_excel.columns:
             df_for_excel['question_difficulty'] = pd.to_numeric(df_for_excel['question_difficulty'], errors='coerce').map('{:.2f}'.format)
        if 'question_time' in df_for_excel.columns:
             df_for_excel['question_time'] = pd.to_numeric(df_for_excel['question_time'], errors='coerce').map('{:.2f}'.format)
        # Convert bools to string representation if desired for Excel output clarity
        if 'is_correct' in df_for_excel.columns:
             df_for_excel['is_correct'] = df_for_excel['is_correct'].astype(str) # Convert TRUE/FALSE to text
        if 'is_sfe' in df_for_excel.columns:
             df_for_excel['is_sfe'] = df_for_excel['is_sfe'].astype(str)


        excel_bytes = to_excel(df_for_excel, excel_map) # Pass the df subset and the map

        tab_container.download_button(
            label=f"ä¸‹è¼‰ {subject} ç§‘è©³ç´°æ•¸æ“š (Excel)",
            data=excel_bytes,
            file_name=f"gmat_diag_{subject}_detailed_data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key=f"download_excel_{subject}"
        )
    except Exception as e:
        tab_container.error(f"ç„¡æ³•ç”Ÿæˆ {subject} ç§‘çš„ Excel ä¸‹è¼‰æ–‡ä»¶: {e}")
        # tab_container.code(traceback.format_exc()) # Optional debug info
# --- End Results Display Function ---


# ==============================================================================
# --- Streamlit App Main Logic ---
# ==============================================================================

# --- Initialize Session State ---
# Use functions to avoid repeating keys
def init_session_state():
    defaults = {
        'analysis_run': False,
        'diagnosis_complete': False, # Track if diagnosis step finished
        'report_dict': {},
        # 'ai_summary': None, # Removing old AI summary state if not used elsewhere
        'final_thetas': {},
        'processed_df': None, # Store the final DataFrame after processing+diagnosis
        'error_message': None,
        'analysis_error': False, # Initialize analysis_error flag
        'input_dfs': {}, # Store loaded & validated DFs from tabs
        'validation_errors': {}, # Store validation errors per subject
        'data_source_types': {}, # Store how data was loaded ('File Upload' or 'Pasted Data')
        'theta_plots': {}, # NEW: Initialize dict for plots
        # --- AI Chat State ---
        'openai_api_key': None,
        'show_chat': False,
        'chat_history': [] # List of dicts: {"role": "user/assistant", "content": "..."}
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state() # Call it once at the start

# Function to reset state for new upload or analysis start
def reset_session_for_new_upload():
    st.session_state.analysis_run = False # Reset run flag too
    st.session_state.diagnosis_complete = False
    st.session_state.report_dict = {}
    # st.session_state.ai_summary = None # Removed
    st.session_state.final_thetas = {}
    st.session_state.processed_df = None
    st.session_state.error_message = None
    st.session_state.analysis_error = False # Reset error flag
    # Keep input_dfs, validation_errors, data_source_types as they are related to input
    st.session_state.theta_plots = {} # Reset plots
    # --- Reset AI Chat State ---
    # Keep the API key entered by user unless they clear it?
    # st.session_state.openai_api_key = None
    st.session_state.show_chat = False
    st.session_state.chat_history = []

# --- Sidebar Settings --- (Remove Chat Interface part) ---
st.sidebar.subheader("OpenAI è¨­å®š (é¸ç”¨)")
api_key_input = st.sidebar.text_input(
    "è¼¸å…¥æ‚¨çš„ OpenAI API Key å•Ÿç”¨ AI å•ç­”ï¼š",
    type="password",
    key="openai_api_key_input", # Use a dedicated key
    value=st.session_state.get('openai_api_key', ''), # Load from state if exists
    help="è¼¸å…¥æœ‰æ•ˆé‡‘é‘°ä¸¦æˆåŠŸå®Œæˆåˆ†æå¾Œï¼Œä¸‹æ–¹å°‡å‡ºç¾ AI å°è©±æ¡†ã€‚"
)

# Update session state when input changes
if api_key_input:
    st.session_state.openai_api_key = api_key_input
else:
    # Optionally clear the key if the input is cleared
    st.session_state.openai_api_key = None
    st.session_state.show_chat = False # Hide chat if key is removed
    st.session_state.chat_history = [] # Clear history if key is removed

# --- Activate Chat Logic (place this logically after analysis completion check) ---
# This check should ideally happen within the main app logic where diagnosis_complete is set
# Moved the activation logic to where diagnosis_complete is set

# --- (Removed Chat Display from Sidebar) ---

st.sidebar.divider() # Keep divider maybe?

# --- IRT Simulation Settings --- 
st.sidebar.subheader("IRT æ¨¡æ“¬è¨­å®š")
# Store initial thetas in session state to persist them
if 'initial_theta_q' not in st.session_state: st.session_state.initial_theta_q = 0.0
if 'initial_theta_v' not in st.session_state: st.session_state.initial_theta_v = 0.0
if 'initial_theta_di' not in st.session_state: st.session_state.initial_theta_di = 0.0

st.session_state.initial_theta_q = st.sidebar.number_input("Q ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=st.session_state.initial_theta_q, step=0.1, key="theta_q_input")
st.session_state.initial_theta_v = st.sidebar.number_input("V ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=st.session_state.initial_theta_v, step=0.1, key="theta_v_input")
st.session_state.initial_theta_di = st.sidebar.number_input("DI ç§‘ç›®åˆå§‹ Theta ä¼°è¨ˆ", value=st.session_state.initial_theta_di, step=0.1, key="theta_di_input")


# --- Main Page ---
st.title('GMAT æˆç¸¾è¨ºæ–·å¹³å°')

# --- Data Input Section (Using Tabs and Refactored Function) ---
st.header("1. ä¸Šå‚³æˆ–è²¼ä¸Šå„ç§‘æˆç¸¾å–®")
st.info(f"æç¤ºï¼šä¸Šå‚³çš„ CSV æª”æ¡ˆå¤§å°è«‹å‹¿è¶…é {MAX_FILE_SIZE_MB}MBã€‚è²¼ä¸Šçš„è³‡æ–™æ²’æœ‰æ­¤é™åˆ¶ã€‚")

tab_q, tab_v, tab_di = st.tabs(["Quantitative (Q)", "Verbal (V)", "Data Insights (DI)"])
tabs = {'Q': tab_q, 'V': tab_v, 'DI': tab_di}

# Process each subject tab using the refactored function
for subject in SUBJECTS:
    tab_container = tabs[subject]
    with tab_container:
        processed_df, source_type, warnings = process_subject_tab(subject, tab_container, BASE_RENAME_MAP)
        # Store results in session state immediately after processing each tab
        st.session_state.input_dfs[subject] = processed_df # Will be None if error/no data
        st.session_state.validation_errors[subject] = warnings
        if source_type:
             st.session_state.data_source_types[subject] = source_type


# --- Combine Input Data (After Tabs) ---
# Retrieve potentially updated DFs and errors from session state
input_dfs = st.session_state.input_dfs
validation_errors = st.session_state.validation_errors

# Filter out subjects where df is None (error, no data, or validation failed)
valid_input_dfs = {subj: df for subj, df in input_dfs.items() if df is not None}
loaded_subjects = list(valid_input_dfs.keys()) # Subjects with valid dataframes

df_combined_input = None
if len(valid_input_dfs) == len(SUBJECTS): # Only combine if ALL subjects are valid
    try:
        df_list = [valid_input_dfs[subj] for subj in SUBJECTS] # Ensure consistent order if needed
        df_combined_input = pd.concat(df_list, ignore_index=True)
        # Basic check after concat
        if df_combined_input.empty or 'question_position' not in df_combined_input.columns or df_combined_input['question_position'].isnull().any():
             st.error("åˆä½µå¾Œçš„è³‡æ–™ç‚ºç©ºæˆ–ç¼ºå°‘æœ‰æ•ˆçš„ 'question_position'ï¼Œç„¡æ³•ç¹¼çºŒã€‚")
             df_combined_input = None # Invalidate combination
    except Exception as e:
        st.error(f"åˆä½µå·²é©—è­‰çš„è¼¸å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        df_combined_input = None

# --- Analysis Trigger Button ---
st.divider()

# Check if *any* validation errors occurred across all tabs
any_validation_errors = any(bool(warnings) for warnings in validation_errors.values())
all_subjects_loaded_and_valid = (len(valid_input_dfs) == len(SUBJECTS)) and (df_combined_input is not None)

# Determine button state
button_disabled = True
button_message = ""

if all_subjects_loaded_and_valid:
    button_disabled = False # Enable button
elif any_validation_errors:
    button_message = "éƒ¨åˆ†ç§‘ç›®æ•¸æ“šé©—è­‰å¤±æ•—ï¼Œè«‹ä¿®æ­£ä¸Šæ–¹æ¨™ç¤ºçš„éŒ¯èª¤å¾Œå†è©¦ã€‚"
    st.error(button_message)
else: # Not all subjects loaded or combined DF failed
    subjects_actually_loaded = [subj for subj, df in input_dfs.items() if df is not None]
    missing_subjects = [subj for subj in SUBJECTS if subj not in subjects_actually_loaded]
    if missing_subjects:
         button_message = f"è«‹ç¢ºä¿å·²ç‚º {'ã€'.join(missing_subjects)} ç§‘ç›®æä¾›æœ‰æ•ˆæ•¸æ“šã€‚"
         st.warning(button_message, icon="âš ï¸")
    elif not input_dfs: # No data attempted at all
         button_message = "è«‹åœ¨ä¸Šæ–¹åˆ†é ä¸­ç‚º Q, V, DI ä¸‰å€‹ç§‘ç›®ä¸Šå‚³æˆ–è²¼ä¸Šè³‡æ–™ã€‚"
         st.info(button_message)
    else: # All attempted, but maybe combination failed or validation error cleared?
         button_message = "è«‹æª¢æŸ¥æ‰€æœ‰ç§‘ç›®çš„æ•¸æ“šæ˜¯å¦å·²æˆåŠŸåŠ è¼‰ä¸”ç„¡èª¤ã€‚"
         st.warning(button_message, icon="âš ï¸")


# Display the button
if st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary", disabled=button_disabled, key="analyze_button"):
    if not button_disabled: # Extra check
        st.session_state.analysis_run = True
        # Reset previous analysis outputs
        st.session_state.diagnosis_complete = False
        st.session_state.report_dict = {}
        st.session_state.ai_summary = None
        st.session_state.final_thetas = {}
        st.session_state.processed_df = None # Clear previous processed data
        st.session_state.error_message = None
        st.rerun() # Rerun to enter the analysis block immediately
    # No 'else' needed, button is disabled if conditions aren't met

# --- Analysis Execution Block ---
# Use df_combined_input which is only non-None if all subjects were valid and combined successfully
if st.session_state.analysis_run and df_combined_input is not None and not st.session_state.diagnosis_complete:
    st.header("2. åŸ·è¡Œ IRT æ¨¡æ“¬èˆ‡è¨ºæ–·")
    analysis_success = True # Flag to track overall success
    df_final_for_diagnosis = None # Initialize
    total_steps = 5 # Define the total number of major steps for the progress bar
    current_step = 0

    # Initialize progress bar and status text placeholder
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: æº–å‚™é–‹å§‹åˆ†æ...")

    # --- 1. Calculate Time Pressure ---
    time_pressure_map = {}
    try:
        current_step = 1
        status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: è¨ˆç®—æ™‚é–“å£“åŠ›...")
        # Use helper constants directly by accessing the THRESHOLDS dictionary
        q_total_time = pd.to_numeric(df_combined_input.loc[df_combined_input['Subject'] == 'Q', 'question_time'], errors='coerce').sum()
        v_total_time = pd.to_numeric(df_combined_input.loc[df_combined_input['Subject'] == 'V', 'question_time'], errors='coerce').sum()
        di_total_time = pd.to_numeric(df_combined_input.loc[df_combined_input['Subject'] == 'DI', 'question_time'], errors='coerce').sum()

        # --- Q Time Pressure Calculation (Modified) ---
        q_df = df_combined_input[df_combined_input['Subject'] == 'Q'].copy()
        q_df['question_time'] = pd.to_numeric(q_df['question_time'], errors='coerce')
        q_df['question_position'] = pd.to_numeric(q_df['question_position'], errors='coerce') # Ensure numeric for sorting
        q_df = q_df.sort_values('question_position').dropna(subset=['question_position']) # Sort and remove NaNs

        time_diff_q = THRESHOLDS['Q']['MAX_ALLOWED_TIME'] - q_total_time
        time_diff_check = time_diff_q <= THRESHOLDS['Q']['TIME_DIFF_PRESSURE'] # Check time diff <= 3

        fast_end_questions_exist = False
        if not q_df.empty:
            total_q_questions = len(q_df)
            last_third_start_index = int(total_q_questions * 2 / 3) # Start index for last third
            # Select last third based on sorted position index
            last_third_q_df = q_df.iloc[last_third_start_index:]
            if not last_third_q_df.empty:
                # Check if any question time in the last third is < 1.0
                fast_end_questions_exist = (last_third_q_df['question_time'] < 1.0).any()

        # Combine conditions as per markdown
        time_pressure_q = time_diff_check and fast_end_questions_exist
        # --- End Q Time Pressure Modification ---

        time_pressure_v = (THRESHOLDS['V']['MAX_ALLOWED_TIME'] - v_total_time) < 1.0 # Keep V logic as is for now, might need its own threshold in THRESHOLDS
        time_pressure_di = (THRESHOLDS['DI']['MAX_ALLOWED_TIME'] - di_total_time) <= THRESHOLDS['DI']['TIME_PRESSURE_DIFF_MIN'] # Using DI pressure diff

        time_pressure_map = {'Q': time_pressure_q, 'V': time_pressure_v, 'DI': time_pressure_di}
        # st.write(f"æ™‚é–“å£“åŠ›ç‹€æ…‹: {time_pressure_map}") # Removed detailed write
        progress_bar.progress(current_step / total_steps)
    except Exception as e:
        st.error(f"è¨ˆç®—æ™‚é–“å£“åŠ›æ™‚å‡ºéŒ¯: {e}")
        analysis_success = False
        status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: è¨ˆç®—æ™‚é–“å£“åŠ›æ™‚å‡ºéŒ¯ã€‚") # Update status on error

    # --- 2. Calculate Overtime ---
    df_with_overtime = None
    if analysis_success:
        try:
            current_step = 2
            status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: è¨ˆç®—è¶…æ™‚ç‹€æ…‹...")
            # Ensure df_combined_input has necessary columns ('question_time', 'question_type', 'Subject')
            # These should exist due to validation and standardization steps.
            # df_with_overtime = calculate_overtime(df_combined_input, time_pressure_map) # REMOVED CALL
            df_final_input_for_sim = df_combined_input # Use the combined input directly for simulation
            # st.write("è¶…æ™‚ç‹€æ…‹è¨ˆç®—å®Œæˆã€‚") # Removed detailed write
            # Use this dataframe going forward, it includes the user-confirmed 'is_invalid' flag
            # df_final_input_for_sim = df_with_overtime # This line is no longer needed
            progress_bar.progress(current_step / total_steps)

        except Exception as e:
            st.error(f"è¨ˆç®— overtime æ™‚å‡ºéŒ¯: {e}")
            analysis_success = False
            status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: è¨ˆç®—è¶…æ™‚ç‹€æ…‹æ™‚å‡ºéŒ¯ã€‚") # Update status on error

    # --- 3. IRT Simulation ---
    all_simulation_histories = {}
    final_thetas_local = {}
    all_theta_plots = {} # NEW: Store plots locally first
    question_banks = {} # Define banks here
    if analysis_success:
        # st.write("åŸ·è¡Œ IRT æ¨¡æ“¬...") # Removed detailed write
        current_step = 3
        status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: åŸ·è¡Œ IRT æ¨¡æ“¬...")
        try:
            # Initialize banks
            # st.write("  åˆå§‹åŒ–æ¨¡æ“¬é¡Œåº«...") # Removed detailed write
            for subject in SUBJECTS:
                seed = RANDOM_SEED + SUBJECT_SIM_PARAMS[subject]['seed_offset']
                question_banks[subject] = irt.initialize_question_bank(BANK_SIZE, seed=seed)
                if question_banks[subject] is None:
                     raise ValueError(f"Failed to initialize question bank for {subject}")
            # st.write("  æ¨¡æ“¬é¡Œåº«å‰µå»ºå®Œæˆã€‚") # Removed detailed write

            # Run simulation per subject
            for subject in SUBJECTS:
                # st.write(f"  åŸ·è¡Œ {subject} ç§‘ç›®æ¨¡æ“¬...") # Removed detailed write
                params = SUBJECT_SIM_PARAMS[subject]
                initial_theta = st.session_state[params['initial_theta_key']]
                bank = question_banks[subject]

                # Get user data for the subject, sorted by position
                user_df_subj = df_final_input_for_sim[df_final_input_for_sim['Subject'] == subject].sort_values(by='question_position')

                # --- Calculate total questions and wrong positions from user_df_subj --- #
                if user_df_subj.empty:
                    st.warning(f"  {subject}: æ²’æœ‰æ‰¾åˆ°è©²ç§‘ç›®çš„ä½œç­”æ•¸æ“šï¼Œç„¡æ³•åŸ·è¡Œæ¨¡æ“¬ã€‚", icon="âš ï¸")
                    final_thetas_local[subject] = initial_theta
                    all_simulation_histories[subject] = pd.DataFrame(columns=['question_number', 'question_id', 'a', 'b', 'c', 'answered_correctly', 'theta_est_before_answer', 'theta_est_after_answer'])
                    all_simulation_histories[subject].attrs['simulation_skipped'] = True
                    continue

                try:
                    total_questions_attempted = len(user_df_subj) # Correctly use the subject-specific dataframe length
                    if total_questions_attempted <= 0:
                        raise ValueError("Number of rows is not positive.")

                    # Calculate wrong positions from the subject-specific dataframe
                    if 'is_correct' not in user_df_subj.columns:
                        raise ValueError(f"{subject}: ç¼ºå°‘ 'is_correct' æ¬„ä½ã€‚")
                    user_df_subj['is_correct'] = user_df_subj['is_correct'].astype(bool)
                    user_df_subj['question_position'] = pd.to_numeric(user_df_subj['question_position'], errors='coerce')
                    wrong_positions = user_df_subj.loc[(user_df_subj['is_correct'] == False) & user_df_subj['question_position'].notna(), 'question_position'].astype(int).tolist()

                    # st.write(f"    {subject}: æ¨¡æ“¬å…¨éƒ¨ {total_questions_attempted} å€‹ä½œç­”é¡Œç›®ï¼Œå…¶ä¸­éŒ¯èª¤ä½ç½®: {wrong_positions}") # Removed detailed write

                except (KeyError, ValueError, TypeError) as e:
                     st.error(f"  {subject}: ç„¡æ³•ç¢ºå®šç¸½ä½œç­”é¡Œæ•¸æˆ–éŒ¯èª¤ä½ç½®: {e}ã€‚è·³éæ¨¡æ“¬ã€‚", icon="ğŸš¨")
                     final_thetas_local[subject] = initial_theta
                     all_simulation_histories[subject] = pd.DataFrame(columns=['question_number', 'question_id', 'a', 'b', 'c', 'answered_correctly', 'theta_est_before_answer', 'theta_est_after_answer'])
                     all_simulation_histories[subject].attrs['simulation_skipped'] = True
                     continue # Skip to next subject
                # --- End Calculation --- #

                # --- Filter valid responses for later difficulty assignment --- #
                valid_responses = user_df_subj[~user_df_subj['is_invalid']].copy()
                if valid_responses.empty:
                    st.warning(f"  {subject}: æ‰€æœ‰é¡Œç›®å‡è¢«æ¨™è¨˜ç‚ºç„¡æ•ˆï¼ŒTheta æ¨¡æ“¬ä»åŸºæ–¼å®Œæ•´åºåˆ—ã€‚", icon="âš ï¸")
                # --- End Filtering --- #

                # --- Run simulation using the calculated total and wrong list --- #
                history_df = irt.simulate_cat_exam(
                    question_bank=bank,
                    wrong_question_positions=wrong_positions,
                    initial_theta=initial_theta,
                    total_questions=total_questions_attempted # Pass the correct total number
                )
                # --- End Simulation Call --- #

                if history_df is not None and not history_df.empty:
                    # Store results only if simulation ran
                    all_simulation_histories[subject] = history_df
                    final_theta_subj = history_df['theta_est_after_answer'].iloc[-1]
                    final_thetas_local[subject] = final_theta_subj
                    # st.write(f"    {subject}: æ¨¡æ“¬å®Œæˆã€‚æœ€å¾Œ Theta ä¼°è¨ˆ: {final_theta_subj:.3f}") # Removed detailed write

                    # --- Generate and store plot ---
                    try:
                        theta_plot = create_theta_plot(history_df, subject)
                        if theta_plot:
                            all_theta_plots[subject] = theta_plot
                            # st.write(f"    {subject}: Theta æ­·å²åœ–è¡¨å·²ç”Ÿæˆã€‚") # Removed detailed write
                        else:
                            st.warning(f"    {subject}: æœªèƒ½ç”Ÿæˆ Theta åœ–è¡¨ (create_theta_plot è¿”å› None)ã€‚", icon="ğŸ“Š") # More specific warning
                    except Exception as plot_err:
                        st.warning(f"    {subject}: ç”Ÿæˆ Theta åœ–è¡¨æ™‚å‡ºéŒ¯: {plot_err}", icon="ğŸ“Š")
                    # --- End plot generation ---
                elif history_df is not None and history_df.empty: # Succeeded but empty
                    st.warning(f"  {subject}: æ¨¡æ“¬åŸ·è¡Œä½†æœªç”¢ç”Ÿæ­·å²è¨˜éŒ„ã€‚å°‡ä½¿ç”¨åˆå§‹ Thetaã€‚")
                    final_thetas_local[subject] = initial_theta
                    all_simulation_histories[subject] = pd.DataFrame(columns=['question_number', 'question_id', 'a', 'b', 'c', 'answered_correctly', 'theta_est_before_answer', 'theta_est_after_answer'])
                    all_simulation_histories[subject].attrs['simulation_skipped'] = True # Mark skipped
                else: # Failed (returned None)
                     raise ValueError(f"IRT simulation failed for subject {subject}")
            # After loop completes successfully
            progress_bar.progress(current_step / total_steps)

        except Exception as e:
            st.error(f"åŸ·è¡Œ IRT æ¨¡æ“¬æ™‚å‡ºéŒ¯: {e}")
            # st.code(traceback.format_exc()) # Optional debug
            analysis_success = False
            status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: IRT æ¨¡æ“¬æ™‚å‡ºéŒ¯ã€‚") # Update status on error

    # --- 4. Prepare Data for Diagnosis ---
    if analysis_success:
        # st.write("æº–å‚™è¨ºæ–·æ•¸æ“š...") # Removed detailed write
        current_step = 4
        status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: æº–å‚™è¨ºæ–·æ•¸æ“š...")
        df_final_for_diagnosis_list = []
        try:
            for subject in SUBJECTS:
                # st.write(f"  è™•ç† {subject} ç§‘ç›®...") # Removed detailed write
                user_df_subj = df_final_input_for_sim[df_final_input_for_sim['Subject'] == subject].copy()
                sim_history_df = all_simulation_histories.get(subject)
                final_theta = final_thetas_local.get(subject)

                if sim_history_df is None: # Should not happen if simulation ran/skipped correctly
                    st.error(f"æ‰¾ä¸åˆ° {subject} çš„æ¨¡æ“¬æ­·å²ï¼Œç„¡æ³•æº–å‚™æ•¸æ“šã€‚")
                    analysis_success = False; break

                # Add final theta estimate to all rows for context
                user_df_subj['estimated_ability'] = final_theta

                # --- Assign Difficulty ---
                # Sort user data by position *before* filtering invalid
                user_df_subj_sorted = user_df_subj.sort_values(by='question_position').reset_index(drop=True)

                # Get simulated difficulties IF simulation was not skipped
                sim_b_values = []
                if not sim_history_df.attrs.get('simulation_skipped', False):
                      sim_b_values = sim_history_df['b'].tolist()

                # Create a mapping from valid question original position to its sim difficulty
                valid_indices = user_df_subj_sorted[~user_df_subj_sorted['is_invalid']].index
                if len(valid_indices) != len(sim_b_values):
                     st.warning(f"{subject}: æœ‰æ•ˆé¡Œç›®æ•¸é‡ ({len(valid_indices)}) èˆ‡æ¨¡æ“¬é›£åº¦å€¼æ•¸é‡ ({len(sim_b_values)}) ä¸ç¬¦ã€‚é›£åº¦å¯èƒ½åˆ†é…ä¸æº–ç¢ºã€‚", icon="âš ï¸")
                     # Truncate to minimum? Or assign NaN? Assign NaN for safety.
                     min_len = min(len(valid_indices), len(sim_b_values))
                     difficulty_map = {user_df_subj_sorted.loc[idx, 'question_position']: sim_b_values[i]
                                       for i, idx in enumerate(valid_indices[:min_len])}
                else:
                     difficulty_map = {user_df_subj_sorted.loc[idx, 'question_position']: sim_b_values[i]
                                       for i, idx in enumerate(valid_indices)}


                # Assign difficulty based on the map, assign NaN to invalid or unmapped questions
                user_df_subj_sorted['question_difficulty'] = user_df_subj_sorted['question_position'].map(difficulty_map)

                # Fill missing essential columns (content_domain, fundamental_skill) if they don't exist
                if 'content_domain' not in user_df_subj_sorted.columns:
                     user_df_subj_sorted['content_domain'] = 'Unknown Domain'
                if 'question_fundamental_skill' not in user_df_subj_sorted.columns:
                     user_df_subj_sorted['question_fundamental_skill'] = 'Unknown Skill'

                # Select and reorder columns needed for diagnosis functions
                cols_to_keep = [col for col in FINAL_DIAGNOSIS_INPUT_COLS if col in user_df_subj_sorted.columns]
                df_final_for_diagnosis_list.append(user_df_subj_sorted[cols_to_keep])

            if analysis_success and df_final_for_diagnosis_list: # Check again after loop
                 df_final_for_diagnosis = pd.concat(df_final_for_diagnosis_list, ignore_index=True)
                 # st.write("è¨ºæ–·æ•¸æ“šæº–å‚™å®Œæˆã€‚") # Removed detailed write
                 progress_bar.progress(current_step / total_steps)
            elif analysis_success: # List is empty but no error flagged?
                 st.warning("æœªèƒ½æº–å‚™ä»»ä½•ç§‘ç›®çš„è¨ºæ–·æ•¸æ“šã€‚")
                 analysis_success = False # Mark as failure if no data to diagnose
                 status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: æ•¸æ“šæº–å‚™å®Œæˆï¼Œä½†ç„¡æ•¸æ“šå¯è¨ºæ–·ã€‚")


        except Exception as e:
            st.error(f"æº–å‚™è¨ºæ–·æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # st.code(traceback.format_exc()) # Optional debug
            analysis_success = False
            status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: æº–å‚™è¨ºæ–·æ•¸æ“šæ™‚å‡ºéŒ¯ã€‚") # Update status on error

    # --- 5. Run Diagnosis ---
    all_diagnosed_dfs = []
    if analysis_success and df_final_for_diagnosis is not None:
        # st.write("åŸ·è¡Œè¨ºæ–·åˆ†æ...") # Removed detailed write
        current_step = 5
        status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: åŸ·è¡Œè¨ºæ–·åˆ†æ...")
        temp_report_dict = {} # Use temporary dict during run
        try:
             # Pre-calculate V average times if V is present
             v_avg_time_per_type = {}
             if 'V' in SUBJECTS:
                 df_v_temp = df_final_for_diagnosis[df_final_for_diagnosis['Subject'] == 'V']
                 if not df_v_temp.empty and 'question_time' in df_v_temp.columns and 'question_type' in df_v_temp.columns:
                      # Ensure time is numeric for calculation
                      df_v_temp['question_time'] = pd.to_numeric(df_v_temp['question_time'], errors='coerce')
                      # Filter out rows where time is NaN after conversion
                      v_avg_time_per_type = df_v_temp.dropna(subset=['question_time']).groupby('question_type')['question_time'].mean().to_dict()


             for subject in SUBJECTS:
                 # st.write(f"  è¨ºæ–· {subject} ç§‘...") # Removed detailed write
                 df_subj = df_final_for_diagnosis[df_final_for_diagnosis['Subject'] == subject].copy()
                 # --- Calculate Overtime for the subject AFTER potential filtering (assuming filtering happens before/in diagnosis funcs) ---
                 time_pressure_subj = time_pressure_map.get(subject, False)
                 try:
                     # Pass only the subject's data and its pressure status
                     # Need to pass df_subj which might be filtered by subsequent steps
                     # Let's assume run_q_diagnosis_processed etc. handle filtering *or* operate on non-filtered data where invalid is just a flag
                     # calculate_overtime needs the DataFrame for the specific subject.
                     # Create a mini-map for the current subject
                     current_subj_pressure_map = {subject: time_pressure_subj}
                     df_subj_with_overtime = calculate_overtime(df_subj, current_subj_pressure_map)
                     df_subj = df_subj_with_overtime # Replace df_subj

                     # === START DEBUG LOGGING ===
                     logging.info(f"[{subject}] Overtime calculated. Overtime count: {df_subj['overtime'].sum()}")
                     logging.info(f"[{subject}] df_subj head with overtime before diagnosis:\n{df_subj[['Subject', 'question_position', 'question_type', 'question_time', 'overtime']].head().to_string()}")
                     # === END DEBUG LOGGING ===

                 except Exception as overtime_calc_err:
                      st.error(f"  {subject}: è¨ˆç®— Overtime æ™‚å‡ºéŒ¯: {overtime_calc_err}")
                      # Decide how to proceed: skip subject? continue without overtime?
                      # Let's add a placeholder report and continue without diagnosis for this subject
                      temp_report_dict[subject] = f"**{subject} ç§‘è¨ºæ–·å ±å‘Š**\n\n* è¨ˆç®—è¶…æ™‚ç‹€æ…‹æ™‚å‡ºéŒ¯: {overtime_calc_err}*\n"
                      all_diagnosed_dfs.append(df_subj) # Append original df without overtime/diagnosis
                      continue # Skip diagnosis for this subject
                 # --- End Overtime Calculation ---

                 # Fill NaN difficulties with a default (e.g., average ability) before diagnosis? Or handle in diagnostic funcs.
                 # Let's assume diagnostic functions can handle potential NaNs in difficulty for invalid items.
                 # If not, fill here:
                 # final_theta = final_thetas_local.get(subject, 0) # Get final theta
                 # df_subj['question_difficulty'].fillna(final_theta, inplace=True)

                 time_pressure = time_pressure_map.get(subject, False)
                 subj_results, subj_report, df_subj_diagnosed = None, None, None

                 try:
                     if subject == 'Q':
                         # Pass the df_subj which now includes the calculated 'overtime' column
                         subj_results, subj_report, df_subj_diagnosed = run_q_diagnosis_processed(df_subj, time_pressure)
                     elif subject == 'DI':
                         subj_results, subj_report, df_subj_diagnosed = run_di_diagnosis_processed(df_subj, time_pressure)
                     elif subject == 'V':
                         subj_results, subj_report, df_subj_diagnosed = run_v_diagnosis_processed(df_subj, time_pressure, v_avg_time_per_type)
                 except Exception as diag_err:
                      st.error(f"  {subject} ç§‘è¨ºæ–·å‡½æ•¸åŸ·è¡Œæ™‚å‡ºéŒ¯: {diag_err}")


                 if subj_report is not None and df_subj_diagnosed is not None:
                     # --- Attempt OpenAI Summarization --- #
                     final_report_for_subject = subj_report # Start with the original
                     # Retrieve api key (it's loaded near the top)
                     if st.session_state.openai_api_key and subj_report and "ç™¼ç”ŸéŒ¯èª¤" not in subj_report and "æœªæˆåŠŸåŸ·è¡Œ" not in subj_report:
                         # st.write(f"    å˜—è©¦ä½¿ç”¨ OpenAI æ•´ç† {subject} ç§‘å ±å‘Š...") # Removed detailed write
                         summarized_report = summarize_report_with_openai(subj_report, st.session_state.openai_api_key)
                         if summarized_report != subj_report: # Check if summarization actually changed something
                             # st.write(f"      -> {subject} ç§‘å ±å‘Šå·²ç”± AI æ•´ç†ã€‚") # Removed detailed write
                             final_report_for_subject = summarized_report
                         # else: # No need to write anything if no change
                             # st.write(f"      -> {subject} ç§‘å ±å‘Šæ•´ç†å¤±æ•—æˆ–ç„¡éœ€æ•´ç†ã€‚")
                     # elif not st.session_state.openai_api_key:
                          # st.info(f"    æœªæä¾› OpenAI API Keyï¼Œè·³é {subject} ç§‘å ±å‘Šæ•´ç†ã€‚") # Removed, less verbose

                     temp_report_dict[subject] = final_report_for_subject # Store the final version
                     all_diagnosed_dfs.append(df_subj_diagnosed) # Append the diagnosed dataframe
                     # st.write(f"    {subject} ç§‘è¨ºæ–·è™•ç†å®Œæˆã€‚") # Removed detailed write

                 else:
                     st.error(f"  {subject} ç§‘è¨ºæ–·æœªè¿”å›é æœŸçµæœã€‚")
                     temp_report_dict[subject] = f"**{subject} ç§‘è¨ºæ–·å ±å‘Š**\n\n* è¨ºæ–·æœªæˆåŠŸåŸ·è¡Œæˆ–æœªè¿”å›çµæœã€‚*\n"


             # Combine results *after* loop
             valid_diagnosed_dfs = [df for df in all_diagnosed_dfs if df is not None and not df.empty]
             if valid_diagnosed_dfs:
                  st.session_state.processed_df = pd.concat(valid_diagnosed_dfs, ignore_index=True)

                  # === START DEBUG LOGGING ===
                  logging.info("Concatenated diagnosed dfs. Final processed_df info:")
                  logging.info(f"Shape: {st.session_state.processed_df.shape}")
                  logging.info(f"Columns: {st.session_state.processed_df.columns.tolist()}")
                  if 'overtime' in st.session_state.processed_df.columns:
                      logging.info("Overtime counts per subject in final df:")
                      logging.info(st.session_state.processed_df.groupby('Subject')['overtime'].sum())
                      logging.info(f"Final df head with overtime:\n{st.session_state.processed_df[['Subject', 'question_position', 'question_type', 'question_time', 'overtime', 'time_performance_category', 'diagnostic_params_list']].head().to_string()}") # Added related columns
                  else:
                      logging.warning("Final processed_df is missing 'overtime' column after concatenation.")
                  # === END DEBUG LOGGING ===

                  st.session_state.report_dict = temp_report_dict
                  st.session_state.final_thetas = final_thetas_local # Store thetas from sim
                  st.session_state.theta_plots = all_theta_plots # NEW: Store the plots in session state
                  # st.write("æ‰€æœ‰ç§‘ç›®è¨ºæ–·å®Œæˆã€‚") # Removed detailed write
                  progress_bar.progress(current_step / total_steps) # Mark final step complete
             else:
                  st.error("æ‰€æœ‰ç§‘ç›®å‡æœªèƒ½æˆåŠŸè¨ºæ–·æˆ–ç„¡æ•¸æ“šã€‚")
                  st.session_state.processed_df = None # Ensure no stale data
                  st.session_state.report_dict = temp_report_dict # Still show error reports
                  st.session_state.theta_plots = {} # Clear plots
                  analysis_success = False
                  status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: è¨ºæ–·å®Œæˆï¼Œä½†ç„¡æœ‰æ•ˆçµæœã€‚")


        except Exception as e:
            st.error(f"è¨ºæ–·éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            # st.code(traceback.format_exc()) # Optional debug
            analysis_success = False
            status_text.text(f"æ­¥é©Ÿ {current_step}/{total_steps}: åŸ·è¡Œè¨ºæ–·æ™‚å‡ºéŒ¯ã€‚") # Update status on error

    # --- Final Status Update ---
    # Use st.session_state.processed_df existence check directly
    if analysis_success and st.session_state.processed_df is not None:
         # analysis_status.update(label="åˆ†ææˆåŠŸå®Œæˆï¼", state="complete", expanded=False) # Removed status block update
         status_text.text("åˆ†ææˆåŠŸå®Œæˆï¼") # Update final status text
         st.session_state.diagnosis_complete = True # Mark diagnosis as done
         st.session_state.error_message = None
         st.balloons() # Celebrate success
    else:
         # analysis_status.update(label="åˆ†æéç¨‹ä¸­æ–·æˆ–å¤±æ•—ã€‚", state="error", expanded=True) # Removed status block update
         # Error message already set in status_text during the step failure
         if analysis_success and st.session_state.processed_df is None:
             # Handle the case where analysis technically succeeded but produced no df
             status_text.error("åˆ†æå®Œæˆä½†æœªç”¢ç”Ÿæœ‰æ•ˆæ•¸æ“šï¼Œè«‹æª¢æŸ¥è¼¸å…¥æˆ–è¨ºæ–·é‚è¼¯ã€‚")
         elif not analysis_success:
              # If analysis failed earlier, the status_text should already reflect the error step
              # Optionally add a generic error message if it's still the default text
              if "æº–å‚™é–‹å§‹åˆ†æ" in status_text.text("") : # Check if it's still initial text
                  status_text.error("åˆ†æéç¨‹ä¸­æ–·æˆ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤è¨Šæ¯ã€‚")

         st.session_state.diagnosis_complete = False # Ensure it's False on error
         st.session_state.error_message = "åˆ†ææœªèƒ½æˆåŠŸå®Œæˆï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤è¨Šæ¯ã€‚"
         st.session_state.theta_plots = {} # Clear plots on error

    # Explicitly clear the progress bar maybe? Or leave it at 100% / error state.
    # Leaving it might be fine.


# --- Display Results Section ---
st.divider()
if st.session_state.analysis_run: # Only show results area if analysis was at least started
    st.header("ğŸ“Š è¨ºæ–·çµæœ")

    # --- Define Column Configuration and Excel Map ---
    # (Moved near the top as constants/config)
    COLUMN_DISPLAY_CONFIG = {
        "question_position": st.column_config.NumberColumn("é¡Œè™Ÿ", help="é¡Œç›®é †åº"),
        # "Subject": st.column_config.TextColumn("ç§‘ç›®"), # Usually known from tab context
        "question_type": st.column_config.TextColumn("é¡Œå‹"),
        "content_domain": st.column_config.TextColumn("å…§å®¹é ˜åŸŸ"), # Added back
        "question_fundamental_skill": st.column_config.TextColumn("è€ƒå¯Ÿèƒ½åŠ›"),
        "is_correct": st.column_config.CheckboxColumn("ç­”å°?", help="æ˜¯å¦å›ç­”æ­£ç¢º"),
        "question_difficulty": st.column_config.NumberColumn("é›£åº¦(æ¨¡æ“¬)", help="ç³»çµ±æ¨¡æ“¬çš„é¡Œç›®é›£åº¦ (æœ‰æ•ˆé¡Œç›®)", format="%.2f", width="small"),
        # "estimated_ability": st.column_config.NumberColumn("èƒ½åŠ›ä¼°è¨ˆ", help="æœ¬ç§‘ç›®æœ€çµ‚èƒ½åŠ›ä¼°è¨ˆå€¼", format="%.2f", width="small"), # Add overall ability back (Removed)
        "question_time": st.column_config.NumberColumn("ç”¨æ™‚(åˆ†)", format="%.2f", width="small"),
        "time_performance_category": st.column_config.TextColumn("æ™‚é–“è¡¨ç¾"),
        "is_sfe": st.column_config.CheckboxColumn("SFE?", help="æ˜¯å¦ç‚ºSpecial Focus Error", width="small"),
        "diagnostic_params_list": st.column_config.ListColumn("è¨ºæ–·æ¨™ç±¤", help="åˆæ­¥è¨ºæ–·æ¨™ç±¤", width="medium"),
        "is_invalid": st.column_config.CheckboxColumn("æ¨™è¨˜ç„¡æ•ˆ?", help="æ­¤é¡Œæ˜¯å¦è¢«æ¨™è¨˜ç‚ºç„¡æ•ˆ (æ‰‹å‹•å„ªå…ˆ)", width="small"), # Show the final invalid flag
        # Internal columns for styling, set config to None to hide in st.dataframe
        "overtime": None,
        "is_manually_invalid": None, # Hide the intermediate manual flag
    }

    EXCEL_COLUMN_MAP = {
        "question_position": "é¡Œè™Ÿ",
        "Subject": "ç§‘ç›®", # Keep subject in Excel export
        "question_type": "é¡Œå‹",
        "content_domain": "å…§å®¹é ˜åŸŸ",
        "question_fundamental_skill": "è€ƒå¯Ÿèƒ½åŠ›",
        "is_correct": "ç­”å°", # Use text TRUE/FALSE (converted in display_subject_results)
        "question_difficulty": "é›£åº¦(æ¨¡æ“¬)",
        # "estimated_ability": "èƒ½åŠ›ä¼°è¨ˆ", # Removed as per user request
        "question_time": "ç”¨æ™‚(åˆ†)",
        "time_performance_category": "æ™‚é–“è¡¨ç¾",
        "is_sfe": "SFE", # Use text TRUE/FALSE
        "diagnostic_params_list": "è¨ºæ–·æ¨™ç±¤",
        "is_invalid": "æ˜¯å¦ç„¡æ•ˆ", # Use text TRUE/FALSE
        "overtime": "overtime_flag", # Internal flag for Excel styling, will be hidden by to_excel
    }

    if st.session_state.analysis_error:
        st.error(st.session_state.error_message)
    elif not st.session_state.diagnosis_complete:
        st.info("åˆ†ææ­£åœ¨é€²è¡Œä¸­æˆ–å°šæœªå®Œæˆã€‚")
    elif st.session_state.processed_df is None or st.session_state.processed_df.empty:
        st.warning("è¨ºæ–·å®Œæˆï¼Œä½†æ²’æœ‰å¯é¡¯ç¤ºçš„æ•¸æ“šã€‚")
        # Display any report messages even if df is empty (e.g., all invalid)
        if st.session_state.report_dict:
            st.subheader("è¨ºæ–·æ‘˜è¦")
            for subject, report_md in st.session_state.report_dict.items():
                st.markdown(f"### {subject} ç§‘:")
                st.markdown(report_md, unsafe_allow_html=True)
    else:
        st.success("è¨ºæ–·åˆ†æå·²å®Œæˆï¼")
        # --- Results Display Tabs --- #
        subjects_with_data = [subj for subj in SUBJECTS if subj in st.session_state.processed_df['Subject'].unique()]
        if not subjects_with_data:
            st.warning("è™•ç†å¾Œçš„æ•¸æ“šä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç§‘ç›®ã€‚")
        else:
            # Create tabs for each subject with data
            result_tabs = st.tabs([f"{subj} ç§‘çµæœ" for subj in subjects_with_data])

            for i, subject in enumerate(subjects_with_data):
                subject_tab = result_tabs[i]
                with subject_tab:
                    df_subject = st.session_state.processed_df[st.session_state.processed_df['Subject'] == subject]
                    report_md = st.session_state.report_dict.get(subject, f"*æœªæ‰¾åˆ° {subject} ç§‘çš„å ±å‘Šã€‚*")
                    subject_excel_map = EXCEL_COLUMN_MAP
                    subject_col_config = COLUMN_DISPLAY_CONFIG

                    # --- æ·»åŠ æ—¥èªŒèª¿è©¦ ---
                    # Removed logging block here
                    # --- çµæŸæ—¥èªŒèª¿è©¦ ---

                    # --- NEW: Display Theta Plot --- #
                    st.subheader(f"{subject} ç§‘èƒ½åŠ›ä¼°è¨ˆ (Theta) èµ°å‹¢")
                    theta_plot = st.session_state.theta_plots.get(subject)
                    if theta_plot:
                        st.plotly_chart(theta_plot, use_container_width=True)
                    else:
                        # Access the original simulation history from run_analysis output if needed
                        # This might require passing all_simulation_histories to session state too,
                        # or recalculating the skipped status here.
                        # For simplicity, let's just check if the plot exists.
                        st.info(f"{subject} ç§‘ç›®çš„ Theta ä¼°è¨ˆåœ–è¡¨ä¸å¯ç”¨ã€‚")
                    st.divider() # Add a separator
                    # --- End NEW --- #

                    display_subject_results(subject, subject_tab, report_md, df_subject, subject_col_config, subject_excel_map)

# --- AI Chat Interface (Moved to Main Page Bottom) ---
st.divider()

# Check conditions to show chat
if st.session_state.openai_api_key and st.session_state.diagnosis_complete:
    st.session_state.show_chat = True
else:
    st.session_state.show_chat = False
    
if st.session_state.show_chat:
    st.header("ğŸ’¬ èˆ‡ AI å°è©± (åŸºæ–¼æœ¬æ¬¡å ±å‘Š)")

    # Display chat history
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input at the bottom of the main page
    if prompt := st.chat_input("é‡å°å ±å‘Šå’Œæ•¸æ“šæå•..."):
        # Add user message to history and display
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Prepare context and call OpenAI
        with st.chat_message("assistant"):
            message_placeholder = st.empty() # Placeholder for streaming or waiting message
            message_placeholder.markdown("æ€è€ƒä¸­...")
            try:
                # --- Prepare Context --- 
                report_context = _get_combined_report_context() 
                dataframe_context = _get_dataframe_context()
                # --- Call OpenAI --- 
                ai_response_text, response_id = _get_openai_response(
                    st.session_state.chat_history, 
                    report_context, 
                    dataframe_context
                )
                
                # Display AI response and add to history with response_id
                message_placeholder.markdown(ai_response_text)
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": ai_response_text,
                    "response_id": response_id # Store the ID for the next turn
                })

            except Exception as e:
                error_message = f"å‘¼å« AI æ™‚å‡ºéŒ¯: {e}"
                message_placeholder.error(error_message)
                # Add error message to history, without a response_id
                st.session_state.chat_history.append({"role": "assistant", "content": error_message})

# --- End of Main Page --- 
# (Add any final footers or elements after the chat interface if needed)
