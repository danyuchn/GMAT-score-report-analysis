"""
èŠå¤©ç•Œé¢æ¨¡çµ„
æä¾›AIèŠå¤©å°è©±åŠŸèƒ½
"""

import streamlit as st
from gmat_diagnosis_app.services.openai_service import get_chat_context, get_openai_response

def display_chat_interface(session_state):
    """é¡¯ç¤ºèŠå¤©ç•Œé¢ï¼Œè™•ç†è¨Šæ¯äº¤æ›"""
    st.divider()

    # Check conditions to show chat
    show_chat = check_chat_conditions(session_state)
    session_state.show_chat = show_chat

    if show_chat:
        st.subheader("ğŸ’¬ èˆ‡ AI å°è©± (åŸºæ–¼æœ¬æ¬¡å ±å‘Š)")
        
        # æ·»åŠ è‡ªå®šç¾©CSSï¼Œå‰µå»ºå›ºå®šé«˜åº¦çš„èŠå¤©å®¹å™¨
        st.markdown("""
        <style>
        .chat-container {
            height: 400px;
            overflow-y: auto;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 15px;
            background-color: #f8f9fa;
            margin-bottom: 15px;
        }
        </style>
        """, unsafe_allow_html=True)
        
        # å‰µå»ºå›ºå®šé«˜åº¦çš„èŠå¤©å®¹å™¨
        chat_container = st.container()
        
        # ä½¿ç”¨HTMLå¯¦ç¾å›ºå®šé«˜åº¦å’Œæ»¾å‹•
        chat_html = '<div class="chat-container">'
        
        # é¡¯ç¤ºèŠå¤©æ­·å²
        with chat_container:
            display_chat_history(session_state)
            
            # åœ¨æ­·å²é¡¯ç¤ºå¾Œè‡ªå‹•æ»¾å‹•åˆ°åº•éƒ¨
            if session_state.chat_history:
                st.markdown("""
                <script>
                    function scrollChatToBottom() {
                        const chatContainer = document.querySelector('.chat-container');
                        if (chatContainer) {
                            chatContainer.scrollTop = chatContainer.scrollHeight;
                        }
                    }
                    setTimeout(scrollChatToBottom, 100);
                </script>
                """, unsafe_allow_html=True)

        # Chat input below the fixed container
        handle_chat_input(session_state)
        
def check_chat_conditions(session_state):
    """æª¢æŸ¥æ˜¯å¦æ»¿è¶³é¡¯ç¤ºèŠå¤©çš„æ¢ä»¶"""
    if session_state.openai_api_key and session_state.diagnosis_complete:
        return True
    return False

def display_chat_history(session_state):
    """é¡¯ç¤ºèŠå¤©æ­·å²"""
    for message in session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def handle_chat_input(session_state):
    """è™•ç†ç”¨æˆ¶è¼¸å…¥å’ŒAIå›æ‡‰"""
    if prompt := st.chat_input("é‡å°å ±å‘Šå’Œæ•¸æ“šæå•..."):
        # Add user message to history and display
        session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Prepare context and call OpenAI
        with st.chat_message("assistant"):
            message_placeholder = st.empty()  # Placeholder for streaming or waiting message
            message_placeholder.markdown("æ€è€ƒä¸­...")
            try:
                # Get context
                context = get_chat_context(session_state)
                
                # Call OpenAI
                ai_response_text, response_id = get_openai_response(
                    session_state.chat_history,
                    context["report"],
                    context["dataframe"],
                    session_state.openai_api_key
                )

                # Display AI response and add to history with response_id
                message_placeholder.markdown(ai_response_text)
                session_state.chat_history.append({
                    "role": "assistant",
                    "content": ai_response_text,
                    "response_id": response_id  # Store the ID for the next turn
                })

            except Exception as e:
                error_message = f"å‘¼å« AI æ™‚å‡ºéŒ¯: {e}"
                message_placeholder.error(error_message)
                # Add error message to history, without a response_id
                session_state.chat_history.append({"role": "assistant", "content": error_message}) 