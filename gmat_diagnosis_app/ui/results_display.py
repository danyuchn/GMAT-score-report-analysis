"""
çµæœé¡¯ç¤ºæ¨¡çµ„
é¡¯ç¤ºè¨ºæ–·çµæœçš„åŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from gmat_diagnosis_app.utils.styling import apply_styles
from gmat_diagnosis_app.utils.excel_utils import to_excel
from gmat_diagnosis_app.constants.config import SUBJECTS, EXCEL_COLUMN_MAP
from gmat_diagnosis_app.ui.chat_interface import display_chat_interface
from gmat_diagnosis_app.diagnostics.q_modules.reporting import generate_q_summary_report # Placeholder for actual Q AI prompt function
from gmat_diagnosis_app.diagnostics.di_modules.report_generation import _generate_di_summary_report # Placeholder
from gmat_diagnosis_app.diagnostics.v_modules.reporting import generate_v_summary_report # Placeholder
import logging

# --- Column Display Configuration (Moved from app.py) ---
COLUMN_DISPLAY_CONFIG = {
    "question_position": st.column_config.NumberColumn("é¡Œè™Ÿ", help="é¡Œç›®é †åº"),
    "question_type": st.column_config.TextColumn("é¡Œå‹"),
    "question_fundamental_skill": st.column_config.TextColumn("è€ƒå¯Ÿèƒ½åŠ›"),
    "question_difficulty": st.column_config.NumberColumn("é›£åº¦(æ¨¡æ“¬)", help="ç³»çµ±æ¨¡æ“¬çš„é¡Œç›®é›£åº¦ (æœ‰æ•ˆé¡Œç›®)", format="%.2f", width="small"),
    "question_time": st.column_config.NumberColumn("ç”¨æ™‚(åˆ†)", format="%.2f", width="small"),
    "time_performance_category": st.column_config.TextColumn("æ™‚é–“è¡¨ç¾"),
    "content_domain": st.column_config.TextColumn("å…§å®¹é ˜åŸŸ"),
    "diagnostic_params_list": st.column_config.ListColumn("è¨ºæ–·æ¨™ç±¤", help="åˆæ­¥è¨ºæ–·æ¨™ç±¤", width="medium"),
    "is_correct": st.column_config.CheckboxColumn("ç­”å°?", help="æ˜¯å¦å›ç­”æ­£ç¢º"),
    "is_sfe": st.column_config.CheckboxColumn("SFE?", help="æ˜¯å¦ç‚ºSpecial Focus Error", width="small"),
    "is_invalid": st.column_config.CheckboxColumn("æ¨™è¨˜ç„¡æ•ˆ?", help="æ­¤é¡Œæ˜¯å¦è¢«æ¨™è¨˜ç‚ºç„¡æ•ˆ (æ‰‹å‹•å„ªå…ˆ)", width="small"),
    "overtime": None, # Internal column for styling
    "is_manually_invalid": None, # Hide the intermediate manual flag
}

def display_subject_results(subject, tab_container, report_md, df_subject, col_config, excel_map):
    """Displays the diagnosis report, styled DataFrame, and download button for a subject."""
    tab_container.subheader(f"{subject} ç§‘è¨ºæ–·å ±å‘Š")
    
    # æº–å‚™æ•¸æ“šè¡¨æ ¼ (å°æ‰€æœ‰ç§‘ç›®é€šç”¨)
    styled_df = None
    df_to_display = None
    columns_for_st_display_order = []
    
    if df_subject is not None and not df_subject.empty:
        # æº–å‚™è¡¨æ ¼é¡¯ç¤ºå‰çš„æ•¸æ“šè™•ç†
        subject_col_config = col_config.copy()
        subject_excel_map = excel_map.copy()
        
        # è¤‡è£½æ•¸æ“šæ¡†ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
        df_display = df_subject.copy()
        
        # ç¢ºä¿æŒ‰é¡Œè™Ÿæ’åº
        if 'question_position' in df_display.columns:
            df_display = df_display.sort_values(by='question_position').reset_index(drop=True)
        
        # ç§‘ç›®ç‰¹æ®Šè™•ç†
        if subject == 'DI':
            # é‡å°DIç§‘ç›®ç§»é™¤ã€Œè€ƒå¯Ÿèƒ½åŠ›ã€æ¬„ä½
            if 'question_fundamental_skill' in subject_col_config:
                del subject_col_config['question_fundamental_skill']
            if 'question_fundamental_skill' in subject_excel_map:
                del subject_excel_map['question_fundamental_skill']
        
        # æª¢æŸ¥ç„¡æ•ˆé …æ•¸æ“šçš„é¡å‹å’Œå€¼
        if 'is_invalid' in df_display.columns:
            invalid_type = df_display['is_invalid'].dtype
            logging.debug(f"{subject}ç§‘ç„¡æ•ˆé …æ•¸æ“šé¡å‹: {invalid_type}")
            
            # ç¢ºä¿ç„¡æ•ˆé …æ˜¯å¸ƒçˆ¾å€¼
            try:
                df_display['is_invalid'] = df_display['is_invalid'].fillna(False).astype(bool)
            except Exception as e:
                tab_container.error(f"è½‰æ›ç„¡æ•ˆé …æ™‚å‡ºéŒ¯: {e}")
        
        # é‡è¦ä¿®æ”¹ï¼šç¢ºä¿is_invalidå®Œå…¨ä»¥æ‰‹å‹•æ¨™è¨˜ç‚ºæº–
        if 'is_manually_invalid' in df_display.columns:
            if 'is_invalid' in df_display.columns:
                # å…ˆå…¨éƒ¨è¨­ç‚ºFalse
                df_display['is_invalid'] = False
                # åªå°‡æ‰‹å‹•æ¨™è¨˜çš„é …è¨­ç‚ºTrue
                df_display.loc[df_display['is_manually_invalid'] == True, 'is_invalid'] = True

        # æº–å‚™æ•¸æ“šæ¡†é¡¯ç¤º
        cols_available = [k for k in subject_col_config.keys() if k in df_display.columns]
        df_to_display = df_display[cols_available].copy()
        columns_for_st_display_order = [k for k in cols_available if subject_col_config.get(k) is not None]

        # ç¢ºä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'overtime' not in df_to_display.columns: df_to_display['overtime'] = False
        if 'is_correct' not in df_to_display.columns: df_to_display['is_correct'] = True
        if 'is_invalid' not in df_to_display.columns: df_to_display['is_invalid'] = False
        
        # å†æ¬¡ç¢ºä¿is_invalidä»¥æ‰‹å‹•æ¨™è¨˜ç‚ºæº–
        if 'is_manually_invalid' in df_to_display.columns:
            df_to_display['is_invalid'] = False
            df_to_display.loc[df_to_display['is_manually_invalid'] == True, 'is_invalid'] = True
            
        # ç¢ºä¿is_invalidç‚ºå¸ƒæ—å€¼
        df_to_display['is_invalid'] = df_to_display['is_invalid'].astype(bool)
        
        try:
            styled_df = df_to_display.style.set_properties(**{'text-align': 'left'}) \
                                   .set_table_styles([dict(selector='th', props=[('text-align', 'left')])]) \
                                   .apply(apply_styles, axis=1)
        except Exception as e:
            logging.error(f"æ‡‰ç”¨æ¨£å¼æ™‚å‡ºéŒ¯: {e}")
            styled_df = None
    
    # 1. é¦–å…ˆé¡¯ç¤ºæ•¸æ“šè¡¨æ ¼ (æ‰€æœ‰ç§‘ç›®)
    tab_container.subheader(f"{subject} ç§‘è©³ç´°æ•¸æ“š (å«è¨ºæ–·æ¨™ç±¤)")
    if styled_df is not None:
        try:
            tab_container.dataframe(
                styled_df,
                column_config=subject_col_config,
                column_order=columns_for_st_display_order,
                hide_index=True,
                use_container_width=True
            )
        except Exception as e:
            tab_container.error(f"é¡¯ç¤ºè¡¨æ ¼æ™‚å‡ºéŒ¯: {e}")
            tab_container.info("ç„¡æ³•é¡¯ç¤ºæ•¸æ“šè¡¨æ ¼ï¼šæ•¸æ“šè™•ç†éç¨‹ä¸­å‡ºéŒ¯ã€‚")
    else:
        tab_container.info("ç„¡æ³•é¡¯ç¤ºæ•¸æ“šè¡¨æ ¼ï¼šæ•¸æ“šç‚ºç©ºæˆ–è™•ç†éç¨‹ä¸­å‡ºéŒ¯ã€‚")
    
    # 2. é¡¯ç¤ºthetaæŠ˜ç·šåœ–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'theta_plots' in st.session_state and subject in st.session_state.theta_plots:
        tab_container.subheader(f"{subject} ç§‘èƒ½åŠ›å€¼ (Theta) è®ŠåŒ–åœ–")
        tab_container.plotly_chart(st.session_state.theta_plots[subject], use_container_width=True)
    
    # 3. æœ€å¾Œé¡¯ç¤ºè¨ºæ–·å ±å‘Š
    if report_md:
        tab_container.subheader(f"{subject} ç§‘è¨ºæ–·å ±å‘Šè©³æƒ…")
        tab_container.markdown(report_md, unsafe_allow_html=True)
    else:
        tab_container.info(f"æœªæ‰¾åˆ° {subject} ç§‘çš„è¨ºæ–·å ±å‘Šã€‚")

    # 4. Download Button (ä¸€æ¨£ç‚ºæ‰€æœ‰ç§‘ç›®é¡¯ç¤ºä¸‹è¼‰æŒ‰éˆ•)
    try:
        # Prepare a copy specifically for Excel export using excel_map
        df_for_excel = df_subject[[k for k in subject_excel_map.keys() if k in df_subject.columns]].copy()
        
        # ç¢ºä¿æŒ‰é¡Œè™Ÿæ’åº
        if 'question_position' in df_for_excel.columns:
            df_for_excel = df_for_excel.sort_values(by='question_position').reset_index(drop=True)

        # æ‰€æœ‰ç§‘ç›®çš„Excelè™•ç†é‚è¼¯çµ±ä¸€ï¼ˆä¸å†å€åˆ†V/DI/Qï¼‰
        logging.debug(f"{subject}ç§‘Excelå°å‡ºæ•¸æ“šåˆ—: {list(df_for_excel.columns)}")
        
        # ç¢ºä¿is_invalidåˆ—ä¸€å®šå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
        if 'is_invalid' not in df_for_excel.columns:
            df_for_excel['is_invalid'] = False
            logging.info(f"{subject}ç§‘åŸæœ¬æ²’æœ‰is_invalidåˆ—ï¼Œå·²å‰µå»º")
            
        # è¨˜éŒ„åŸå§‹ç„¡æ•ˆé …æ•¸é‡
        orig_invalid_sum = df_for_excel['is_invalid'].sum()
        logging.debug(f"{subject}ç§‘Excelå°å‡ºå‰ç„¡æ•ˆé …æ•¸é‡: {orig_invalid_sum}")
        
        # é‡è¦ï¼šç¢ºä¿is_invalidå®Œå…¨ä»¥æ‰‹å‹•æ¨™è¨˜ç‚ºæº–ï¼ˆExcelå°å‡ºå‰ï¼‰
        if 'is_manually_invalid' in df_for_excel.columns:
            # é‡ç½®is_invalidåˆ—
            df_for_excel['is_invalid'] = False
            # åƒ…å°‡æ‰‹å‹•æ¨™è¨˜çš„é …è¨­ç‚ºç„¡æ•ˆ
            df_for_excel.loc[df_for_excel['is_manually_invalid'] == True, 'is_invalid'] = True
            
            logging.debug(f"{subject}ç§‘åƒ…ä½¿ç”¨æ‰‹å‹•æ¨™è¨˜å¾Œï¼ŒExcelå°å‡ºç„¡æ•ˆé …æ•¸é‡: {df_for_excel['is_invalid'].sum()}")
            
            # é©—è­‰æ‰‹å‹•æ¨™è¨˜é …è¢«æ­£ç¢ºè¨­ç½® (åƒ…è¨˜éŒ„åˆ°æ—¥èªŒ)
            manual_invalid_count = df_for_excel['is_manually_invalid'].sum()
            invalid_count = df_for_excel['is_invalid'].sum()
            if manual_invalid_count != invalid_count:
                logging.error(f"éŒ¯èª¤ï¼š{subject}ç§‘Excelå°å‡ºå‰ï¼Œç„¡æ•ˆé …æ•¸é‡ ({invalid_count}) èˆ‡æ‰‹å‹•æ¨™è¨˜æ•¸é‡ ({manual_invalid_count}) ä¸ä¸€è‡´ï¼")
        
        # Apply number formatting *before* calling to_excel if needed
        if 'question_difficulty' in df_for_excel.columns:
            df_for_excel['question_difficulty'] = pd.to_numeric(df_for_excel['question_difficulty'], errors='coerce')
        if 'question_time' in df_for_excel.columns:
            df_for_excel['question_time'] = pd.to_numeric(df_for_excel['question_time'], errors='coerce')
            
        # Convert boolean columns to strings for Excel export
        if 'is_correct' in df_for_excel.columns:
            df_for_excel['is_correct'] = df_for_excel['is_correct'].astype(str) # Convert TRUE/FALSE to text
        if 'is_sfe' in df_for_excel.columns:
            df_for_excel['is_sfe'] = df_for_excel['is_sfe'].astype(str)
            
        # Handle is_invalid specifically since we *just* processed it
        if 'is_invalid' in df_for_excel.columns:
            df_for_excel['is_invalid'] = df_for_excel['is_invalid'].astype(str) # Convert TRUE/FALSE to text
            
        # Final validation just to be sure we're exporting valid data
        # Ensures consistent, expectable log output for is_invalid
        try:
            value_counts = df_for_excel['is_invalid'].value_counts().to_dict()
            logging.debug(f"{subject}ç§‘Excelå°å‡ºç›´å‰ï¼Œis_invalidå€¼åˆ†ä½ˆ: {value_counts}")
        except Exception as e:
            logging.warning(f"è¨ˆç®—{subject}ç§‘is_invalidåˆ†ä½ˆæ™‚å‡ºéŒ¯: {e}")
            
        # Calculate & display final invalid count in log
        invalid_count = (df_for_excel['is_invalid'] == 'True').sum() if 'is_invalid' in df_for_excel.columns else 0
        logging.info(f"{subject}ç§‘Excelå°å‡ºåŒ…å« {invalid_count} å€‹ç„¡æ•ˆé¡Œç›®")
        
        # Generate Excel and offer for download - Use function from excel_utils
        excel_bytes = to_excel(df_for_excel, subject_excel_map) # ä½¿ç”¨ç§‘ç›®ç‰¹å®šçš„excel_map
        
        # Offer download button for Excel file - provide bytes to streamlit
        today_str = pd.Timestamp.now().strftime('%Y%m%d')
        tab_container.download_button(
            f"ä¸‹è¼‰ {subject} ç§‘è©³ç´°æ•¸æ“š (Excel)",
            data=excel_bytes,
            file_name=f"{today_str}_GMAT_{subject}_detailed_data.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    except Exception as e:
        tab_container.error(f"æº–å‚™Excelä¸‹è¼‰æ™‚å‡ºéŒ¯: {e}")
        import traceback
        logging.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        tab_container.info(f"å¦‚æœ‰éœ€è¦ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡ä¸¦æä¾›ä»¥ä¸ŠéŒ¯èª¤ä¿¡æ¯ã€‚")

def display_total_results(tab_container):
    """é¡¯ç¤ºTotalåˆ†æ•¸çš„ç™¾åˆ†ä½æ•¸å’Œåœ–è¡¨åˆ†æ"""
    total_data_df = st.session_state.get('total_data')
    # Correctly check if the DataFrame is None, not a DataFrame, or empty
    if total_data_df is None or not isinstance(total_data_df, pd.DataFrame) or total_data_df.empty:
        tab_container.info("å°šæœªè¨­å®šç¸½åˆ†æ•¸æ“šã€‚è«‹åœ¨ã€Œæ•¸æ“šè¼¸å…¥èˆ‡åˆ†æã€æ¨™ç±¤ä¸­çš„ã€ŒTotalã€é ç±¤è¨­å®šåˆ†æ•¸ã€‚")
        return
    
    # ç²å–åˆ†æ•¸æ•¸æ“š
    total_score = st.session_state.total_score
    q_score = st.session_state.q_score
    v_score = st.session_state.v_score
    di_score = st.session_state.di_score
    
    # ä½¿ç”¨ scale-percentile-simulation.ipynb ä¸­æ›´æº–ç¢ºçš„æ•¸æ“šé›†
    datasets = {
        'Quantitative': {
            'color': 'red',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 97, 95, 94, 91, 88, 85, 81, 76, 70,
                64, 57, 50, 43, 37, 32, 26, 22, 19, 15,
                12, 10, 8, 6, 4, 3, 2, 2, 1, 1, 1
            ])
        },
        'Verbal': {
            'color': 'blue',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 99, 99, 98, 97, 94, 90, 84, 76, 67,
                57, 48, 39, 31, 23, 18, 13, 10, 7, 5,
                4, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1
            ])
        },
        'Data Insights': {
            'color': 'black',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 100, 99, 99, 99, 98, 97, 96, 93, 89,
                84, 77, 70, 63, 54, 48, 42, 36, 31, 26,
                21, 18, 15, 12, 10, 8, 7, 6, 5, 4, 4
            ])
        }
    }
    
    # å‡½æ•¸ï¼šæ ¹æ“šç´šåˆ†æ‰¾å°æ‡‰ç™¾åˆ†ä½
    def find_percentile(scale_score, dataset):
        scale = dataset['scale'][::-1]  # åè½‰ç‚ºå‡åº
        percentile = dataset['percentile'][::-1]
    
        if scale_score < scale[0]:
            return percentile[0]
        elif scale_score > scale[-1]:
            return percentile[-1]
        else:
            return np.interp(scale_score, scale, percentile)
    
    # è¨ˆç®—ç™¾åˆ†ä½æ•¸
    q_percentile = find_percentile(q_score, datasets['Quantitative'])
    v_percentile = find_percentile(v_score, datasets['Verbal'])
    di_percentile = find_percentile(di_score, datasets['Data Insights'])
    
    # ç¸½åˆ†ç™¾åˆ†ä½æ•¸è¿‘ä¼¼å°æ‡‰é—œä¿‚ (ä¿ç•™åŸæœ‰ç¸½åˆ†æ˜ å°„)
    total_scores = np.array([800, 770, 740, 710, 680, 650, 620, 590, 560, 530, 500, 450, 400, 350, 300, 250, 200])
    total_percentiles = np.array([99.9, 99, 97, 92, 85, 75, 65, 51, 38, 28, 18, 8, 4, 2, 1, 0.5, 0.1])
    
    # æ’å€¼è¨ˆç®—ç¸½åˆ†ç™¾åˆ†ä½æ•¸
    total_percentile = np.interp(total_score, total_scores[::-1], total_percentiles[::-1])
    
    # çµ„åˆåœ– - å–®ä¸€åœ–è¡¨é¡¯ç¤ºæ‰€æœ‰ç§‘ç›®æ•¸æ“š
    tab_container.subheader("ä¸‰ç§‘åˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰åœ–")
    
    candidate_scores = {
        'Quantitative': q_score,
        'Verbal': v_score,
        'Data Insights': di_score
    }
    
    # å‰µå»ºPlotlyåœ–è¡¨
    fig_combined = go.Figure()
    
    # ä¸åŒç§‘ç›®çš„é¡è‰²æ˜ å°„
    colors = {
        'Quantitative': 'red',
        'Verbal': 'blue',
        'Data Insights': 'black'
    }
    
    # ç¹ªè£½æ‰€æœ‰ç§‘ç›®çš„ç™¾åˆ†ä½æ›²ç·š
    for name, dataset in datasets.items():
        fig_combined.add_trace(
            go.Scatter(
                x=dataset['scale'],
                y=dataset['percentile'],
                mode='lines+markers',
                name=name,
                line=dict(color=colors[name], width=2),
                marker=dict(size=6, color=colors[name])
            )
        )
        
        # æ·»åŠ ç•¶å‰åˆ†æ•¸é»
        score = candidate_scores[name]
        percentile = find_percentile(score, dataset)
        
        # åœ¨ plotly ä¸­ç”Ÿæˆåˆ‡ç·š
        # é¦–å…ˆæº–å‚™æ•¸æ“šç”¨æ–¼æ’å€¼
        sorted_scale = dataset['scale'][::-1]  # åè½‰ç‚ºå‡åº
        sorted_percentile = dataset['percentile'][::-1]
        
        # ä½¿ç”¨ scipy.interpolate.interp1d ä»£æ›¿ UnivariateSplineï¼Œå› ç‚ºæˆ‘å€‘åªéœ€è¦ç°¡å–®çš„æ’å€¼
        from scipy.interpolate import interp1d
        
        # è¨ˆç®—åˆ‡ç·šæ‰€éœ€çš„é»
        # ç‚ºäº†è¨ˆç®—æ–œç‡ï¼Œæˆ‘å€‘å–é»å·¦å³çš„æ•¸æ“šé»
        idx = np.searchsorted(sorted_scale, score)
        if idx > 0 and idx < len(sorted_scale):
            # è¨ˆç®—ç›¸é„°é»çš„æ–œç‡ä¾†è¿‘ä¼¼åˆ‡ç·šæ–œç‡
            x_left = sorted_scale[idx-1]
            y_left = sorted_percentile[idx-1]
            x_right = sorted_scale[idx+1] if idx+1 < len(sorted_scale) else sorted_scale[idx]
            y_right = sorted_percentile[idx+1] if idx+1 < len(sorted_percentile) else sorted_percentile[idx]
            
            # è¨ˆç®—æ–œç‡
            slope = (y_right - y_left) / (x_right - x_left)
            
            # å®šç¾©åˆ‡ç·šç¯„åœ (score Â± 5)
            tangent_range = 5
            x_min = max(score - tangent_range, sorted_scale[0])
            x_max = min(score + tangent_range, sorted_scale[-1])
            x_tangent = np.linspace(x_min, x_max, 50)
            
            # è¨ˆç®—åˆ‡ç·šä¸Šçš„é»
            y_tangent = percentile + slope * (x_tangent - score)
            
            # ç¹ªè£½åˆ‡ç·š
            fig_combined.add_trace(
                go.Scatter(
                    x=x_tangent,
                    y=y_tangent,
                    mode='lines',
                    line=dict(color=colors[name], dash='dash', width=2),
                    name=f"{name} åˆ‡ç·š",
                    showlegend=False
                )
            )
        
        # æ·»åŠ çªå‡ºé¡¯ç¤ºçš„åˆ†æ•¸é»
        fig_combined.add_trace(
            go.Scatter(
                x=[score],
                y=[percentile],
                mode='markers',
                name=f"{name} åˆ†æ•¸",
                marker=dict(
                    color=colors[name],
                    size=15,
                    symbol='x',
                    line=dict(color='white', width=2)
                )
            )
        )
    
    # æ›´æ–°åœ–è¡¨å¸ƒå±€
    fig_combined.update_layout(
        title="GMATåˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰é—œä¿‚",
        xaxis_title="ç´šåˆ†",
        yaxis_title="ç™¾åˆ†ä½",
        xaxis=dict(range=[60, 90], tickmode='linear', tick0=60, dtick=5),
        yaxis=dict(range=[0, 100], tickmode='linear', tick0=0, dtick=10),
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        grid=dict(rows=1, columns=1),
        paper_bgcolor="white",
        plot_bgcolor="white",
        font=dict(family="Arial", size=12)
    )
    
    # æ·»åŠ ç¶²æ ¼ç·š
    fig_combined.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')
    fig_combined.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')
    
    # é¡¯ç¤ºçµ„åˆåœ–
    tab_container.plotly_chart(fig_combined, use_container_width=True)
    
    # æ–°å¢åŠ çš„éƒ¨åˆ†ï¼šåµŒå…¥YouTubeè¦–é »
    tab_container.subheader("äº†è§£ç´šåˆ†è·Ÿç™¾åˆ†ä½ä¹‹é–“çš„é—œä¿‚")
    tab_container.markdown("""
    <iframe width="560" height="315" src="https://www.youtube.com/embed/MLVT-zxaBkE?si=9SJ68LSrvvii35p-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
    """, unsafe_allow_html=True)

# --- Display Results Function (Moved from app.py) ---
def display_results():
    """Displays all diagnostic results in separate tabs."""
    if not st.session_state.get("diagnosis_complete", False) and not st.session_state.get("original_processed_df") :
        st.info("å°šæœªåŸ·è¡Œåˆ†ææˆ–åˆ†ææœªæˆåŠŸå®Œæˆã€‚è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦åŸ·è¡Œåˆ†æã€‚")
        return

    tab_titles = ["Total (ç¸½åˆ†èˆ‡ç™¾åˆ†ä½)"]
    if st.session_state.get("consolidated_report_text"):
        tab_titles.append("âœ¨ AI ç¸½çµå»ºè­°")
    
    # Add subject result tabs
    tab_titles.extend([f"{subject} ç§‘çµæœ" for subject in SUBJECTS])
    
    # Add the new Edit tab
    tab_titles.append("ğŸ”§ ç·¨è¼¯è¨ºæ–·æ¨™ç±¤ & æ›´æ–°AIå»ºè­°")
    
    # Add AI Chat tab last
    tab_titles.append("ğŸ’¬ AI å³æ™‚å•ç­”")

    tabs = st.tabs(tab_titles)
    
    current_tab_index = 0

    # Tab 1: Total Score Analysis
    with tabs[current_tab_index]:
        display_total_results(tabs[current_tab_index])
    current_tab_index += 1
    
    # Tab (Optional): AI Consolidated Report
    if "âœ¨ AI ç¸½çµå»ºè­°" in tab_titles:
        with tabs[current_tab_index]:
            tabs[current_tab_index].subheader("AI æ™ºèƒ½åŒ¯ç¸½èˆ‡å»ºè­°è¡Œå‹•")
            # Make sure to use consolidated_report_text, which is set by set_analysis_results
            report_text_to_display = st.session_state.get("consolidated_report_text", "AIç¸½çµå ±å‘Šç”Ÿæˆä¸­æˆ–ä¸å¯ç”¨ã€‚")
            tabs[current_tab_index].markdown(report_text_to_display)
        current_tab_index += 1

    # Tabs for Q, V, DI
    for subject in SUBJECTS: 
        report_md = st.session_state.report_dict.get(subject, f"æœªæ‰¾åˆ° {subject} ç§‘çš„è¨ºæ–·å ±å‘Šã€‚")
        # Use original_processed_df if processed_df is None (e.g. after an error)
        df_for_subject_display = st.session_state.processed_df if st.session_state.processed_df is not None else st.session_state.original_processed_df
        
        df_subject = pd.DataFrame()
        if df_for_subject_display is not None and not df_for_subject_display.empty:
            df_subject = df_for_subject_display[df_for_subject_display['Subject'] == subject]
        
        subject_tab_title = f"{subject} ç§‘çµæœ"
        # Find the correct index for the subject tab
        try:
            actual_tab_index_for_subject = tab_titles.index(subject_tab_title)
            with tabs[actual_tab_index_for_subject]:
                display_subject_results(subject, tabs[actual_tab_index_for_subject], report_md, df_subject, COLUMN_DISPLAY_CONFIG, EXCEL_COLUMN_MAP)
        except ValueError:
            # This case should ideally not be reached if tab_titles is constructed correctly
            st.error(f"ç„¡æ³•æ‰¾åˆ°åˆ†é  '{subject_tab_title}'ã€‚Tabé…ç½®: {tab_titles}")
            # Do not increment current_tab_index here, as it might mess up subsequent tab indexing if used linearly.
            # Instead, rely on finding index directly.

    # Tab for Editing Diagnostic Labels
    edit_tab_title = "ğŸ”§ ç·¨è¼¯è¨ºæ–·æ¨™ç±¤ & æ›´æ–°AIå»ºè­°"
    try:
        edit_tab_index = tab_titles.index(edit_tab_title)
        with tabs[edit_tab_index]:
            tabs[edit_tab_index].subheader("ç·¨è¼¯è¨ºæ–·æ¨™ç±¤ä¸¦æ›´æ–°AIå·¥å…·/æç¤ºå»ºè­°")
            
            if st.session_state.original_processed_df is None:
                tabs[edit_tab_index].info("æ²’æœ‰å¯ä¾›ç·¨è¼¯çš„è¨ºæ–·æ•¸æ“šã€‚è«‹å…ˆæˆåŠŸåŸ·è¡Œä¸€æ¬¡åˆ†æã€‚")
            else:
                # Initialize edited_df in session_state if it doesn't exist or if we need to reset
                if 'editable_diagnostic_df' not in st.session_state or st.session_state.original_processed_df is not st.session_state.get('_editable_df_source'):
                    st.session_state.editable_diagnostic_df = st.session_state.original_processed_df.copy()
                    st.session_state._editable_df_source = st.session_state.original_processed_df # Track source to detect reset needs

                # Define user-requested columns and their display order
                user_requested_internal_names = [
                    "Subject", "question_position", "is_correct", "question_time",
                    "question_type", "content_domain", "question_fundamental_skill",
                    "is_invalid", "time_performance_category", "diagnostic_params_list"
                ]
                
                # Create a DataFrame view for the editor with only these columns, in this order.
                # Make sure all these columns actually exist in the editable_diagnostic_df.
                # If a column is missing, this will raise a KeyError, which is good for debugging.
                # Alternatively, one could filter `user_requested_internal_names` to only include
                # columns that are actually present in `st.session_state.editable_diagnostic_df.columns`.
                cols_to_display = [col for col in user_requested_internal_names if col in st.session_state.editable_diagnostic_df.columns]
                df_for_editor = st.session_state.editable_diagnostic_df[cols_to_display].copy()

                # Prepare 'diagnostic_params_list' for TextColumn: convert list to comma-separated string
                if 'diagnostic_params_list' in df_for_editor.columns:
                    def format_tags_for_text_editor(tags_list):
                        if isinstance(tags_list, list):
                            # Filter out None or empty strings from list before joining
                            return ", ".join(str(tag).strip() for tag in tags_list if tag and str(tag).strip())
                        if pd.isna(tags_list) or tags_list is None:
                            return "" # Return empty string for NaN/None
                        # If it's already a string (e.g., from previous edit), just return it after stripping
                        return str(tags_list).strip()
                    df_for_editor['diagnostic_params_list'] = df_for_editor['diagnostic_params_list'].apply(format_tags_for_text_editor)

                # Define column configurations for the data_editor, tailored to the new view
                editor_column_config = {
                    "Subject": st.column_config.TextColumn("ç§‘ç›®", disabled=True),
                    "question_position": st.column_config.NumberColumn("é¡Œè™Ÿ", help="é¡Œç›®åœ¨è©²ç§‘ç›®ä¸­çš„é †åº", disabled=True),
                    "is_correct": st.column_config.CheckboxColumn("ç­”å°", help="è©²é¡Œæ˜¯å¦å›ç­”æ­£ç¢º", disabled=True),
                    "question_time": st.column_config.NumberColumn("ç”¨æ™‚", help="è©²é¡Œä½œç­”ç”¨æ™‚(åˆ†é˜)", format="%.2f", disabled=True),
                    "question_type": st.column_config.TextColumn("é¡Œå‹", disabled=True),
                    "content_domain": st.column_config.TextColumn("å…§å®¹é ˜åŸŸ", disabled=True),
                    "question_fundamental_skill": st.column_config.TextColumn("è€ƒå¯Ÿèƒ½åŠ›", disabled=True),
                    "is_invalid": st.column_config.CheckboxColumn("æ¨™è¨˜ç„¡æ•ˆ", help="è©²é¡Œæ˜¯å¦è¢«æ¨™è¨˜ç‚ºç„¡æ•ˆ", disabled=True),
                    "time_performance_category": st.column_config.SelectboxColumn(
                        "æ™‚é–“è¡¨ç¾",
                        help="é»æ“Šç·¨è¼¯ä»¥é¸æ“‡æ™‚é–“è¡¨ç¾åˆ†é¡",
                        options=["Slow & Wrong", "Slow & Right", "Normal & Wrong", "Normal & Right", "Fast & Wrong", "Fast & Right", "N/A"],
                        required=True
                    ),
                    "diagnostic_params_list": st.column_config.TextColumn(
                        "è¨ºæ–·æ¨™ç±¤ (é€—è™Ÿåˆ†éš”)",
                        help="è«‹ç”¨é€—è™Ÿ (,) åˆ†éš”å¤šå€‹æ¨™ç±¤ã€‚ä¾‹å¦‚ï¼šæ¨™ç±¤1,æ¨™ç±¤2,æ¨™ç±¤3",
                        width="large"
                    )
                }
                
                final_editor_column_config = {k: v for k, v in editor_column_config.items() if k in df_for_editor.columns}

                tabs[edit_tab_index].markdown("**èªªæ˜:** åœ¨ä¸‹æ–¹è¡¨æ ¼ä¸­ä¿®æ”¹ã€Œè¨ºæ–·æ¨™ç±¤ã€æˆ–ã€Œæ™‚é–“è¡¨ç¾ã€ã€‚å°æ–¼ã€Œè¨ºæ–·æ¨™ç±¤ã€ï¼Œè«‹ç”¨é€—è™Ÿåˆ†éš”å¤šå€‹æ¨™ç±¤ã€‚å®Œæˆå¾Œé»æ“Šã€Œå¥—ç”¨è®Šæ›´ã€æŒ‰éˆ•ã€‚")
                
                edited_df_subset_from_editor = tabs[edit_tab_index].data_editor(
                    df_for_editor,
                    column_config=final_editor_column_config,
                    use_container_width=True,
                    num_rows="fixed", 
                    key="diagnosis_label_editor" 
                )

                if edited_df_subset_from_editor is not None:
                    updated_full_df = st.session_state.editable_diagnostic_df.copy()
                    
                    for col_name in edited_df_subset_from_editor.columns:
                        if col_name in updated_full_df.columns:
                            if col_name == 'diagnostic_params_list':
                                # Convert comma-separated string back to list of strings
                                def parse_tags_from_text_editor(tags_str):
                                    if pd.isna(tags_str) or not isinstance(tags_str, str) or not tags_str.strip():
                                        return []
                                    return [tag.strip() for tag in tags_str.split(',') if tag.strip()]
                                
                                updated_full_df[col_name] = edited_df_subset_from_editor[col_name].apply(parse_tags_from_text_editor)
                            else:
                                updated_full_df[col_name] = edited_df_subset_from_editor[col_name]
                    
                    st.session_state.editable_diagnostic_df = updated_full_df
                    st.session_state.ai_prompts_need_regeneration = True

                col1, col2 = tabs[edit_tab_index].columns(2)
                if col1.button("â†º é‡è¨­ç‚ºåŸå§‹æ¨™ç±¤", key="reset_editable_df"):
                    st.session_state.editable_diagnostic_df = st.session_state.original_processed_df.copy()
                    st.session_state.ai_prompts_need_regeneration = False # No need to regenerate if reset
                    if 'generated_ai_prompts_for_edit_tab' in st.session_state:
                        del st.session_state['generated_ai_prompts_for_edit_tab'] # Clear previous prompts
                    st.experimental_rerun()

                if col2.button("âœ“ å¥—ç”¨è®Šæ›´ä¸¦æ›´æ–°AIå»ºè­°", key="apply_editable_df", type="primary"):
                    # The editor already updated st.session_state.editable_diagnostic_df
                    # So, we just need to flag for regeneration
                    st.session_state.ai_prompts_need_regeneration = True
                    tabs[edit_tab_index].success("è®Šæ›´å·²å¥—ç”¨ï¼AIå»ºè­°å°‡åœ¨ä¸‹æ–¹æ›´æ–°ã€‚")
                    # We will handle regeneration and display below
                
                # Display AI Prompts if regeneration is needed or already generated
                if st.session_state.get('ai_prompts_need_regeneration', False) or 'generated_ai_prompts_for_edit_tab' in st.session_state:
                    with st.spinner("æ­£åœ¨æ ¹æ“šæ‚¨çš„ç·¨è¼¯ç”ŸæˆAIå»ºè­°..."):
                        # --- TODO: Call new AI prompt generation functions here --- 
                        # These functions will take st.session_state.editable_diagnostic_df as input
                        # For now, using placeholders. These need to be implemented in respective diagnostic modules.
                        
                        q_prompts = ""
                        v_prompts = ""
                        di_prompts = ""

                        df_to_generate_prompts = st.session_state.editable_diagnostic_df

                        # Placeholder: Simulate calling the actual functions when they are ready
                        # Q Prompts
                        # from gmat_diagnosis_app.diagnostics.q_modules.ai_prompts import generate_q_ai_tool_recommendations 
                        # q_df_subject = df_to_generate_prompts[df_to_generate_prompts['Subject'] == 'Q']
                        # if not q_df_subject.empty: q_prompts = generate_q_ai_tool_recommendations(q_df_subject)
                        
                        # V Prompts - similar structure
                        # from gmat_diagnosis_app.diagnostics.v_modules.ai_prompts import generate_v_ai_tool_recommendations
                        # v_df_subject = df_to_generate_prompts[df_to_generate_prompts['Subject'] == 'V']
                        # if not v_df_subject.empty: v_prompts = generate_v_ai_tool_recommendations(v_df_subject)

                        # DI Prompts - similar structure
                        # from gmat_diagnosis_app.diagnostics.di_modules.ai_prompts import generate_di_ai_tool_recommendations
                        # di_df_subject = df_to_generate_prompts[df_to_generate_prompts['Subject'] == 'DI']
                        # if not di_df_subject.empty: di_prompts = generate_di_ai_tool_recommendations(di_df_subject)

                        # For demonstration, using mock data
                        q_prompts = "Qç§‘AIå»ºè­° (åŸºæ–¼ç·¨è¼¯):\n- å·¥å…·A: ...\n- æç¤ºB: ..."
                        v_prompts = "Vç§‘AIå»ºè­° (åŸºæ–¼ç·¨è¼¯):\n- å·¥å…·C: ...\n- æç¤ºD: ..."
                        di_prompts = "DIç§‘AIå»ºè­° (åŸºæ–¼ç·¨è¼¯):\n- å·¥å…·E: ...\n- æç¤ºF: ..."

                        all_prompts = f"### AI å·¥å…·èˆ‡æç¤ºå»ºè­° (åŸºæ–¼æ‚¨çš„ç·¨è¼¯)\n\n**Quantitative (Q) ç§‘ç›®:**\n{q_prompts if q_prompts else '(ç„¡ç‰¹å®šå»ºè­°)'}\n\n**Verbal (V) ç§‘ç›®:**\n{v_prompts if v_prompts else '(ç„¡ç‰¹å®šå»ºè­°)'}\n\n**Data Insights (DI) ç§‘ç›®:**\n{di_prompts if di_prompts else '(ç„¡ç‰¹å®šå»ºè­°)'}"
                        
                        st.session_state.generated_ai_prompts_for_edit_tab = all_prompts
                        st.session_state.ai_prompts_need_regeneration = False # Reset flag after generation
                    
                if 'generated_ai_prompts_for_edit_tab' in st.session_state:
                    tabs[edit_tab_index].markdown(st.session_state.generated_ai_prompts_for_edit_tab)

    except ValueError:
        # This case should ideally not be reached if tab_titles is constructed correctly
        st.error(f"ç„¡æ³•æ‰¾åˆ°åˆ†é  '{edit_tab_title}'ã€‚Tabé…ç½®: {tab_titles}")
        

    # Tab for AI Chat - find its index
    ai_chat_tab_title = "ğŸ’¬ AI å³æ™‚å•ç­”"
    try:
        ai_chat_tab_index = tab_titles.index(ai_chat_tab_title)
        with tabs[ai_chat_tab_index]:
            tabs[ai_chat_tab_index].subheader("èˆ‡ AI å³æ™‚å•ç­”")
            if st.session_state.get('openai_api_key'):
                display_chat_interface(st.session_state)
            else:
                tabs[ai_chat_tab_index].info("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥ OpenAI API Key ä»¥å•Ÿç”¨ AI å•ç­”åŠŸèƒ½ã€‚")
    except ValueError:
        # This should not happen if it's in tab_titles
        st.error(f"ç„¡æ³•æ‰¾åˆ°åˆ†é  '{ai_chat_tab_title}'.") 