"""
çµæœé¡¯ç¤ºæ¨¡çµ„
é¡¯ç¤ºè¨ºæ–·çµæœçš„åŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from gmat_diagnosis_app.utils.styling import apply_styles
from gmat_diagnosis_app.utils.excel_utils import to_excel
from gmat_diagnosis_app.constants.config import SUBJECTS, EXCEL_COLUMN_MAP

# --- Column Display Configuration (Moved from app.py) ---
COLUMN_DISPLAY_CONFIG = {
    "question_position": st.column_config.NumberColumn("é¡Œè™Ÿ", help="é¡Œç›®é †åº"),
    "question_type": st.column_config.TextColumn("é¡Œå‹"),
    "question_fundamental_skill": st.column_config.TextColumn("è€ƒå¯Ÿèƒ½åŠ›"),
    "question_difficulty": st.column_config.NumberColumn("é›£åº¦(æ¨¡æ“¬)", help="ç³»çµ±æ¨¡æ“¬çš„é¡Œç›®é›£åº¦ (æœ‰æ•ˆé¡Œç›®)", format="%.2f", width="small"),
    "question_time": st.column_config.NumberColumn("ç”¨æ™‚(åˆ†)", format="%.2f", width="small"),
    "time_performance_category": st.column_config.TextColumn("æ™‚é–“è¡¨ç¾"),
    "content_domain": st.column_config.TextColumn("å…§å®¹é ˜åŸŸ"),
    "diagnostic_params_list": st.column_config.ListColumn("è¨ºæ–·æ¨™ç±¤", help="åˆæ­¥è¨ºæ–·æ¨™ç±¤", width="medium"),
    "is_correct": st.column_config.CheckboxColumn("ç­”å°?", help="æ˜¯å¦å›ç­”æ­£ç¢º"),
    "is_sfe": st.column_config.CheckboxColumn("SFE?", help="æ˜¯å¦ç‚ºSpecial Focus Error", width="small"),
    "is_invalid": st.column_config.CheckboxColumn("æ¨™è¨˜ç„¡æ•ˆ?", help="æ­¤é¡Œæ˜¯å¦è¢«æ¨™è¨˜ç‚ºç„¡æ•ˆ (æ‰‹å‹•å„ªå…ˆ)", width="small"),
    "overtime": None, # Internal column for styling
    "is_manually_invalid": None, # Hide the intermediate manual flag
}

def display_subject_results(subject, tab_container, report_md, df_subject, col_config, excel_map):
    """Displays the diagnosis report, styled DataFrame, and download button for a subject."""
    tab_container.subheader(f"{subject} ç§‘è¨ºæ–·å ±å‘Š")
    # This line renders the markdown report with wrapping
    tab_container.markdown(report_md if report_md else f"æœªæ‰¾åˆ° {subject} ç§‘çš„è¨ºæ–·å ±å‘Šã€‚", unsafe_allow_html=True) # Added unsafe_allow_html=True just in case AI uses basic HTML

    tab_container.subheader(f"{subject} ç§‘è©³ç´°æ•¸æ“š (å«è¨ºæ–·æ¨™ç±¤)")

    if df_subject is None or df_subject.empty:
        tab_container.write(f"æ²’æœ‰æ‰¾åˆ° {subject} ç§‘çš„è©³ç´°æ•¸æ“šå¯ä¾›é¡¯ç¤ºã€‚")
        return

    # è¤‡è£½é…ç½®ä»¥é€²è¡Œç§‘ç›®ç‰¹å®šèª¿æ•´
    subject_col_config = col_config.copy()
    subject_excel_map = excel_map.copy()
    
    # é‡å°DIç§‘ç›®ç§»é™¤ã€Œè€ƒå¯Ÿèƒ½åŠ›ã€æ¬„ä½
    if subject == 'DI':
        if 'question_fundamental_skill' in subject_col_config:
            del subject_col_config['question_fundamental_skill']
        if 'question_fundamental_skill' in subject_excel_map:
            del subject_excel_map['question_fundamental_skill']

    # Prepare DataFrame for Display
    # 1. Select columns based on keys in col_config that exist in the data
    cols_available = [k for k in subject_col_config.keys() if k in df_subject.columns]
    df_to_display = df_subject[cols_available].copy()

    # 2. Define column order for st.dataframe (exclude those with None config value, like 'overtime')
    columns_for_st_display_order = [k for k in cols_available if subject_col_config.get(k) is not None]

    # 3. Display styled DataFrame
    try:
        # Ensure necessary columns for styling exist with defaults
        if 'overtime' not in df_to_display.columns: df_to_display['overtime'] = False
        if 'is_correct' not in df_to_display.columns: df_to_display['is_correct'] = True # Assume correct if missing for styling

        styled_df = df_to_display.style.set_properties(**{'text-align': 'left'}) \
                                       .set_table_styles([dict(selector='th', props=[('text-align', 'left')])]) \
                                       .apply(apply_styles, axis=1)

        tab_container.dataframe(
            styled_df,
            column_config=subject_col_config,
            column_order=columns_for_st_display_order,
            hide_index=True,
            use_container_width=True
        )
    except Exception as e:
        tab_container.error(f"ç„¡æ³•æ‡‰ç”¨æ¨£å¼æˆ–é¡¯ç¤º {subject} ç§‘æ•¸æ“š: {e}")
        # Fallback: Display without styling, only showing configured columns
        try:
             tab_container.dataframe(
                 df_to_display[columns_for_st_display_order], # Use only displayable columns
                 column_config=subject_col_config,
                 hide_index=True,
                 use_container_width=True
             )
        except Exception as fallback_e:
             tab_container.error(f"é¡¯ç¤ºå›é€€æ•¸æ“šæ™‚ä¹Ÿç™¼ç”ŸéŒ¯èª¤: {fallback_e}")


    # 4. Download Button
    try:
        # Prepare a copy specifically for Excel export using excel_map
        df_for_excel = df_subject[[k for k in subject_excel_map.keys() if k in df_subject.columns]].copy()

        # Apply number formatting *before* calling to_excel if needed
        if 'question_difficulty' in df_for_excel.columns:
             df_for_excel['question_difficulty'] = pd.to_numeric(df_for_excel['question_difficulty'], errors='coerce')
        if 'question_time' in df_for_excel.columns:
             df_for_excel['question_time'] = pd.to_numeric(df_for_excel['question_time'], errors='coerce')
        # Convert bools to string representation if desired for Excel output clarity
        if 'is_correct' in df_for_excel.columns:
             df_for_excel['is_correct'] = df_for_excel['is_correct'].astype(str) # Convert TRUE/FALSE to text
        if 'is_sfe' in df_for_excel.columns:
             df_for_excel['is_sfe'] = df_for_excel['is_sfe'].astype(str)

        # Ensure 'is_invalid' is also string for conditional formatting in to_excel
        if 'is_invalid' in df_for_excel.columns:
             df_for_excel['is_invalid'] = df_for_excel['is_invalid'].astype(str) # Convert TRUE/FALSE to text


        excel_bytes = to_excel(df_for_excel, subject_excel_map) # ä½¿ç”¨ç§‘ç›®ç‰¹å®šçš„excel_map

        tab_container.download_button(
            label=f"ä¸‹è¼‰ {subject} ç§‘è©³ç´°æ•¸æ“š (Excel)",
            data=excel_bytes,
            file_name=f"gmat_diag_{subject}_detailed_data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key=f"download_excel_{subject}"
        )
    except Exception as e:
        tab_container.error(f"ç„¡æ³•ç”Ÿæˆ {subject} ç§‘çš„ Excel ä¸‹è¼‰æ–‡ä»¶: {e}") 

def display_total_results(tab_container):
    """é¡¯ç¤ºTotalåˆ†æ•¸çš„ç™¾åˆ†ä½æ•¸å’Œåœ–è¡¨åˆ†æ"""
    total_data_df = st.session_state.get('total_data')
    # Correctly check if the DataFrame is None, not a DataFrame, or empty
    if total_data_df is None or not isinstance(total_data_df, pd.DataFrame) or total_data_df.empty:
        tab_container.info("å°šæœªè¨­å®šç¸½åˆ†æ•¸æ“šã€‚è«‹åœ¨ã€Œæ•¸æ“šè¼¸å…¥èˆ‡åˆ†æã€æ¨™ç±¤ä¸­çš„ã€ŒTotalã€é ç±¤è¨­å®šåˆ†æ•¸ã€‚")
        return
    
    # ç²å–åˆ†æ•¸æ•¸æ“š
    total_score = st.session_state.total_score
    q_score = st.session_state.q_score
    v_score = st.session_state.v_score
    di_score = st.session_state.di_score
    
    tab_container.subheader("GMAT åˆ†æ•¸åˆ†æ")
    
    # é¡¯ç¤ºæ‰€é¸åˆ†æ•¸
    if st.session_state.get('score_df') is not None:
        tab_container.dataframe(st.session_state.score_df, hide_index=True, use_container_width=True)
    else:
        score_data = {
            'Score_Type': ['Total Score', 'Q Scaled Score', 'V Scaled Score', 'DI Scaled Score'],
            'Score': [total_score, q_score, v_score, di_score]
        }
        tab_container.dataframe(pd.DataFrame(score_data), hide_index=True, use_container_width=True)
    
    # ç”Ÿæˆç™¾åˆ†ä½æ•¸åˆ†æ
    tab_container.subheader("åˆ†æ•¸ç™¾åˆ†ä½åˆ†æ")
    
    # ä½¿ç”¨ scale-percentile-simulation.ipynb ä¸­æ›´æº–ç¢ºçš„æ•¸æ“šé›†
    datasets = {
        'Quantitative': {
            'color': 'red',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 97, 95, 94, 91, 88, 85, 81, 76, 70,
                64, 57, 50, 43, 37, 32, 26, 22, 19, 15,
                12, 10, 8, 6, 4, 3, 2, 2, 1, 1, 1
            ])
        },
        'Verbal': {
            'color': 'blue',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 99, 99, 98, 97, 94, 90, 84, 76, 67,
                57, 48, 39, 31, 23, 18, 13, 10, 7, 5,
                4, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1
            ])
        },
        'Data Insights': {
            'color': 'black',
            'scale': np.array([
                90, 89, 88, 87, 86, 85, 84, 83, 82, 81,
                80, 79, 78, 77, 76, 75, 74, 73, 72, 71,
                70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60
            ]),
            'percentile': np.array([
                100, 100, 99, 99, 99, 98, 97, 96, 93, 89,
                84, 77, 70, 63, 54, 48, 42, 36, 31, 26,
                21, 18, 15, 12, 10, 8, 7, 6, 5, 4, 4
            ])
        }
    }
    
    # å‡½æ•¸ï¼šæ ¹æ“šç´šåˆ†æ‰¾å°æ‡‰ç™¾åˆ†ä½
    def find_percentile(scale_score, dataset):
        scale = dataset['scale'][::-1]  # åè½‰ç‚ºå‡åº
        percentile = dataset['percentile'][::-1]
    
        if scale_score < scale[0]:
            return percentile[0]
        elif scale_score > scale[-1]:
            return percentile[-1]
        else:
            return np.interp(scale_score, scale, percentile)
    
    # è¨ˆç®—ç™¾åˆ†ä½æ•¸
    q_percentile = find_percentile(q_score, datasets['Quantitative'])
    v_percentile = find_percentile(v_score, datasets['Verbal'])
    di_percentile = find_percentile(di_score, datasets['Data Insights'])
    
    # ç¸½åˆ†ç™¾åˆ†ä½æ•¸è¿‘ä¼¼å°æ‡‰é—œä¿‚ (ä¿ç•™åŸæœ‰ç¸½åˆ†æ˜ å°„)
    total_scores = np.array([800, 770, 740, 710, 680, 650, 620, 590, 560, 530, 500, 450, 400, 350, 300, 250, 200])
    total_percentiles = np.array([99.9, 99, 97, 92, 85, 75, 65, 51, 38, 28, 18, 8, 4, 2, 1, 0.5, 0.1])
    
    # æ’å€¼è¨ˆç®—ç¸½åˆ†ç™¾åˆ†ä½æ•¸
    total_percentile = np.interp(total_score, total_scores[::-1], total_percentiles[::-1])
    
    # å‰µå»ºç™¾åˆ†ä½æ•¸DataFrame
    percentile_data = {
        'Score_Type': ['Total Score', 'Q Scaled Score', 'V Scaled Score', 'DI Scaled Score'],
        'Score': [total_score, q_score, v_score, di_score],
        'Percentile': [
            f"{total_percentile:.1f}%", 
            f"{q_percentile:.1f}%", 
            f"{v_percentile:.1f}%", 
            f"{di_percentile:.1f}%"
        ]
    }
    percentile_df = pd.DataFrame(percentile_data)
    
    # é¡¯ç¤ºç™¾åˆ†ä½æ•¸è¡¨æ ¼
    tab_container.dataframe(percentile_df, hide_index=True, use_container_width=True)
    
    # æ¨¡ä»¿ scale-percentile-simulation.ipynb ä¸­çš„åœ–è¡¨ - ä½¿ç”¨å…©ç¨®åœ–è¡¨æ–¹å¼
    
    # 1. çµ„åˆåœ– - å–®ä¸€åœ–è¡¨é¡¯ç¤ºæ‰€æœ‰ç§‘ç›®æ•¸æ“š
    tab_container.subheader("ä¸‰ç§‘åˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰åœ– (çµ„åˆåœ–)")
    
    candidate_scores = {
        'Quantitative': q_score,
        'Verbal': v_score,
        'Data Insights': di_score
    }
    
    # å‰µå»ºPlotlyåœ–è¡¨
    fig_combined = go.Figure()
    
    # ä¸åŒç§‘ç›®çš„é¡è‰²æ˜ å°„
    colors = {
        'Quantitative': 'red',
        'Verbal': 'blue',
        'Data Insights': 'black'
    }
    
    # ç¹ªè£½æ‰€æœ‰ç§‘ç›®çš„ç™¾åˆ†ä½æ›²ç·š
    for name, dataset in datasets.items():
        fig_combined.add_trace(
            go.Scatter(
                x=dataset['scale'],
                y=dataset['percentile'],
                mode='lines+markers',
                name=name,
                line=dict(color=colors[name], width=2),
                marker=dict(size=6, color=colors[name])
            )
        )
        
        # æ·»åŠ ç•¶å‰åˆ†æ•¸é»
        score = candidate_scores[name]
        percentile = find_percentile(score, dataset)
        
        # åœ¨ plotly ä¸­ç”Ÿæˆåˆ‡ç·š
        # é¦–å…ˆæº–å‚™æ•¸æ“šç”¨æ–¼æ’å€¼
        sorted_scale = dataset['scale'][::-1]  # åè½‰ç‚ºå‡åº
        sorted_percentile = dataset['percentile'][::-1]
        
        # ä½¿ç”¨ scipy.interpolate.interp1d ä»£æ›¿ UnivariateSplineï¼Œå› ç‚ºæˆ‘å€‘åªéœ€è¦ç°¡å–®çš„æ’å€¼
        from scipy.interpolate import interp1d
        
        # è¨ˆç®—åˆ‡ç·šæ‰€éœ€çš„é»
        # ç‚ºäº†è¨ˆç®—æ–œç‡ï¼Œæˆ‘å€‘å–é»å·¦å³çš„æ•¸æ“šé»
        idx = np.searchsorted(sorted_scale, score)
        if idx > 0 and idx < len(sorted_scale):
            # è¨ˆç®—ç›¸é„°é»çš„æ–œç‡ä¾†è¿‘ä¼¼åˆ‡ç·šæ–œç‡
            x_left = sorted_scale[idx-1]
            y_left = sorted_percentile[idx-1]
            x_right = sorted_scale[idx+1] if idx+1 < len(sorted_scale) else sorted_scale[idx]
            y_right = sorted_percentile[idx+1] if idx+1 < len(sorted_percentile) else sorted_percentile[idx]
            
            # è¨ˆç®—æ–œç‡
            slope = (y_right - y_left) / (x_right - x_left)
            
            # å®šç¾©åˆ‡ç·šç¯„åœ (score Â± 5)
            tangent_range = 5
            x_min = max(score - tangent_range, sorted_scale[0])
            x_max = min(score + tangent_range, sorted_scale[-1])
            x_tangent = np.linspace(x_min, x_max, 50)
            
            # è¨ˆç®—åˆ‡ç·šä¸Šçš„é»
            y_tangent = percentile + slope * (x_tangent - score)
            
            # ç¹ªè£½åˆ‡ç·š
            fig_combined.add_trace(
                go.Scatter(
                    x=x_tangent,
                    y=y_tangent,
                    mode='lines',
                    line=dict(color=colors[name], dash='dash', width=2),
                    name=f"{name} åˆ‡ç·š",
                    showlegend=False
                )
            )
        
        # æ·»åŠ çªå‡ºé¡¯ç¤ºçš„åˆ†æ•¸é»
        fig_combined.add_trace(
            go.Scatter(
                x=[score],
                y=[percentile],
                mode='markers',
                name=f"{name} åˆ†æ•¸",
                marker=dict(
                    color=colors[name],
                    size=15,
                    symbol='x',
                    line=dict(color='white', width=2)
                )
            )
        )
    
    # æ›´æ–°åœ–è¡¨å¸ƒå±€
    fig_combined.update_layout(
        title="GMATåˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰é—œä¿‚",
        xaxis_title="ç´šåˆ†",
        yaxis_title="ç™¾åˆ†ä½",
        xaxis=dict(range=[60, 90], tickmode='linear', tick0=60, dtick=5),
        yaxis=dict(range=[0, 100], tickmode='linear', tick0=0, dtick=10),
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        grid=dict(rows=1, columns=1),
        paper_bgcolor="white",
        plot_bgcolor="white",
        font=dict(family="Arial", size=12)
    )
    
    # æ·»åŠ ç¶²æ ¼ç·š
    fig_combined.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')
    fig_combined.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')
    
    # é¡¯ç¤ºçµ„åˆåœ–
    tab_container.plotly_chart(fig_combined, use_container_width=True)
    
    # 2. å­åœ–ç‰ˆæœ¬ - ä¿ç•™åŸæœ‰å­åœ–æ–¹å¼ä½†ä½¿ç”¨æ›´æº–ç¢ºçš„æ•¸æ“š
    tab_container.subheader("åˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰åœ– (å­åœ–)")
    
    # å‰µå»ºä¸€å€‹åŒ…å«å››å€‹å­åœ–çš„åœ–è¡¨
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            "GMAT ç¸½åˆ†ç™¾åˆ†ä½åˆ†å¸ƒ", 
            "Qç§‘ç´šåˆ†ç™¾åˆ†ä½åˆ†å¸ƒ", 
            "Vç§‘ç´šåˆ†ç™¾åˆ†ä½åˆ†å¸ƒ", 
            "DIç§‘ç´šåˆ†ç™¾åˆ†ä½åˆ†å¸ƒ"
        ),
        vertical_spacing=0.15,
        horizontal_spacing=0.1
    )
    
    # åœ–è¡¨1: ç¸½åˆ†ç™¾åˆ†ä½
    fig.add_trace(
        go.Scatter(
            x=total_scores, 
            y=total_percentiles, 
            mode='lines+markers',
            name='ç¸½åˆ†ç™¾åˆ†ä½',
            line=dict(color='blue', width=2),
            marker=dict(size=6)
        ),
        row=1, col=1
    )
    # æ·»åŠ ç•¶å‰ç¸½åˆ†ä½ç½®
    fig.add_trace(
        go.Scatter(
            x=[total_score], 
            y=[total_percentile], 
            mode='markers',
            name='ç•¶å‰ç¸½åˆ†',
            marker=dict(color='red', size=12, symbol='star')
        ),
        row=1, col=1
    )
    
    # åœ–è¡¨2: Qç§‘ç´šåˆ†ç™¾åˆ†ä½
    fig.add_trace(
        go.Scatter(
            x=datasets['Quantitative']['scale'], 
            y=datasets['Quantitative']['percentile'], 
            mode='lines+markers',
            name='Qç§‘ç´šåˆ†ç™¾åˆ†ä½',
            line=dict(color='red', width=2),
            marker=dict(size=6)
        ),
        row=1, col=2
    )
    # æ·»åŠ ç•¶å‰Qç§‘ç´šåˆ†ä½ç½®
    fig.add_trace(
        go.Scatter(
            x=[q_score], 
            y=[q_percentile], 
            mode='markers',
            name='ç•¶å‰Qç§‘ç´šåˆ†',
            marker=dict(color='red', size=12, symbol='star')
        ),
        row=1, col=2
    )
    
    # åœ–è¡¨3: Vç§‘ç´šåˆ†ç™¾åˆ†ä½
    fig.add_trace(
        go.Scatter(
            x=datasets['Verbal']['scale'], 
            y=datasets['Verbal']['percentile'], 
            mode='lines+markers',
            name='Vç§‘ç´šåˆ†ç™¾åˆ†ä½',
            line=dict(color='blue', width=2),
            marker=dict(size=6)
        ),
        row=2, col=1
    )
    # æ·»åŠ ç•¶å‰Vç§‘ç´šåˆ†ä½ç½®
    fig.add_trace(
        go.Scatter(
            x=[v_score], 
            y=[v_percentile], 
            mode='markers',
            name='ç•¶å‰Vç§‘ç´šåˆ†',
            marker=dict(color='blue', size=12, symbol='star')
        ),
        row=2, col=1
    )
    
    # åœ–è¡¨4: DIç§‘ç´šåˆ†ç™¾åˆ†ä½
    fig.add_trace(
        go.Scatter(
            x=datasets['Data Insights']['scale'], 
            y=datasets['Data Insights']['percentile'], 
            mode='lines+markers',
            name='DIç§‘ç´šåˆ†ç™¾åˆ†ä½',
            line=dict(color='black', width=2),
            marker=dict(size=6)
        ),
        row=2, col=2
    )
    # æ·»åŠ ç•¶å‰DIç§‘ç´šåˆ†ä½ç½®
    fig.add_trace(
        go.Scatter(
            x=[di_score], 
            y=[di_percentile], 
            mode='markers',
            name='ç•¶å‰DIç§‘ç´šåˆ†',
            marker=dict(color='black', size=12, symbol='star')
        ),
        row=2, col=2
    )
    
    # æ›´æ–°åœ–è¡¨å¸ƒå±€
    fig.update_layout(
        height=800,
        width=800,
        title_text="GMATåˆ†æ•¸èˆ‡ç™¾åˆ†ä½å°æ‡‰é—œä¿‚",
        showlegend=False,
        template="plotly_white"
    )
    
    # æ›´æ–°Xè»¸ç¯„åœ
    fig.update_xaxes(title_text="ç¸½åˆ†", range=[200, 800], row=1, col=1)
    fig.update_xaxes(title_text="Qç§‘ç´šåˆ†", range=[60, 90], row=1, col=2)
    fig.update_xaxes(title_text="Vç§‘ç´šåˆ†", range=[60, 90], row=2, col=1)
    fig.update_xaxes(title_text="DIç§‘ç´šåˆ†", range=[60, 90], row=2, col=2)
    
    # æ›´æ–°Yè»¸ç¯„åœ
    fig.update_yaxes(title_text="ç™¾åˆ†ä½(%)", range=[0, 100], row=1, col=1)
    fig.update_yaxes(title_text="ç™¾åˆ†ä½(%)", range=[0, 100], row=1, col=2)
    fig.update_yaxes(title_text="ç™¾åˆ†ä½(%)", range=[0, 100], row=2, col=1)
    fig.update_yaxes(title_text="ç™¾åˆ†ä½(%)", range=[0, 100], row=2, col=2)
    
    # ä¿å­˜åœ–è¡¨åˆ°session state
    st.session_state.total_plot = fig
    
    # é¡¯ç¤ºåœ–è¡¨
    tab_container.plotly_chart(fig, use_container_width=True)
    
    # æ›´æ–°ç¸½åˆ†è¨ˆç®—å…¬å¼
    new_estimated_score = -1005.3296 + 6.7098 * q_score + 6.6404 * v_score + 6.7954 * di_score
    
    # æ·»åŠ ç¸½åˆ†è¨ˆç®—å…¬å¼èªªæ˜
    tab_container.info(f"æ ¹æ“šå…¬å¼è¨ˆç®—çš„é ä¼°ç¸½åˆ†: {new_estimated_score:.2f}ï¼Œå¯¦éš›ç¸½åˆ†: {total_score}")
    
    # æ·»åŠ è§£é‡‹å’Œåˆ†æ
    tab_container.subheader("åˆ†æ•¸è§£é‡‹")
    
    # æ ¹æ“šç¸½åˆ†åˆ†æ
    if total_score >= 750:
        score_analysis = "æ‚¨çš„ç¸½åˆ†è™•æ–¼é ‚å°–æ°´å¹³ï¼ˆ99%ä»¥ä¸Šï¼‰ï¼Œæœ‰ç«¶çˆ­åŠ›ç”³è«‹å…¨çƒä»»ä½•å•†å­¸é™¢ã€‚"
    elif total_score >= 700:
        score_analysis = "æ‚¨çš„ç¸½åˆ†éå¸¸å„ªç§€ï¼ˆ90%ä»¥ä¸Šï¼‰ï¼Œå°å¤§å¤šæ•¸é ‚å°–å•†å­¸é™¢æœ‰ç«¶çˆ­åŠ›ã€‚"
    elif total_score >= 650:
        score_analysis = "æ‚¨çš„ç¸½åˆ†è‰¯å¥½ï¼ˆå¤§ç´„70-80%ç™¾åˆ†ä½ï¼‰ï¼Œå°è¨±å¤šå„ªç§€å•†å­¸é™¢å…·æœ‰ç«¶çˆ­åŠ›ã€‚"
    elif total_score >= 600:
        score_analysis = "æ‚¨çš„ç¸½åˆ†è™•æ–¼ä¸­ç­‰åä¸Šæ°´å¹³ï¼ˆç´„50-60%ç™¾åˆ†ä½ï¼‰ï¼Œå°éƒ¨åˆ†å•†å­¸é™¢æœ‰ç«¶çˆ­åŠ›ï¼Œå¯è€ƒæ…®æé«˜ã€‚"
    elif total_score >= 550:
        score_analysis = "æ‚¨çš„ç¸½åˆ†è™•æ–¼ä¸­ç­‰æ°´å¹³ï¼ˆç´„30-40%ç™¾åˆ†ä½ï¼‰ï¼Œå»ºè­°ç¹¼çºŒæé«˜ä»¥å¢åŠ ç”³è«‹ç«¶çˆ­åŠ›ã€‚"
    else:
        score_analysis = "æ‚¨çš„ç¸½åˆ†æœ‰æå‡ç©ºé–“ï¼ˆä½æ–¼30%ç™¾åˆ†ä½ï¼‰ï¼Œå»ºè­°åŠ å¼·å‚™è€ƒç­–ç•¥ã€‚"
    
    tab_container.markdown(f"**ç¸½åˆ†åˆ†æ**ï¼š{score_analysis}")
    
    # åˆ†æå„ç§‘è¡¨ç¾å¹³è¡¡æ€§
    scores = [q_percentile, v_percentile, di_percentile]
    max_diff = max(scores) - min(scores)
    
    if max_diff > 30:
        balance_analysis = "æ‚¨çš„å„ç§‘è¡¨ç¾å·®ç•°è¼ƒå¤§ï¼Œå»ºè­°é‡é»æé«˜è¼ƒå¼±çš„ç§‘ç›®ä»¥ç²å¾—æ›´å¹³è¡¡çš„åˆ†æ•¸ã€‚"
    elif max_diff > 15:
        balance_analysis = "æ‚¨çš„å„ç§‘è¡¨ç¾æœ‰ä¸€å®šå·®ç•°ï¼Œå¯è€ƒæ…®é©ç•¶å¹³è¡¡å„ç§‘å‚™è€ƒæ™‚é–“ã€‚"
    else:
        balance_analysis = "æ‚¨çš„å„ç§‘è¡¨ç¾ç›¸å°å¹³è¡¡ï¼Œé€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„å„ªå‹¢ã€‚"
    
    tab_container.markdown(f"**å¹³è¡¡æ€§åˆ†æ**ï¼š{balance_analysis}")
    
    # æ‰¾å‡ºæœ€å¼·å’Œæœ€å¼±ç§‘ç›®
    subject_names = ["Qç§‘", "Vç§‘", "DIç§‘"]
    strongest = subject_names[scores.index(max(scores))]
    weakest = subject_names[scores.index(min(scores))]
    
    tab_container.markdown(f"**å„ªå‹¢ç§‘ç›®**ï¼š{strongest}ï¼ˆ{max(scores):.1f}%ç™¾åˆ†ä½ï¼‰")
    tab_container.markdown(f"**å¾…æé«˜ç§‘ç›®**ï¼š{weakest}ï¼ˆ{min(scores):.1f}%ç™¾åˆ†ä½ï¼‰")
    
    # æä¾›æ”¹é€²å»ºè­°
    tab_container.subheader("æå‡ç­–ç•¥å»ºè­°")
    
    if total_score < 650:
        tab_container.markdown("""
        **æå‡ç¸½åˆ†ç­–ç•¥**:
        1. åˆ¶å®šå…¨é¢çš„å‚™è€ƒè¨ˆåŠƒï¼Œæ¶µè“‹æ‰€æœ‰ç§‘ç›®
        2. å¢åŠ æ¯å‘¨å­¸ç¿’æ™‚é–“ï¼Œç¢ºä¿ç³»çµ±æ€§å­¸ç¿’
        3. ä½¿ç”¨å®˜æ–¹è³‡æºé€²è¡Œç·´ç¿’ä¸¦ç†Ÿæ‚‰è€ƒè©¦
        4. è€ƒæ…®åƒåŠ å‚™è€ƒèª²ç¨‹æˆ–å°‹æ±‚å°ˆæ¥­è¼”å°
        """)
    
    if min(scores) < 70:
        tab_container.markdown(f"""
        **æå‡{weakest}å»ºè­°**:
        1. è¨ºæ–·å…·é«”å¼±é»ï¼Œæ‰¾å‡ºçŸ¥è­˜æˆ–æŠ€èƒ½å·®è·
        2. å°ˆé …ç·´ç¿’è–„å¼±ç’°ç¯€ä¸¦å°‹æ±‚å°ˆé–€æŒ‡å°
        3. å¢åŠ è©²ç§‘ç›®çš„ç·´ç¿’é »ç‡å’Œæ™‚é–“æŠ•å…¥
        4. å®šæœŸè©•ä¼°é€²æ­¥ä¸¦èª¿æ•´å­¸ç¿’ç­–ç•¥
        """)
    
    tab_container.markdown("""
    **å¹³è¡¡ç™¼å±•å»ºè­°**:
    1. æŒçºŒç·´ç¿’å¼·å‹¢ç§‘ç›®ï¼Œä¿æŒå·²æœ‰å„ªå‹¢
    2. é©ç•¶åˆ†é…æ™‚é–“ï¼Œä¸å¿½è¦–ä»»ä½•ä¸€å€‹ç§‘ç›®
    3. å®šæœŸé€²è¡Œæ¨¡æ“¬æ¸¬è©¦ï¼Œç¢ºèªæ•´é«”é€²æ­¥
    4. åœ¨è€ƒå‰ä¿æŒæ²‰è‘—å†·éœï¼Œåˆç†å®‰æ’è¤‡ç¿’
    """)

# --- Display Results Function (Moved from app.py) ---
def display_results():
    """Display analysis results in tabs"""
    st.header("ğŸ“Š è¨ºæ–·çµæœ")

    if st.session_state.analysis_error:
        st.error(st.session_state.error_message or "åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ã€‚")
    elif not st.session_state.diagnosis_complete:
        st.info("åˆ†ææ­£åœ¨é€²è¡Œä¸­æˆ–å°šæœªå®Œæˆã€‚è«‹ç¨å€™æˆ–æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤æç¤ºã€‚")
    elif st.session_state.processed_df is None or st.session_state.processed_df.empty:
        st.warning("è¨ºæ–·å®Œæˆï¼Œä½†æ²’æœ‰å¯é¡¯ç¤ºçš„æ•¸æ“šã€‚")
        if st.session_state.report_dict:
            st.subheader("è¨ºæ–·æ‘˜è¦")
            for subject, report_md in st.session_state.report_dict.items():
                st.markdown(f"### {subject} ç§‘:")
                st.markdown(report_md, unsafe_allow_html=True)
    else:
        st.success("è¨ºæ–·åˆ†æå·²å®Œæˆï¼")
        subjects_with_data = [subj for subj in SUBJECTS if subj in st.session_state.processed_df['Subject'].unique()]
        if not subjects_with_data:
            st.warning("è™•ç†å¾Œçš„æ•¸æ“šä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç§‘ç›®ã€‚")
        else:
            # æ·»åŠ Totalæ¨™ç±¤é 
            tab_titles = [f"{subj} ç§‘çµæœ" for subj in subjects_with_data]
            tab_titles.append("Total")  # æ·»åŠ Totalæ¨™ç±¤é 
            
            show_ai_consolidated_tab = (
                st.session_state.openai_api_key and
                st.session_state.diagnosis_complete and
                st.session_state.ai_consolidated_report
            )
            if show_ai_consolidated_tab:
                tab_titles.append("âœ¨ AI åŒ¯ç¸½å»ºè­°")

            result_tabs = st.tabs(tab_titles)

            # é¡¯ç¤ºå„ç§‘ç›®çµæœ
            for i, subject in enumerate(subjects_with_data):
                subject_tab = result_tabs[i]
                with subject_tab:
                    df_subject = st.session_state.processed_df[st.session_state.processed_df['Subject'] == subject]
                    report_md = st.session_state.report_dict.get(subject, f"*æœªæ‰¾åˆ° {subject} ç§‘çš„å ±å‘Šã€‚*")

                    st.subheader(f"{subject} ç§‘èƒ½åŠ›ä¼°è¨ˆ (Theta) èµ°å‹¢")
                    theta_plot = st.session_state.theta_plots.get(subject)
                    if theta_plot:
                        st.plotly_chart(theta_plot, use_container_width=True)
                    else:
                        st.info(f"{subject} ç§‘ç›®çš„ Theta ä¼°è¨ˆåœ–è¡¨ä¸å¯ç”¨ã€‚")
                    st.divider()
                    
                    # Use the global COLUMN_DISPLAY_CONFIG and EXCEL_COLUMN_MAP from this module
                    display_subject_results(subject, subject_tab, report_md, df_subject, COLUMN_DISPLAY_CONFIG, EXCEL_COLUMN_MAP)
            
            # é¡¯ç¤ºTotalæ¨™ç±¤é çµæœ
            total_tab_index = len(subjects_with_data)
            with result_tabs[total_tab_index]:
                display_total_results(result_tabs[total_tab_index])

            # é¡¯ç¤ºAIåŒ¯ç¸½æ¨™ç±¤é çµæœ
            if show_ai_consolidated_tab:
                ai_tab_index = len(subjects_with_data) + 1  # +1æ˜¯å› ç‚ºTotalæ¨™ç±¤é 
                ai_consolidated_tab = result_tabs[ai_tab_index]
                with ai_consolidated_tab:
                    st.subheader("AI åŒ¯ç¸½ç·´ç¿’å»ºè­°èˆ‡å¾ŒçºŒè¡Œå‹•")
                    st.markdown(st.session_state.ai_consolidated_report)
                    st.caption("æ­¤å…§å®¹ç”± OpenAI (o4-mini) æ¨¡å‹æ ¹æ“šå„ç§‘å ±å‘Šä¸­çš„ç›¸é—œéƒ¨åˆ†ç”Ÿæˆã€‚è«‹å‹™å¿…çµåˆåŸå§‹å ±å‘Šé€²è¡Œæ ¸å°ã€‚") 