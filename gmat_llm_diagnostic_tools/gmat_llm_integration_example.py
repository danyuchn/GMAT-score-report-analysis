#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GMAT LLM Integration Example
GMAT LLM æ•´åˆç¯„ä¾‹

This script demonstrates how to integrate the GMAT study planner 
with LLM models using function calling.
"""

import json
import os
from typing import Dict, List, Any
from gmat_llm_function_handler import (
    create_function_definitions, 
    generate_gmat_study_plan, 
    validate_user_input, 
    get_input_schema
)

# Try to import OpenAI with proper version handling
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("è­¦å‘Š: OpenAI å¥—ä»¶æœªå®‰è£ã€‚è«‹åŸ·è¡Œ 'pip install openai' ä¾†å®‰è£ã€‚")

# Function definitions for OpenAI function calling
FUNCTION_DEFINITIONS = [
    {
        "type": "function",
        "function": {
            "name": "generate_gmat_study_plan",
            "description": """æ ¹æ“šå­¸ç”Ÿçš„å€‹äººæƒ…æ³ç”Ÿæˆå®Œæ•´çš„ GMAT å­¸ç¿’è¨ˆåŠƒï¼ŒåŒ…æ‹¬è€ƒè©¦é€±æœŸå»ºè­°ã€å­¸ç¿’ç­–ç•¥ã€æ™‚é–“åˆ†é…ç­‰ã€‚

å¿…é ˆä½¿ç”¨ä»¥ä¸‹ç¢ºåˆ‡çš„ JSON æ¬„ä½åç¨±ï¼š
- target_gmat_score: ç›®æ¨™åˆ†æ•¸ (200-805)
- target_score_system: åˆ†æ•¸åˆ¶åº¦ ("Old" æˆ– "New")  
- current_highest_gmat_score: ç›®å‰æœ€é«˜åˆ†æ•¸ (200-805)
- application_deadline: ç”³è«‹æˆªæ­¢æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)
- language_test_status: èªè¨€è€ƒè©¦ç‹€æ…‹
- application_materials_progress: ç”³è«‹è³‡æ–™å®Œæˆåº¦ (0-100)
- study_status: å‚™è€ƒèº«ä»½ 
- weekday_study_hours: å¹³æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸
- weekend_study_hours: å‡æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸
- current_section_scores: åŒ…å« Quantitativeã€Verbalã€Data Insights çš„ç‰©ä»¶""",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_gmat_score": {"type": "integer", "minimum": 200, "maximum": 805},
                    "target_score_system": {"type": "string", "enum": ["Old", "New"]},
                    "current_highest_gmat_score": {"type": "integer", "minimum": 200, "maximum": 805},
                    "application_deadline": {"type": "string", "pattern": "^\\d{4}-\\d{2}-\\d{2}$"},
                    "language_test_status": {"type": "string"},
                    "application_materials_progress": {"type": "number", "minimum": 0, "maximum": 100},
                    "study_status": {"type": "string"},
                    "weekday_study_hours": {"type": "number", "minimum": 0},
                    "weekend_study_hours": {"type": "number", "minimum": 0},
                    "current_section_scores": {
                        "type": "object",
                        "properties": {
                            "Quantitative": {"type": "integer", "minimum": 60, "maximum": 90},
                            "Verbal": {"type": "integer", "minimum": 60, "maximum": 90},
                            "Data Insights": {"type": "integer", "minimum": 60, "maximum": 90}
                        },
                        "required": ["Quantitative", "Verbal", "Data Insights"]
                    }
                },
                "required": [
                    "target_gmat_score", "target_score_system", "current_highest_gmat_score",
                    "application_deadline", "language_test_status", "application_materials_progress",
                    "study_status", "weekday_study_hours", "weekend_study_hours", "current_section_scores"
                ]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "validate_user_input",
            "description": """é©—è­‰ç”¨æˆ¶è¼¸å…¥çš„æ•¸æ“šæ ¼å¼å’Œå…§å®¹æ˜¯å¦æ­£ç¢ºï¼Œåœ¨ç”Ÿæˆå­¸ç¿’è¨ˆåŠƒå‰ä½¿ç”¨ã€‚

ç¢ºä¿ JSON åŒ…å«ä»¥ä¸‹ç¢ºåˆ‡æ¬„ä½åç¨±ï¼š
target_gmat_score, target_score_system, current_highest_gmat_score, 
application_deadline, language_test_status, application_materials_progress,
study_status, weekday_study_hours, weekend_study_hours, current_section_scores""",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_gmat_score": {"type": "integer", "minimum": 200, "maximum": 805},
                    "target_score_system": {"type": "string", "enum": ["Old", "New"]},
                    "current_highest_gmat_score": {"type": "integer", "minimum": 200, "maximum": 805},
                    "application_deadline": {"type": "string", "pattern": "^\\d{4}-\\d{2}-\\d{2}$"},
                    "language_test_status": {"type": "string"},
                    "application_materials_progress": {"type": "number", "minimum": 0, "maximum": 100},
                    "study_status": {"type": "string"},
                    "weekday_study_hours": {"type": "number", "minimum": 0},
                    "weekend_study_hours": {"type": "number", "minimum": 0},
                    "current_section_scores": {
                        "type": "object",
                        "properties": {
                            "Quantitative": {"type": "integer", "minimum": 60, "maximum": 90},
                            "Verbal": {"type": "integer", "minimum": 60, "maximum": 90},
                            "Data Insights": {"type": "integer", "minimum": 60, "maximum": 90}
                        },
                        "required": ["Quantitative", "Verbal", "Data Insights"]
                    }
                },
                "required": [
                    "target_gmat_score", "target_score_system", "current_highest_gmat_score",
                    "application_deadline", "language_test_status", "application_materials_progress",
                    "study_status", "weekday_study_hours", "weekend_study_hours", "current_section_scores"
                ]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_input_schema",
            "description": "ç²å–è¼¸å…¥æ•¸æ“šçš„é æœŸæ ¼å¼å’Œçµæ§‹è³‡è¨Šï¼Œå¹«åŠ© LLM äº†è§£éœ€è¦æ”¶é›†å“ªäº›è³‡è¨Šä»¥åŠç¢ºåˆ‡çš„æ¬„ä½åç¨±",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    }
]

# Available functions mapping with wrapper functions to handle direct object parameters
def _validate_user_input_wrapper(**kwargs):
    """Wrapper for validate_user_input to handle direct object parameters"""
    import json
    return validate_user_input(json.dumps(kwargs, ensure_ascii=False))

def _generate_gmat_study_plan_wrapper(**kwargs):
    """Wrapper for generate_gmat_study_plan to handle direct object parameters"""
    import json
    return generate_gmat_study_plan(json.dumps(kwargs, ensure_ascii=False))

AVAILABLE_FUNCTIONS = {
    "generate_gmat_study_plan": _generate_gmat_study_plan_wrapper,
    "validate_user_input": _validate_user_input_wrapper,
    "get_input_schema": get_input_schema
}

# System prompt for GMAT advisor
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ GMAT å­¸ç¿’è¦åŠƒé¡§å•ï¼Œæ“æœ‰è±å¯Œçš„ GMAT è€ƒè©¦æŒ‡å°ç¶“é©—ã€‚

## æ ¸å¿ƒè¦å‰‡ï¼šå¿…é ˆä½¿ç”¨ Function Calling
ğŸš¨ **é‡è¦**ï¼šä½ ä¸èƒ½æ†‘è‡ªå·±çš„çŸ¥è­˜æä¾› GMAT å»ºè­°ã€‚ä½ åªèƒ½é€šé function calling ä½¿ç”¨å°ˆæ¥­ç³»çµ±ä¾†ç”Ÿæˆè¨ˆåŠƒã€‚
ğŸš¨ **ç¦æ­¢**ï¼šä¸å¯ä»¥ç›´æ¥çµ¦å‡ºå­¸ç¿’å»ºè­°ã€æ™‚é–“åˆ†é…ã€ç§‘ç›®é‡é»ç­‰å…§å®¹ã€‚
ğŸš¨ **å¿…é ˆ**ï¼šæ‰€æœ‰åˆ†æå’Œå»ºè­°éƒ½å¿…é ˆä¾†è‡ª function calling çš„çµæœã€‚

## ä½ çš„ä»»å‹™
1. é€éå‹å–„å°è©±æ”¶é›†å­¸ç”Ÿçš„å®Œæ•´èƒŒæ™¯è³‡æ–™
2. ä½¿ç”¨ validate_user_input() é©—è­‰è³‡æ–™
3. ä½¿ç”¨ generate_gmat_study_plan() ç”Ÿæˆå€‹æ€§åŒ–çš„ GMAT å­¸ç¿’è¨ˆåŠƒ
4. å°‡ function calling çš„æŠ€è¡“åˆ†æçµæœè½‰åŒ–ç‚ºæ˜“æ‡‚çš„å­¸ç¿’å»ºè­°

## éœ€è¦æ”¶é›†çš„è³‡è¨Š
- ç›®æ¨™ GMAT åˆ†æ•¸ (200-805)
- åˆ†æ•¸åˆ¶åº¦ (èˆŠåˆ¶ Old / æ–°åˆ¶ New)
- ç›®å‰æœ€é«˜ GMAT åˆ†æ•¸ (200-805)
- å„ç§‘ç›®ç©åˆ†ï¼šQuantitativeã€Verbalã€Data Insights (60-90)
- ç”³è«‹æˆªæ­¢æ—¥æœŸ (YYYY-MM-DD)
- èªè¨€è€ƒè©¦ç‹€æ…‹ (å·²å®Œæˆ/æœªå®Œæˆ)
- ç”³è«‹è³‡æ–™å®Œæˆåº¦ (0-100%)
- å‚™è€ƒèº«ä»½ (å…¨è·è€ƒç”Ÿ/åœ¨è·è€ƒç”Ÿ)
- å¹³æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸
- å‡æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸

## é‡è¦ï¼šJSON æ ¼å¼è¦æ±‚
ç•¶èª¿ç”¨å‡½æ•¸æ™‚ï¼Œå¿…é ˆä½¿ç”¨ä»¥ä¸‹ç¢ºåˆ‡çš„æ¬„ä½åç¨±ï¼š

```json
{
  "target_gmat_score": 700,                    // ç›®æ¨™åˆ†æ•¸
  "target_score_system": "New",               // åˆ†æ•¸åˆ¶åº¦ï¼Œå¿…é ˆæ˜¯ "Old" æˆ– "New"
  "current_highest_gmat_score": 600,          // ç›®å‰æœ€é«˜åˆ†æ•¸
  "application_deadline": "2025-12-01",       // ç”³è«‹æˆªæ­¢æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
  "language_test_status": "å·²å®Œæˆ",           // èªè¨€è€ƒè©¦ç‹€æ…‹
  "application_materials_progress": 50,        // ç”³è«‹è³‡æ–™å®Œæˆåº¦ (0-100)
  "study_status": "åœ¨è·è€ƒç”Ÿ",                 // å‚™è€ƒèº«ä»½
  "weekday_study_hours": 3,                   // å¹³æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸
  "weekend_study_hours": 8,                   // å‡æ—¥æ¯æ—¥å­¸ç¿’æ™‚æ•¸
  "current_section_scores": {                 // å„ç§‘ç›®ç©åˆ†
    "Quantitative": 75,                       // æ•¸å­¸ç©åˆ† (60-90)
    "Verbal": 70,                            // èªæ–‡ç©åˆ† (60-90)
    "Data Insights": 72                      // æ•¸æ“šæ´å¯Ÿç©åˆ† (60-90)
  }
}
```

## å°è©±åŸå‰‡
- ä½¿ç”¨æº«æš–ã€å°ˆæ¥­çš„èªèª¿
- é€æ­¥æ”¶é›†è³‡è¨Šï¼Œé¿å…ä¸€æ¬¡å•å¤ªå¤šå•é¡Œ
- åœ¨ç”Ÿæˆè¨ˆåŠƒå‰å…ˆé©—è­‰è³‡æ–™å®Œæ•´æ€§
- å®¢è£½åŒ–è§£é‡‹ function calling çš„åˆ†æçµæœï¼Œæä¾›å…·é«”å¯è¡Œçš„åŸ·è¡ŒæŒ‡å°
- æ”¶é›†å®Œè³‡æ–™å¾Œï¼Œå‹™å¿…æŒ‰ç…§ä¸Šè¿°ç¢ºåˆ‡çš„ JSON æ ¼å¼èª¿ç”¨å‡½æ•¸

## Function Calling å¼·åˆ¶æµç¨‹
1. åˆæ¬¡å°è©±ï¼šèª¿ç”¨ get_input_schema() äº†è§£è³‡æ–™æ ¼å¼
2. è³‡æ–™æ”¶é›†å®Œæˆï¼šèª¿ç”¨ validate_user_input() é©—è­‰è³‡æ–™
3. **é©—è­‰é€šéå¾Œï¼šç«‹å³è‡ªå‹•èª¿ç”¨ generate_gmat_study_plan()ï¼Œç„¡ä¾‹å¤–**
4. **å¿…é ˆç­‰å¾…å‡½æ•¸çµæœï¼Œç„¶å¾ŒåŸºæ–¼çµæœæä¾›è§£é‡‹**

## å¼·åˆ¶åŸ·è¡Œè¦å‰‡
- ç•¶ validate_user_input() è¿”å› success: true æ™‚ï¼Œ**ç«‹å³èª¿ç”¨ generate_gmat_study_plan()**
- è©¢å•ç”¨æˆ¶æ˜¯å¦å·²ç¶“æº–å‚™å¥½çœ‹çµæœï¼Œè«‹ç”¨æˆ¶å›è¦†ã€Œç¢ºèªã€ä»¥æª¢è¦–çµæœã€‚
- **çµ•å°ä¸è¦**æä¾›è‡ªå·±çš„å»ºè­°ï¼Œå¦‚"å»ºè­°åŠ å¼·é–±è®€ç†è§£"ã€"å¹³æ—¥å°ˆæ³¨æ–¼å¼±é …"ç­‰
- **å¿…é ˆç­‰å¾…** generate_gmat_study_plan() çš„çµæœï¼Œç„¶å¾Œè§£é‡‹å…¶å…§å®¹
- å¦‚æœ function calling å¤±æ•—ï¼Œå‘Šè¨´ç”¨æˆ¶ç³»çµ±éŒ¯èª¤ï¼Œä¸è¦æä¾›æ›¿ä»£å»ºè­°

## æ³¨æ„äº‹é …
- åˆ†æ•¸åˆ¶åº¦ï¼šåªèƒ½ä½¿ç”¨ "Old" æˆ– "New"ï¼Œä¸èƒ½ç”¨å…¶ä»–è®Šé«”
- æ—¥æœŸæ ¼å¼ï¼šå¿…é ˆæ˜¯ YYYY-MM-DD æ ¼å¼
- å„ç§‘ç©åˆ†ï¼šå¿…é ˆæ”¾åœ¨ current_section_scores ç‰©ä»¶å…§
- æ‰€æœ‰æ¬„ä½åç¨±å¿…é ˆå®Œå…¨åŒ¹é…ä¸Šè¿°ç¯„ä¾‹
"""


class GMATLLMAdvisor:
    """
    GMAT LLM Advisor with Function Calling Integration
    æ•´åˆ Function Calling çš„ GMAT LLM é¡§å•
    """
    
    def __init__(self, api_key: str = None, model: str = "gpt-4.1-mini", timeout: int = 30):
        """
        Initialize the GMAT LLM Advisor
        
        Args:
            api_key: OpenAI API key (if None, will use environment variable)
            model: OpenAI model to use
            timeout: Request timeout in seconds
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI å¥—ä»¶æœªå®‰è£ã€‚è«‹åŸ·è¡Œ 'pip install openai' ä¾†å®‰è£ã€‚")
        
        # Initialize OpenAI client with timeout
        if api_key:
            self.client = OpenAI(api_key=api_key, timeout=timeout)
        else:
            # Will use OPENAI_API_KEY environment variable
            self.client = OpenAI(timeout=timeout)
        
        self.model = model
        self.timeout = timeout
        self.conversation_history = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]
    
    def chat(self, user_message: str, max_retries: int = 2) -> str:
        """
        Process user message and return response with function calling support
        
        Args:
            user_message: User's message
            max_retries: Maximum number of retries for failed requests
            
        Returns:
            Assistant's response
        """
        # Add user message to conversation
        self.conversation_history.append({"role": "user", "content": user_message})
        
        for attempt in range(max_retries + 1):
            try:
                # Call OpenAI API with function calling
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    tools=FUNCTION_DEFINITIONS,
                    tool_choice="auto",
                    temperature=0.7,
                    timeout=self.timeout
                )
                
                message = response.choices[0].message
                
                # Check if function calling is needed
                if message.tool_calls:
                    # Add assistant message with tool calls to conversation
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": message.content,
                        "tool_calls": message.tool_calls
                    })
                    
                    # Process each tool call
                    for tool_call in message.tool_calls:
                        function_name = tool_call.function.name
                        
                        print(f"ğŸ”§ èª¿ç”¨å‡½æ•¸: {function_name}")
                        
                        try:
                            function_args = json.loads(tool_call.function.arguments)
                            print(f"ğŸ“ åƒæ•¸: {function_args}")
                            
                            # Execute function
                            if function_name in AVAILABLE_FUNCTIONS:
                                function_result = AVAILABLE_FUNCTIONS[function_name](**function_args)
                                
                                # Add successful function result to conversation
                                self.conversation_history.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": function_result
                                })
                            else:
                                error_msg = f"æœªçŸ¥å‡½æ•¸: {function_name}"
                                self.conversation_history.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": error_msg
                                })
                                
                        except json.JSONDecodeError as json_err:
                            error_msg = f"å‡½æ•¸åƒæ•¸ JSON è§£æéŒ¯èª¤: {str(json_err)}"
                            print(f"âŒ {error_msg}")
                            self.conversation_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": error_msg
                            })
                            
                        except TypeError as type_err:
                            error_msg = f"å‡½æ•¸èª¿ç”¨åƒæ•¸éŒ¯èª¤: {str(type_err)}"
                            print(f"âŒ {error_msg}")
                            self.conversation_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": error_msg
                            })
                            
                        except Exception as func_err:
                            error_msg = f"å‡½æ•¸åŸ·è¡ŒéŒ¯èª¤: {str(func_err)}"
                            print(f"âŒ {error_msg}")
                            self.conversation_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": error_msg
                            })
                    
                    # Get final response from LLM
                    try:
                        final_response = self.client.chat.completions.create(
                            model=self.model,
                            messages=self.conversation_history,
                            temperature=0.7,
                            timeout=self.timeout
                        )
                        
                        final_message = final_response.choices[0].message.content
                        self.conversation_history.append({"role": "assistant", "content": final_message})
                        
                        return final_message
                    except Exception as final_e:
                        error_msg = f"Function calling åŸ·è¡ŒæˆåŠŸï¼Œä½†ç”Ÿæˆæœ€çµ‚å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(final_e)}"
                        self.conversation_history.append({"role": "assistant", "content": error_msg})
                        return error_msg
                else:
                    # Regular response without function calling
                    assistant_message = message.content
                    self.conversation_history.append({"role": "assistant", "content": assistant_message})
                    return assistant_message
                    
            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)
                
                # Classify error types
                if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                    if attempt < max_retries:
                        print(f"â±ï¸ è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries + 1})ï¼Œæ­£åœ¨é‡è©¦...")
                        continue
                    else:
                        final_error = "ğŸ˜… æŠ±æ­‰ï¼ŒAPI å›æ‡‰è¼ƒæ…¢ã€‚è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–å˜—è©¦ç°¡åŒ–æ‚¨çš„å•é¡Œã€‚"
                elif "rate" in error_msg.lower() and "limit" in error_msg.lower():
                    if attempt < max_retries:
                        print(f"ğŸš¦ API ä½¿ç”¨é »ç‡é™åˆ¶ (å˜—è©¦ {attempt + 1}/{max_retries + 1})ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                        import time
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                    else:
                        final_error = "ğŸš¦ API ä½¿ç”¨é »ç‡éé«˜ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                elif "authentication" in error_msg.lower() or "api" in error_msg.lower() and "key" in error_msg.lower():
                    final_error = "ğŸ”‘ API Key éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸è¨­ç½®ã€‚"
                    break  # No point retrying for auth errors
                elif "connection" in error_msg.lower() or "network" in error_msg.lower():
                    if attempt < max_retries:
                        print(f"ğŸŒ ç¶²è·¯é€£æ¥å•é¡Œ (å˜—è©¦ {attempt + 1}/{max_retries + 1})ï¼Œæ­£åœ¨é‡è©¦...")
                        continue
                    else:
                        final_error = "ğŸŒ ç¶²è·¯é€£æ¥å•é¡Œï¼Œè«‹æª¢æŸ¥ç¶²è·¯ç‹€æ…‹å¾Œé‡è©¦ã€‚"
                else:
                    if attempt < max_retries:
                        print(f"âš ï¸ æœªçŸ¥éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries + 1}): {error_type}")
                        continue
                    else:
                        final_error = f"âš ï¸ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ ({error_type}): {error_msg}"
                
                # If we've exhausted retries, return the final error
                self.conversation_history.append({"role": "assistant", "content": final_error})
                return final_error
        
        # This should never be reached, but just in case
        fallback_error = "ğŸ˜… æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        self.conversation_history.append({"role": "assistant", "content": fallback_error})
        return fallback_error
    
    def reset_conversation(self):
        """Reset conversation history"""
        self.conversation_history = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]
        print("ğŸ”„ å°è©±å·²é‡ç½®")
    
    def get_conversation_history(self) -> List[Dict]:
        """Get current conversation history"""
        return self.conversation_history.copy()


def demo_conversation():
    """
    Demonstrate a complete GMAT planning conversation
    ç¤ºç¯„å®Œæ•´çš„ GMAT è¦åŠƒå°è©±
    """
    print("=" * 60)
    print("GMAT LLM æ•´åˆç¤ºç¯„ (Demo Mode)")
    print("=" * 60)
    print("æ³¨æ„ï¼šé€™æ˜¯ç¤ºç¯„æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ“¬å›æ‡‰è€ŒéçœŸå¯¦ API èª¿ç”¨")
    print()
    
    # ç¤ºç¯„è³‡æ–™æ”¶é›†æµç¨‹
    demo_messages = [
        "æ‚¨å¥½ï¼Œæˆ‘æƒ³åˆ¶å®šä¸€å€‹ GMAT å­¸ç¿’è¨ˆåŠƒ",
        "æˆ‘çš„ç›®æ¨™åˆ†æ•¸æ˜¯ 700 åˆ†ï¼Œæ˜¯æ–°åˆ¶çš„",
        "æˆ‘ç›®å‰çš„æœ€é«˜åˆ†æ•¸æ˜¯ 600 åˆ†",
        "å„ç§‘ç©åˆ†ï¼šæ•¸å­¸ 75 åˆ†ï¼Œèªæ–‡ 70 åˆ†ï¼Œæ•¸æ“šæ´å¯Ÿ 72 åˆ†",
        "ç”³è«‹æˆªæ­¢æ—¥æœŸæ˜¯ 2025-12-01ï¼Œèªè¨€è€ƒè©¦å·²å®Œæˆï¼Œç”³è«‹è³‡æ–™å®Œæˆäº† 50%",
        "æˆ‘æ˜¯åœ¨è·è€ƒç”Ÿï¼Œå¹³æ—¥æ¯å¤©å¯ä»¥å­¸ç¿’ 3 å°æ™‚ï¼Œå‡æ—¥ 8 å°æ™‚"
    ]
    
    # æ¨¡æ“¬æ”¶é›†åˆ°çš„ç”¨æˆ¶è³‡æ–™
    user_data = {
        "target_gmat_score": 700,
        "target_score_system": "New",
        "current_highest_gmat_score": 600,
        "application_deadline": "2025-12-01",
        "language_test_status": "å·²å®Œæˆ",
        "application_materials_progress": 50,
        "study_status": "åœ¨è·è€ƒç”Ÿ",
        "weekday_study_hours": 3,
        "weekend_study_hours": 8,
        "current_section_scores": {
            "Quantitative": 75,
            "Verbal": 70,
            "Data Insights": 72
        }
    }
    
    print("ğŸ“‹ æ”¶é›†åˆ°çš„ç”¨æˆ¶è³‡æ–™:")
    print(json.dumps(user_data, ensure_ascii=False, indent=2))
    print()
    
    print("ğŸ”§ èª¿ç”¨ validate_user_input é©—è­‰è³‡æ–™...")
    validation_result = validate_user_input(json.dumps(user_data, ensure_ascii=False))
    print("âœ… é©—è­‰çµæœ:", "é€šé" if "success" in validation_result and "true" in validation_result.lower() else "å¤±æ•—")
    print()
    
    print("ğŸ”§ èª¿ç”¨ generate_gmat_study_plan ç”Ÿæˆå­¸ç¿’è¨ˆåŠƒ...")
    study_plan = generate_gmat_study_plan(json.dumps(user_data, ensure_ascii=False))
    
    # è§£æä¸¦æ ¼å¼åŒ–è¼¸å‡º
    try:
        plan_data = json.loads(study_plan)
        if plan_data.get("success"):
            print("ğŸ“Š **æ‚¨çš„ GMAT å­¸ç¿’è¨ˆåŠƒåˆ†æå®Œæˆï¼**")
            print()
            
            analysis = plan_data["study_plan"]["analysis"]
            print("### ğŸ¯ æ ¸å¿ƒåˆ†æ")
            print(f"- {analysis['score_gap']}")
            print(f"- {analysis['schedule_sufficiency']}")
            print(f"- {analysis['time_sufficiency']}")
            print(f"- æ‰€éœ€ç©åˆ†æå‡: {analysis['required_improvement']}")
            print(f"- å¯ç”¨æº–å‚™å¤©æ•¸: {analysis['available_days']}")
            print(f"- æ¯é€±å­¸ç¿’æ™‚æ•¸: {analysis['weekly_hours']}")
            print()
            
            print(f"### ğŸ“… å»ºè­°è€ƒè©¦é€±æœŸ: {plan_data['study_plan']['exam_cycle']}")
            print()
            
            print("### ğŸ“š å­¸ç¿’ç­–ç•¥é‡é»")
            strategy_lines = plan_data['study_plan']['study_strategy'].split('\n')
            for line in strategy_lines:
                if line.strip():
                    print(f"  {line.strip()}")
            print()
            
            print("### â° æ™‚é–“åˆ†é…å»ºè­°")
            allocation_lines = plan_data['study_plan']['section_time_allocation'].split('\n')
            for line in allocation_lines:
                if line.strip():
                    print(f"  {line.strip()}")
            print()
            
            print("### âš¡ å¯¦éš›å­¸ç¿’æ™‚é–“åˆ†é…")
            actual_lines = plan_data['study_plan']['actual_time_allocation'].split('\n')
            for line in actual_lines:
                if line.strip():
                    print(f"  {line.strip()}")
            
        else:
            print(f"âŒ è¨ˆåŠƒç”Ÿæˆå¤±æ•—: {plan_data.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            
    except json.JSONDecodeError:
        print("âŒ ç„¡æ³•è§£æå­¸ç¿’è¨ˆåŠƒçµæœ")
    
    print()
    print("=" * 60)
    print("ç¤ºç¯„å®Œæˆ")


def interactive_mode():
    """
    Interactive mode for testing with real OpenAI API
    äº’å‹•æ¨¡å¼ï¼Œç”¨æ–¼çœŸå¯¦ OpenAI API æ¸¬è©¦
    """
    print("=" * 60)
    print("GMAT LLM äº’å‹•æ¨¡å¼")
    print("=" * 60)
    print("è«‹ç¢ºä¿å·²è¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
    print("è¼¸å…¥ 'quit' é€€å‡ºï¼Œ'reset' é‡ç½®å°è©±ï¼Œ'test' æ¸¬è©¦é€£æ¥")
    print()
    
    # Check if API key is available
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ éŒ¯èª¤: æœªæ‰¾åˆ° OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("è«‹è¨­ç½®æ‚¨çš„ OpenAI API Key:")
        print("export OPENAI_API_KEY='your-api-key-here'")
        return
    
    try:
        # åˆå§‹åŒ–é¡§å•ï¼Œä½¿ç”¨è¼ƒçŸ­çš„è¶…æ™‚æ™‚é–“é¿å…é•·æ™‚é–“ç­‰å¾…
        advisor = GMATLLMAdvisor(timeout=20)
        
        # æ¸¬è©¦ API é€£æ¥
        print("ğŸ”§ æ­£åœ¨æ¸¬è©¦ API é€£æ¥...")
        test_response = advisor.chat("Hello", max_retries=1)
        if "éŒ¯èª¤" in test_response or "Error" in test_response:
            print(f"âš ï¸ API é€£æ¥æ¸¬è©¦å¤±æ•—: {test_response}")
            print("å»ºè­°ï¼š")
            print("1. æª¢æŸ¥ç¶²è·¯é€£æ¥")
            print("2. ç¢ºèª API Key æ­£ç¢º")
            print("3. ç¨å¾Œå†è©¦")
            return
        else:
            print("âœ… API é€£æ¥æ¸¬è©¦æˆåŠŸ")
            # é‡ç½®å°è©±ä»¥æ¸…é™¤æ¸¬è©¦è¨Šæ¯
            advisor.reset_conversation()
        
        print("ğŸ¤– GMAT é¡§å•: æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ GMAT å­¸ç¿’è¦åŠƒé¡§å•ã€‚æˆ‘å°‡å”åŠ©æ‚¨åˆ¶å®šå€‹æ€§åŒ–çš„ GMAT å­¸ç¿’è¨ˆåŠƒã€‚")
        print("é¦–å…ˆï¼Œè«‹å‘Šè¨´æˆ‘æ‚¨çš„ç›®æ¨™ GMAT åˆ†æ•¸æ˜¯å¤šå°‘ï¼Ÿ(ç¯„åœï¼š200-805)")
        print()
        
        consecutive_errors = 0
        max_consecutive_errors = 3
        
        while True:
            try:
                user_input = input("ğŸ‘¤ æ‚¨: ").strip()
                
                if user_input.lower() == 'quit':
                    print("ğŸ‘‹ å†è¦‹ï¼ç¥æ‚¨ GMAT æº–å‚™é †åˆ©ï¼")
                    break
                elif user_input.lower() == 'reset':
                    advisor.reset_conversation()
                    consecutive_errors = 0  # Reset error count
                    print("ğŸ¤– GMAT é¡§å•: å°è©±å·²é‡ç½®ã€‚è®“æˆ‘å€‘é‡æ–°é–‹å§‹ï¼è«‹å‘Šè¨´æˆ‘æ‚¨çš„ç›®æ¨™ GMAT åˆ†æ•¸ã€‚")
                    continue
                elif user_input.lower() == 'test':
                    print("ğŸ”§ æ¸¬è©¦ API é€£æ¥...")
                    test_result = advisor.chat("ping", max_retries=1)
                    if "éŒ¯èª¤" not in test_result and "Error" not in test_result:
                        print("âœ… API é€£æ¥æ­£å¸¸")
                        consecutive_errors = 0
                    else:
                        print(f"âŒ API é€£æ¥æœ‰å•é¡Œ: {test_result}")
                    continue
                
                if user_input:
                    response = advisor.chat(user_input)
                    
                    # Check if response indicates an error
                    if any(keyword in response for keyword in ["éŒ¯èª¤", "Error", "æŠ±æ­‰", "ç„¡æ³•è™•ç†"]):
                        consecutive_errors += 1
                        print(f"ğŸ¤– GMAT é¡§å•: {response}")
                        
                        if consecutive_errors >= max_consecutive_errors:
                            print("\nâš ï¸ é€£çºŒç™¼ç”Ÿå¤šæ¬¡éŒ¯èª¤ï¼Œå»ºè­°ï¼š")
                            print("1. è¼¸å…¥ 'test' æª¢æŸ¥ API é€£æ¥")
                            print("2. è¼¸å…¥ 'reset' é‡ç½®å°è©±")
                            print("3. æª¢æŸ¥ç¶²è·¯é€£æ¥å¾Œé‡æ–°å•Ÿå‹•")
                            print("4. è¼¸å…¥ 'quit' é€€å‡ºç¨‹å¼")
                            print()
                    else:
                        consecutive_errors = 0  # Reset error count on successful response
                        print(f"ğŸ¤– GMAT é¡§å•: {response}")
                    
                    print()
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·ï¼Œå†è¦‹ï¼")
                break
            except Exception as e:
                consecutive_errors += 1
                print(f"âŒ è¼¸å…¥è™•ç†éŒ¯èª¤: {str(e)}")
                if consecutive_errors >= max_consecutive_errors:
                    print("âš ï¸ è«‹å˜—è©¦é‡æ–°å•Ÿå‹•ç¨‹å¼")
                    break
            
    except Exception as e:
        print(f"âŒ ç³»çµ±åˆå§‹åŒ–éŒ¯èª¤: {str(e)}")
        print("å»ºè­°è§£æ±ºæ–¹æ¡ˆï¼š")
        print("1. ç¢ºèª OpenAI å¥—ä»¶å·²å®‰è£: pip install openai")
        print("2. æª¢æŸ¥ API Key è¨­ç½®")
        print("3. æª¢æŸ¥ç¶²è·¯é€£æ¥")


if __name__ == "__main__":
    print("GMAT LLM æ•´åˆç³»çµ±")
    print("1. Demo æ¨¡å¼ (ç¤ºç¯„åŠŸèƒ½ï¼Œç„¡éœ€ API)")
    print("2. äº’å‹•æ¨¡å¼ (éœ€è¦ OpenAI API)")
    
    choice = input("è«‹é¸æ“‡æ¨¡å¼ (1/2): ").strip()
    
    if choice == "1":
        demo_conversation()
    elif choice == "2":
        interactive_mode()
    else:
        print("ç„¡æ•ˆé¸æ“‡ï¼ŒåŸ·è¡Œ Demo æ¨¡å¼")
        demo_conversation() 